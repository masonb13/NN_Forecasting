{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets\n",
    "from torchvision.transforms import ToTensor\n",
    "from torch.utils.data import Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [],
   "source": [
    "#torch data set\n",
    "class dataSetAll(Dataset):\n",
    "    def __init__(self, yearLow, yearHigh):\n",
    "        #import data from CDC\n",
    "        self.df = pd.read_csv(\"data\\FluViewPhase2Data\\WHO_NREVSS_Combined_prior_to_2015_16.csv\")\n",
    "        self.df = self.df[(yearLow <= self.df[\"YEAR\"]) & (self.df[\"YEAR\"] < yearHigh)][\"TOTAL\"]\n",
    "        #turn data into features and output\n",
    "        #features: 5 previous + one from last year for predicted\n",
    "        #output: prediction for next time\n",
    "\n",
    "        #create test data\n",
    "        self.numFeat = 5 #------------------------\n",
    "        self.numOut = 1\n",
    "        self.data = np.asarray(self.df,dtype=np.float32)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)-self.numFeat-self.numOut\n",
    "    \n",
    "    def __getitem__(self,idx):\n",
    "        return self.data[idx:idx+self.numFeat],self.data[idx+self.numFeat]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create data loaders\n",
    "train_data = dataSetAll(1900,2013)\n",
    "test_data = dataSetAll(2013,2100)\n",
    "train_dataloader = DataLoader(train_data, batch_size=64)\n",
    "test_dataloader = DataLoader(test_data, batch_size=64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create our RNN based network with an RNN followed by a linear layer\n",
    "inputSize = 1\n",
    "sequenceLength = 5\n",
    "numLayers = 1\n",
    "hiddenSize = 256\n",
    "batchSize = 64\n",
    "\n",
    "class RNN(nn.Module):\n",
    "    def __init__(self, inputSize, hiddenSize, numLayers,sequenceLength=1):\n",
    "        super(RNN, self).__init__()\n",
    "        self.inputSize = inputSize\n",
    "        self.hiddenSize = hiddenSize\n",
    "        self.numLayers = numLayers\n",
    "        self.RNN = nn.RNN(inputSize,hiddenSize,numLayers,nonlinearity='relu',batch_first=True)\n",
    "        self.fc = nn.Linear(hiddenSize*sequenceLength,1)\n",
    "        \n",
    "    def forward(self,x):\n",
    "        h0 = torch.zeros(self.numLayers,x.size(0),self.hiddenSize)\n",
    "        out, _ = self.RNN(x,h0)\n",
    "        out = self.fc(out[:,-1,:])\n",
    "        return out\n",
    "\n",
    "model = RNN(inputSize,hiddenSize,numLayers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train/test loop\n",
    "def train_loop(dataloader, model, loss_fn, optimizer,t):\n",
    "    size = len(dataloader.dataset)\n",
    "    for batch, (X, y) in enumerate(dataloader):\n",
    "        # Compute prediction and loss\n",
    "        X = X[:,:,None]\n",
    "        # print(X.size())\n",
    "        pred = model(X).squeeze()\n",
    "        loss = loss_fn(pred, y)\n",
    "        # print(\"pred\",pred)\n",
    "        # print(\"Y\",y)\n",
    "        # Backpropagation\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if batch % size == 0:\n",
    "            loss, current = loss.item(), batch * len(X)\n",
    "            print(f\"loss({t}): {loss:>7f}  [{current:>5d}/{size:>5d}]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss: 7895528.500000  [    0/  787]\n",
      "loss: 7446405.000000  [    0/  787]\n",
      "loss: 7134137.000000  [    0/  787]\n",
      "loss: 6802432.500000  [    0/  787]\n",
      "loss: 6427969.000000  [    0/  787]\n",
      "loss: 5983659.500000  [    0/  787]\n",
      "loss: 5415042.500000  [    0/  787]\n",
      "loss: 4665453.500000  [    0/  787]\n",
      "loss: 3686128.000000  [    0/  787]\n",
      "loss: 2554547.500000  [    0/  787]\n",
      "loss: 1622550.000000  [    0/  787]\n",
      "loss: 1337516.125000  [    0/  787]\n",
      "loss: 1297299.500000  [    0/  787]\n",
      "loss: 1262819.250000  [    0/  787]\n",
      "loss: 1213488.500000  [    0/  787]\n",
      "loss: 1164117.375000  [    0/  787]\n",
      "loss: 1121127.125000  [    0/  787]\n",
      "loss: 1078239.125000  [    0/  787]\n",
      "loss: 1035556.625000  [    0/  787]\n",
      "loss: 992582.250000  [    0/  787]\n",
      "loss: 949375.437500  [    0/  787]\n",
      "loss: 906107.875000  [    0/  787]\n",
      "loss: 862471.500000  [    0/  787]\n",
      "loss: 819443.625000  [    0/  787]\n",
      "loss: 778086.062500  [    0/  787]\n",
      "loss: 739185.375000  [    0/  787]\n",
      "loss: 702710.750000  [    0/  787]\n",
      "loss: 670236.312500  [    0/  787]\n",
      "loss: 644516.000000  [    0/  787]\n",
      "loss: 621376.250000  [    0/  787]\n",
      "loss: 629293.812500  [    0/  787]\n",
      "loss: 622171.437500  [    0/  787]\n",
      "loss: 634872.562500  [    0/  787]\n",
      "loss: 601743.687500  [    0/  787]\n",
      "loss: 661035.375000  [    0/  787]\n",
      "loss: 667185.562500  [    0/  787]\n",
      "loss: 625810.625000  [    0/  787]\n",
      "loss: 605931.625000  [    0/  787]\n",
      "loss: 586686.312500  [    0/  787]\n",
      "loss: 570628.000000  [    0/  787]\n",
      "loss: 558002.000000  [    0/  787]\n",
      "loss: 547198.625000  [    0/  787]\n",
      "loss: 538247.437500  [    0/  787]\n",
      "loss: 530708.750000  [    0/  787]\n",
      "loss: 523301.156250  [    0/  787]\n",
      "loss: 517662.062500  [    0/  787]\n",
      "loss: 511587.812500  [    0/  787]\n",
      "loss: 508372.375000  [    0/  787]\n",
      "loss: 501148.812500  [    0/  787]\n",
      "loss: 501101.375000  [    0/  787]\n",
      "loss: 491862.468750  [    0/  787]\n",
      "loss: 496226.750000  [    0/  787]\n",
      "loss: 483420.843750  [    0/  787]\n",
      "loss: 497028.062500  [    0/  787]\n",
      "loss: 477325.937500  [    0/  787]\n",
      "loss: 496130.406250  [    0/  787]\n",
      "loss: 477512.250000  [    0/  787]\n",
      "loss: 494011.812500  [    0/  787]\n",
      "loss: 486301.187500  [    0/  787]\n",
      "loss: 493425.312500  [    0/  787]\n",
      "loss: 510731.812500  [    0/  787]\n",
      "loss: 501786.250000  [    0/  787]\n",
      "loss: 507503.250000  [    0/  787]\n",
      "loss: 489212.062500  [    0/  787]\n",
      "loss: 485982.218750  [    0/  787]\n",
      "loss: 478338.406250  [    0/  787]\n",
      "loss: 472582.125000  [    0/  787]\n",
      "loss: 470020.062500  [    0/  787]\n",
      "loss: 463264.593750  [    0/  787]\n",
      "loss: 466342.781250  [    0/  787]\n",
      "loss: 453921.906250  [    0/  787]\n",
      "loss: 459623.187500  [    0/  787]\n",
      "loss: 447182.500000  [    0/  787]\n",
      "loss: 454700.281250  [    0/  787]\n",
      "loss: 445722.937500  [    0/  787]\n",
      "loss: 455495.187500  [    0/  787]\n",
      "loss: 446679.843750  [    0/  787]\n",
      "loss: 450539.750000  [    0/  787]\n",
      "loss: 458117.875000  [    0/  787]\n",
      "loss: 453591.625000  [    0/  787]\n",
      "loss: 459456.687500  [    0/  787]\n",
      "loss: 449396.093750  [    0/  787]\n",
      "loss: 451141.468750  [    0/  787]\n",
      "loss: 444529.031250  [    0/  787]\n",
      "loss: 441289.593750  [    0/  787]\n",
      "loss: 442587.218750  [    0/  787]\n",
      "loss: 431990.125000  [    0/  787]\n",
      "loss: 434779.500000  [    0/  787]\n",
      "loss: 427508.937500  [    0/  787]\n",
      "loss: 431701.562500  [    0/  787]\n",
      "loss: 424520.343750  [    0/  787]\n",
      "loss: 427651.656250  [    0/  787]\n",
      "loss: 426255.718750  [    0/  787]\n",
      "loss: 427836.906250  [    0/  787]\n",
      "loss: 426343.812500  [    0/  787]\n",
      "loss: 424506.593750  [    0/  787]\n",
      "loss: 429490.906250  [    0/  787]\n",
      "loss: 425770.250000  [    0/  787]\n",
      "loss: 421540.937500  [    0/  787]\n",
      "loss: 419149.281250  [    0/  787]\n",
      "loss: 419165.281250  [    0/  787]\n",
      "loss: 417116.187500  [    0/  787]\n",
      "loss: 412835.437500  [    0/  787]\n",
      "loss: 416512.906250  [    0/  787]\n",
      "loss: 406593.156250  [    0/  787]\n",
      "loss: 413759.000000  [    0/  787]\n",
      "loss: 407979.781250  [    0/  787]\n",
      "loss: 410629.000000  [    0/  787]\n",
      "loss: 421935.093750  [    0/  787]\n",
      "loss: 411629.812500  [    0/  787]\n",
      "loss: 415582.312500  [    0/  787]\n",
      "loss: 406298.750000  [    0/  787]\n",
      "loss: 404252.906250  [    0/  787]\n",
      "loss: 400245.937500  [    0/  787]\n",
      "loss: 395876.812500  [    0/  787]\n",
      "loss: 394985.250000  [    0/  787]\n",
      "loss: 390136.562500  [    0/  787]\n",
      "loss: 392084.968750  [    0/  787]\n",
      "loss: 384417.843750  [    0/  787]\n",
      "loss: 387587.281250  [    0/  787]\n",
      "loss: 383696.375000  [    0/  787]\n",
      "loss: 387658.250000  [    0/  787]\n",
      "loss: 380962.843750  [    0/  787]\n",
      "loss: 384263.812500  [    0/  787]\n",
      "loss: 386893.062500  [    0/  787]\n",
      "loss: 384876.437500  [    0/  787]\n",
      "loss: 385340.875000  [    0/  787]\n",
      "loss: 381498.343750  [    0/  787]\n",
      "loss: 381717.593750  [    0/  787]\n",
      "loss: 378696.625000  [    0/  787]\n",
      "loss: 376689.000000  [    0/  787]\n",
      "loss: 375449.062500  [    0/  787]\n",
      "loss: 372091.093750  [    0/  787]\n",
      "loss: 376748.812500  [    0/  787]\n",
      "loss: 365447.375000  [    0/  787]\n",
      "loss: 373784.750000  [    0/  787]\n",
      "loss: 369864.687500  [    0/  787]\n",
      "loss: 373351.531250  [    0/  787]\n",
      "loss: 385443.281250  [    0/  787]\n",
      "loss: 374474.968750  [    0/  787]\n",
      "loss: 377003.000000  [    0/  787]\n",
      "loss: 367672.875000  [    0/  787]\n",
      "loss: 365095.156250  [    0/  787]\n",
      "loss: 361892.593750  [    0/  787]\n",
      "loss: 357719.812500  [    0/  787]\n",
      "loss: 357839.937500  [    0/  787]\n",
      "loss: 352520.000000  [    0/  787]\n",
      "loss: 354787.468750  [    0/  787]\n",
      "loss: 349344.343750  [    0/  787]\n",
      "loss: 352991.218750  [    0/  787]\n",
      "loss: 347678.375000  [    0/  787]\n",
      "loss: 350648.625000  [    0/  787]\n",
      "loss: 348513.062500  [    0/  787]\n",
      "loss: 349274.375000  [    0/  787]\n",
      "loss: 348425.031250  [    0/  787]\n",
      "loss: 346733.937500  [    0/  787]\n",
      "loss: 347484.343750  [    0/  787]\n",
      "loss: 344756.000000  [    0/  787]\n",
      "loss: 343633.812500  [    0/  787]\n",
      "loss: 342170.062500  [    0/  787]\n",
      "loss: 343309.187500  [    0/  787]\n",
      "loss: 341350.000000  [    0/  787]\n",
      "loss: 337257.500000  [    0/  787]\n",
      "loss: 336648.437500  [    0/  787]\n",
      "loss: 335621.875000  [    0/  787]\n",
      "loss: 335718.062500  [    0/  787]\n",
      "loss: 332449.437500  [    0/  787]\n",
      "loss: 332053.562500  [    0/  787]\n",
      "loss: 331780.250000  [    0/  787]\n",
      "loss: 332203.125000  [    0/  787]\n",
      "loss: 329701.562500  [    0/  787]\n",
      "loss: 328547.843750  [    0/  787]\n",
      "loss: 328681.000000  [    0/  787]\n",
      "loss: 327754.500000  [    0/  787]\n",
      "loss: 325293.125000  [    0/  787]\n",
      "loss: 324635.375000  [    0/  787]\n",
      "loss: 324355.937500  [    0/  787]\n",
      "loss: 323639.437500  [    0/  787]\n",
      "loss: 321239.625000  [    0/  787]\n",
      "loss: 321017.281250  [    0/  787]\n",
      "loss: 318015.906250  [    0/  787]\n",
      "loss: 319222.812500  [    0/  787]\n",
      "loss: 320476.000000  [    0/  787]\n",
      "loss: 319584.000000  [    0/  787]\n",
      "loss: 315548.437500  [    0/  787]\n",
      "loss: 315574.937500  [    0/  787]\n",
      "loss: 316785.625000  [    0/  787]\n",
      "loss: 314902.781250  [    0/  787]\n",
      "loss: 314001.656250  [    0/  787]\n",
      "loss: 312153.312500  [    0/  787]\n",
      "loss: 311349.375000  [    0/  787]\n",
      "loss: 310440.750000  [    0/  787]\n",
      "loss: 308903.812500  [    0/  787]\n",
      "loss: 308613.750000  [    0/  787]\n",
      "loss: 306331.406250  [    0/  787]\n",
      "loss: 306668.437500  [    0/  787]\n",
      "loss: 305018.281250  [    0/  787]\n",
      "loss: 304832.125000  [    0/  787]\n",
      "loss: 303738.093750  [    0/  787]\n",
      "loss: 303308.343750  [    0/  787]\n",
      "loss: 302425.062500  [    0/  787]\n",
      "loss: 301727.187500  [    0/  787]\n",
      "loss: 300560.875000  [    0/  787]\n",
      "loss: 299688.906250  [    0/  787]\n",
      "loss: 298726.375000  [    0/  787]\n",
      "loss: 297897.343750  [    0/  787]\n",
      "loss: 297052.843750  [    0/  787]\n",
      "loss: 296130.437500  [    0/  787]\n",
      "loss: 295170.218750  [    0/  787]\n",
      "loss: 294508.812500  [    0/  787]\n",
      "loss: 293956.000000  [    0/  787]\n",
      "loss: 293217.625000  [    0/  787]\n",
      "loss: 291583.343750  [    0/  787]\n",
      "loss: 291194.343750  [    0/  787]\n",
      "loss: 290528.437500  [    0/  787]\n",
      "loss: 289872.468750  [    0/  787]\n",
      "loss: 288731.156250  [    0/  787]\n",
      "loss: 288701.656250  [    0/  787]\n",
      "loss: 285958.531250  [    0/  787]\n",
      "loss: 286945.250000  [    0/  787]\n",
      "loss: 286559.781250  [    0/  787]\n",
      "loss: 286039.093750  [    0/  787]\n",
      "loss: 285241.687500  [    0/  787]\n",
      "loss: 284101.750000  [    0/  787]\n",
      "loss: 283564.062500  [    0/  787]\n",
      "loss: 282417.250000  [    0/  787]\n",
      "loss: 282118.343750  [    0/  787]\n",
      "loss: 281401.500000  [    0/  787]\n",
      "loss: 280827.125000  [    0/  787]\n",
      "loss: 279891.656250  [    0/  787]\n",
      "loss: 278311.156250  [    0/  787]\n",
      "loss: 278091.625000  [    0/  787]\n",
      "loss: 277685.812500  [    0/  787]\n",
      "loss: 277253.718750  [    0/  787]\n",
      "loss: 275352.312500  [    0/  787]\n",
      "loss: 275151.500000  [    0/  787]\n",
      "loss: 274168.968750  [    0/  787]\n",
      "loss: 274197.718750  [    0/  787]\n",
      "loss: 273521.875000  [    0/  787]\n",
      "loss: 272924.500000  [    0/  787]\n",
      "loss: 272064.968750  [    0/  787]\n",
      "loss: 271380.875000  [    0/  787]\n",
      "loss: 271147.343750  [    0/  787]\n",
      "loss: 270176.000000  [    0/  787]\n",
      "loss: 269465.250000  [    0/  787]\n",
      "loss: 268598.281250  [    0/  787]\n",
      "loss: 268126.531250  [    0/  787]\n",
      "loss: 267240.500000  [    0/  787]\n",
      "loss: 266728.000000  [    0/  787]\n",
      "loss: 266025.031250  [    0/  787]\n",
      "loss: 265121.062500  [    0/  787]\n",
      "loss: 264543.906250  [    0/  787]\n",
      "loss: 263963.937500  [    0/  787]\n",
      "loss: 263546.843750  [    0/  787]\n",
      "loss: 262519.812500  [    0/  787]\n",
      "loss: 262432.500000  [    0/  787]\n",
      "loss: 259894.156250  [    0/  787]\n",
      "loss: 261335.250000  [    0/  787]\n",
      "loss: 261724.609375  [    0/  787]\n",
      "loss: 260197.109375  [    0/  787]\n",
      "loss: 260521.593750  [    0/  787]\n",
      "loss: 258085.578125  [    0/  787]\n",
      "loss: 257400.515625  [    0/  787]\n",
      "loss: 257459.562500  [    0/  787]\n",
      "loss: 257446.640625  [    0/  787]\n",
      "loss: 255939.781250  [    0/  787]\n",
      "loss: 255737.984375  [    0/  787]\n",
      "loss: 254998.578125  [    0/  787]\n",
      "loss: 253832.937500  [    0/  787]\n",
      "loss: 253584.531250  [    0/  787]\n",
      "loss: 252423.625000  [    0/  787]\n",
      "loss: 252893.328125  [    0/  787]\n",
      "loss: 251672.875000  [    0/  787]\n",
      "loss: 251104.640625  [    0/  787]\n",
      "loss: 250201.125000  [    0/  787]\n",
      "loss: 249862.734375  [    0/  787]\n",
      "loss: 249362.359375  [    0/  787]\n",
      "loss: 248610.718750  [    0/  787]\n",
      "loss: 248765.218750  [    0/  787]\n",
      "loss: 247710.750000  [    0/  787]\n",
      "loss: 247328.687500  [    0/  787]\n",
      "loss: 246299.687500  [    0/  787]\n",
      "loss: 245786.437500  [    0/  787]\n",
      "loss: 244942.390625  [    0/  787]\n",
      "loss: 245080.140625  [    0/  787]\n",
      "loss: 244065.296875  [    0/  787]\n",
      "loss: 243073.671875  [    0/  787]\n",
      "loss: 242661.968750  [    0/  787]\n",
      "loss: 242349.875000  [    0/  787]\n",
      "loss: 241661.312500  [    0/  787]\n",
      "loss: 241146.390625  [    0/  787]\n",
      "loss: 240584.062500  [    0/  787]\n",
      "loss: 239959.468750  [    0/  787]\n",
      "loss: 239168.500000  [    0/  787]\n",
      "loss: 239138.921875  [    0/  787]\n",
      "loss: 238494.625000  [    0/  787]\n",
      "loss: 238092.031250  [    0/  787]\n",
      "loss: 237376.437500  [    0/  787]\n",
      "loss: 236862.281250  [    0/  787]\n",
      "loss: 236636.718750  [    0/  787]\n",
      "loss: 235872.437500  [    0/  787]\n",
      "loss: 235355.593750  [    0/  787]\n",
      "loss: 235084.671875  [    0/  787]\n",
      "loss: 234398.875000  [    0/  787]\n",
      "loss: 234235.093750  [    0/  787]\n",
      "loss: 233598.593750  [    0/  787]\n",
      "loss: 232974.328125  [    0/  787]\n",
      "loss: 232307.640625  [    0/  787]\n",
      "loss: 232486.203125  [    0/  787]\n",
      "loss: 231639.156250  [    0/  787]\n",
      "loss: 230981.468750  [    0/  787]\n",
      "loss: 230491.234375  [    0/  787]\n",
      "loss: 230122.953125  [    0/  787]\n",
      "loss: 229685.000000  [    0/  787]\n",
      "loss: 229081.562500  [    0/  787]\n",
      "loss: 228625.250000  [    0/  787]\n",
      "loss: 228688.093750  [    0/  787]\n",
      "loss: 227924.781250  [    0/  787]\n",
      "loss: 227646.687500  [    0/  787]\n",
      "loss: 226981.062500  [    0/  787]\n",
      "loss: 227480.875000  [    0/  787]\n",
      "loss: 226546.062500  [    0/  787]\n",
      "loss: 226579.109375  [    0/  787]\n",
      "loss: 225811.593750  [    0/  787]\n",
      "loss: 225403.531250  [    0/  787]\n",
      "loss: 224838.953125  [    0/  787]\n",
      "loss: 224839.343750  [    0/  787]\n",
      "loss: 224277.578125  [    0/  787]\n",
      "loss: 223425.593750  [    0/  787]\n",
      "loss: 223505.562500  [    0/  787]\n",
      "loss: 223260.171875  [    0/  787]\n",
      "loss: 222804.531250  [    0/  787]\n",
      "loss: 222733.781250  [    0/  787]\n",
      "loss: 222273.687500  [    0/  787]\n",
      "loss: 221891.234375  [    0/  787]\n",
      "loss: 221335.156250  [    0/  787]\n",
      "loss: 221255.156250  [    0/  787]\n",
      "loss: 220372.250000  [    0/  787]\n",
      "loss: 220991.750000  [    0/  787]\n",
      "loss: 219709.875000  [    0/  787]\n",
      "loss: 219420.109375  [    0/  787]\n",
      "loss: 219334.984375  [    0/  787]\n",
      "loss: 218661.250000  [    0/  787]\n",
      "loss: 218962.968750  [    0/  787]\n",
      "loss: 218008.437500  [    0/  787]\n",
      "loss: 217753.031250  [    0/  787]\n",
      "loss: 217528.781250  [    0/  787]\n",
      "loss: 216998.062500  [    0/  787]\n",
      "loss: 217161.265625  [    0/  787]\n",
      "loss: 216382.375000  [    0/  787]\n",
      "loss: 216348.406250  [    0/  787]\n",
      "loss: 215484.062500  [    0/  787]\n",
      "loss: 213393.796875  [    0/  787]\n",
      "loss: 215874.765625  [    0/  787]\n",
      "loss: 215511.343750  [    0/  787]\n",
      "loss: 216147.937500  [    0/  787]\n",
      "loss: 216059.078125  [    0/  787]\n",
      "loss: 214221.812500  [    0/  787]\n",
      "loss: 215729.718750  [    0/  787]\n",
      "loss: 213883.109375  [    0/  787]\n",
      "loss: 214592.328125  [    0/  787]\n",
      "loss: 213022.671875  [    0/  787]\n",
      "loss: 213586.156250  [    0/  787]\n",
      "loss: 212885.093750  [    0/  787]\n",
      "loss: 212054.031250  [    0/  787]\n",
      "loss: 212188.625000  [    0/  787]\n",
      "loss: 211287.046875  [    0/  787]\n",
      "loss: 212073.546875  [    0/  787]\n",
      "loss: 211304.968750  [    0/  787]\n",
      "loss: 211533.500000  [    0/  787]\n",
      "loss: 210863.156250  [    0/  787]\n",
      "loss: 211224.687500  [    0/  787]\n",
      "loss: 210711.406250  [    0/  787]\n",
      "loss: 210713.750000  [    0/  787]\n",
      "loss: 211142.937500  [    0/  787]\n",
      "loss: 209662.218750  [    0/  787]\n",
      "loss: 210380.781250  [    0/  787]\n",
      "loss: 209140.468750  [    0/  787]\n",
      "loss: 210127.765625  [    0/  787]\n",
      "loss: 208765.937500  [    0/  787]\n",
      "loss: 208630.187500  [    0/  787]\n",
      "loss: 208476.750000  [    0/  787]\n",
      "loss: 208020.890625  [    0/  787]\n",
      "loss: 207280.468750  [    0/  787]\n",
      "loss: 207780.390625  [    0/  787]\n",
      "loss: 207376.593750  [    0/  787]\n",
      "loss: 207325.687500  [    0/  787]\n",
      "loss: 206675.250000  [    0/  787]\n",
      "loss: 206637.937500  [    0/  787]\n",
      "loss: 206616.140625  [    0/  787]\n",
      "loss: 205710.906250  [    0/  787]\n",
      "loss: 205516.875000  [    0/  787]\n",
      "loss: 205897.250000  [    0/  787]\n",
      "loss: 205173.515625  [    0/  787]\n",
      "loss: 205871.375000  [    0/  787]\n",
      "loss: 205147.343750  [    0/  787]\n",
      "loss: 204928.078125  [    0/  787]\n",
      "loss: 204599.265625  [    0/  787]\n",
      "loss: 204806.062500  [    0/  787]\n",
      "loss: 204017.125000  [    0/  787]\n",
      "loss: 203768.421875  [    0/  787]\n",
      "loss: 203462.109375  [    0/  787]\n",
      "loss: 203317.250000  [    0/  787]\n",
      "loss: 202676.765625  [    0/  787]\n",
      "loss: 203831.750000  [    0/  787]\n",
      "loss: 203175.000000  [    0/  787]\n",
      "loss: 202624.343750  [    0/  787]\n",
      "loss: 202415.250000  [    0/  787]\n",
      "loss: 202398.265625  [    0/  787]\n",
      "loss: 202212.062500  [    0/  787]\n",
      "loss: 202079.187500  [    0/  787]\n",
      "loss: 201917.718750  [    0/  787]\n",
      "loss: 201582.531250  [    0/  787]\n",
      "loss: 201092.937500  [    0/  787]\n",
      "loss: 201220.687500  [    0/  787]\n",
      "loss: 200838.250000  [    0/  787]\n",
      "loss: 201109.109375  [    0/  787]\n",
      "loss: 200763.187500  [    0/  787]\n",
      "loss: 200541.140625  [    0/  787]\n",
      "loss: 200537.953125  [    0/  787]\n",
      "loss: 199857.468750  [    0/  787]\n",
      "loss: 199943.140625  [    0/  787]\n",
      "loss: 199839.187500  [    0/  787]\n",
      "loss: 199379.812500  [    0/  787]\n",
      "loss: 199858.343750  [    0/  787]\n",
      "loss: 199276.718750  [    0/  787]\n",
      "loss: 199610.125000  [    0/  787]\n",
      "loss: 199633.328125  [    0/  787]\n",
      "loss: 199337.796875  [    0/  787]\n",
      "loss: 199110.500000  [    0/  787]\n",
      "loss: 198872.640625  [    0/  787]\n",
      "loss: 198746.156250  [    0/  787]\n",
      "loss: 198837.343750  [    0/  787]\n",
      "loss: 198701.062500  [    0/  787]\n",
      "loss: 198036.406250  [    0/  787]\n",
      "loss: 197272.406250  [    0/  787]\n",
      "loss: 198692.406250  [    0/  787]\n",
      "loss: 197716.312500  [    0/  787]\n",
      "loss: 197432.687500  [    0/  787]\n",
      "loss: 197735.906250  [    0/  787]\n",
      "loss: 197097.000000  [    0/  787]\n",
      "loss: 197479.171875  [    0/  787]\n",
      "loss: 197138.937500  [    0/  787]\n",
      "loss: 196542.046875  [    0/  787]\n",
      "loss: 197203.718750  [    0/  787]\n",
      "loss: 196535.765625  [    0/  787]\n",
      "loss: 197028.796875  [    0/  787]\n",
      "loss: 196289.593750  [    0/  787]\n",
      "loss: 196362.687500  [    0/  787]\n",
      "loss: 196921.187500  [    0/  787]\n",
      "loss: 195733.812500  [    0/  787]\n",
      "loss: 195268.156250  [    0/  787]\n",
      "loss: 196145.312500  [    0/  787]\n",
      "loss: 195377.359375  [    0/  787]\n",
      "loss: 195586.234375  [    0/  787]\n",
      "loss: 194898.062500  [    0/  787]\n",
      "loss: 195102.609375  [    0/  787]\n",
      "loss: 195482.375000  [    0/  787]\n",
      "loss: 195310.000000  [    0/  787]\n",
      "loss: 195310.375000  [    0/  787]\n",
      "loss: 194848.375000  [    0/  787]\n",
      "loss: 194695.125000  [    0/  787]\n",
      "loss: 194916.406250  [    0/  787]\n",
      "loss: 194832.390625  [    0/  787]\n",
      "loss: 194113.078125  [    0/  787]\n",
      "loss: 194634.046875  [    0/  787]\n",
      "loss: 194326.906250  [    0/  787]\n",
      "loss: 193535.031250  [    0/  787]\n",
      "loss: 194496.218750  [    0/  787]\n",
      "loss: 193700.437500  [    0/  787]\n",
      "loss: 194579.890625  [    0/  787]\n",
      "loss: 193582.468750  [    0/  787]\n",
      "loss: 193359.250000  [    0/  787]\n",
      "loss: 194179.812500  [    0/  787]\n",
      "loss: 193337.515625  [    0/  787]\n",
      "loss: 193408.515625  [    0/  787]\n",
      "loss: 194035.234375  [    0/  787]\n",
      "loss: 192956.671875  [    0/  787]\n",
      "loss: 193648.203125  [    0/  787]\n",
      "loss: 193374.375000  [    0/  787]\n",
      "loss: 193330.968750  [    0/  787]\n",
      "loss: 193552.437500  [    0/  787]\n",
      "loss: 193021.109375  [    0/  787]\n",
      "loss: 193669.187500  [    0/  787]\n",
      "loss: 192962.468750  [    0/  787]\n",
      "loss: 192660.656250  [    0/  787]\n",
      "loss: 193495.671875  [    0/  787]\n",
      "loss: 192633.156250  [    0/  787]\n",
      "loss: 193409.750000  [    0/  787]\n",
      "loss: 192489.625000  [    0/  787]\n",
      "loss: 191850.500000  [    0/  787]\n",
      "loss: 192546.625000  [    0/  787]\n",
      "loss: 192014.093750  [    0/  787]\n",
      "loss: 192850.687500  [    0/  787]\n",
      "loss: 192293.828125  [    0/  787]\n",
      "loss: 191788.859375  [    0/  787]\n",
      "loss: 192587.937500  [    0/  787]\n",
      "loss: 191758.390625  [    0/  787]\n",
      "loss: 192553.375000  [    0/  787]\n",
      "loss: 191872.109375  [    0/  787]\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "learning_rate = .0001\n",
    "batch_size = 64\n",
    "epochs = 500\n",
    "loss_fn = nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# print(list(model.parameters()))\n",
    "for t in range(epochs):\n",
    "    # print(f\"Epoch {t+1}\\n-------------------------------\")\n",
    "    train_loop(train_dataloader, model, loss_fn, optimizer,t)\n",
    "    # test_loop(test_dataloader, model, loss_fn)\n",
    "\n",
    "print(\"Done!\")\n",
    "# print(list(model.parameters()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(-62.8506)\n",
      "tensor(-322.9737)\n",
      "tensor(-316.4032)\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    model.eval()\n",
    "    yearLow = 2013\n",
    "    yearHigh = 2020\n",
    "    \n",
    "    for i in range(90,110):\n",
    "        feat = test_data[i:i+1][0].numpy()\n",
    "        pred = model(test_data[i:i+1][0]).numpy()\n",
    "        act = test_data[i:i+1][1].numpy()\n",
    "        full_back = data[i-10:i+5]\n",
    "        print(full_back)\n",
    "        print(feat)\n",
    "        print(pred)\n",
    "        print(act)\n",
    "        # plt.plot(full_back)\n",
    "        plt.plot(np.append(full_back,pred))\n",
    "        plt.plot(np.append(full_back,act))\n",
    "        plt.show()\n",
    "\n",
    "def test_loop(dataloader, model):\n",
    "    df = pd.read_csv(\"data\\FluViewPhase2Data\\WHO_NREVSS_Combined_prior_to_2015_16.csv\")\n",
    "    df = df[(yearLow <= df[\"YEAR\"]) & (df[\"YEAR\"] < yearHigh)][\"TOTAL\"]\n",
    "    data = np.array(df)\n",
    "\n",
    "    size = len(dataloader.dataset)\n",
    "    num_batches = len(dataloader)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for X, y in dataloader:\n",
    "            X = X[:,:,None]\n",
    "            pred = model(X).squeeze().numpy()\n",
    "            act = y.numpy()\n",
    "            \n",
    "\n",
    "test_loop(test_dataloader,model)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "a148e496c0f49d57628151d2aab378855c5a8a7aaacdf2673cbe18e166795068"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
