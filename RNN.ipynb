{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 340,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets\n",
    "from torchvision.transforms import ToTensor\n",
    "from torch.utils.data import Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 341,
   "metadata": {},
   "outputs": [],
   "source": [
    "#torch data set\n",
    "class dataSetAll(Dataset):\n",
    "    def __init__(self, yearLow, yearHigh,numFeat,numOut):\n",
    "        #import data from CDC\n",
    "        self.df = pd.read_csv(\"data\\FluViewPhase2Data\\WHO_NREVSS_Combined_prior_to_2015_16.csv\")\n",
    "        self.df = self.df[(yearLow <= self.df[\"YEAR\"]) & (self.df[\"YEAR\"] < yearHigh)][\"TOTAL\"]\n",
    "        #turn data into features and output\n",
    "        #features: 5 previous + one from last year for predicted\n",
    "        #output: prediction for next time\n",
    "\n",
    "        #create test data\n",
    "        self.numFeat = numFeat #------------------------\n",
    "        self.numOut = numOut\n",
    "        self.data = np.asarray(self.df,dtype=np.float32)\n",
    "        self.data = torch.as_tensor(self.data)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)-self.numFeat-self.numOut\n",
    "    \n",
    "    def __getitem__(self,idx):\n",
    "        return self.data[idx:idx+self.numFeat],self.data[idx+self.numFeat:idx+self.numFeat+self.numOut]\n",
    "        # return self.data[idx:idx+self.numFeat],self.data[idx:idx+self.numFeat+self.numOut]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 342,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create data loaders\n",
    "numFeat = 10\n",
    "numOut = 1\n",
    "train_data = dataSetAll(1900,2013,numFeat,numOut)\n",
    "test_data = dataSetAll(2013,2100,numFeat,numOut)\n",
    "train_dataloader = DataLoader(train_data, batch_size=100,drop_last=True)\n",
    "test_dataloader = DataLoader(test_data, batch_size=100,drop_last=True)\n",
    "# print(train_data[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 343,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create our RNN based network with an RNN followed by a linear layer\n",
    "inputSize = 1\n",
    "sequenceLength = numFeat\n",
    "numLayers = 1\n",
    "hiddenSize = 64\n",
    "batchSize = 100\n",
    "\n",
    "class RNN(nn.Module):\n",
    "    def __init__(self, inputSize, hiddenSize, numLayers, numOut, sequenceLength=1,future=0):\n",
    "        super(RNN, self).__init__()\n",
    "        self.numOut = numOut\n",
    "        self.inputSize = inputSize\n",
    "        self.hiddenSize = hiddenSize\n",
    "        self.numLayers = numLayers\n",
    "        self.future = future\n",
    "        # self.RNN = nn.RNN(inputSize,hiddenSize,numLayers,nonlinearity='relu',batch_first=True)\n",
    "        self.RNN1 = nn.RNNCell(inputSize,hiddenSize,nonlinearity='relu')\n",
    "        self.fc = nn.Linear(hiddenSize,numOut)\n",
    "        \n",
    "    # def forward(self,x):\n",
    "    #     h0 = torch.zeros(self.numLayers,x.size(0),self.hiddenSize)\n",
    "    #     out, _ = self.RNN(x,h0)\n",
    "    #     out = self.fc(out[:,-1,:])\n",
    "    #     return out\n",
    "\n",
    "    def forward(self,x):\n",
    "        outputs = []\n",
    "        nSamples = x.size(0)\n",
    "        # print(nSamples,self.hiddenSize)\n",
    "        h_1 = torch.zeros(nSamples, self.hiddenSize, dtype=torch.float32)\n",
    "        \n",
    "        for input in x.split(1,dim=1):\n",
    "            h_1 = self.RNN1(input, h_1)\n",
    "            out = self.fc(h_1)\n",
    "            # outputs.append(out)\n",
    "\n",
    "        # for i in range(self.future):\n",
    "        #     h_1 = self.RNN1(input, h_1)\n",
    "        #     out = self.fc(h_1)\n",
    "        #     outputs.append(out)\n",
    "        \n",
    "        # outputs = torch.cat(outputs, dim=1)\n",
    "        # return outputs[-1]\n",
    "        return out\n",
    "\n",
    "model = RNN(inputSize,hiddenSize,numLayers,numOut)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 344,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train/test loop\n",
    "def train_loop(dataloader, model, loss_fn, optimizer,t):\n",
    "    size = len(dataloader.dataset)\n",
    "    for batch, (X, y) in enumerate(dataloader):\n",
    "        # Compute prediction and loss\n",
    "        # X = X[:,:,None]\n",
    "        # print(X.size())\n",
    "        pred = model(X)\n",
    "        loss = loss_fn(pred, y)\n",
    "        # print(\"pred\",pred)\n",
    "        # print(\"Y\",y)\n",
    "        # Backpropagation\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "\n",
    "        if batch % size == 0:\n",
    "            # loss, current = loss.item(), batch * len(X)\n",
    "            print(f\"loss({t}): {loss.item():>7f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 345,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss(0): 7550898.500000\n",
      "loss(1): 7451178.000000\n",
      "loss(2): 7375723.000000\n",
      "loss(3): 7298963.000000\n",
      "loss(4): 7221334.000000\n",
      "loss(5): 7142528.500000\n",
      "loss(6): 7062160.000000\n",
      "loss(7): 6979878.500000\n",
      "loss(8): 6895542.000000\n",
      "loss(9): 6808377.500000\n",
      "loss(10): 6717299.000000\n",
      "loss(11): 6617766.000000\n",
      "loss(12): 6507741.500000\n",
      "loss(13): 6386799.500000\n",
      "loss(14): 6253005.000000\n",
      "loss(15): 6106387.000000\n",
      "loss(16): 5945782.000000\n",
      "loss(17): 5767291.000000\n",
      "loss(18): 5561947.000000\n",
      "loss(19): 5328811.000000\n",
      "loss(20): 5070036.000000\n",
      "loss(21): 4785508.500000\n",
      "loss(22): 4475346.000000\n",
      "loss(23): 4140295.000000\n",
      "loss(24): 3786105.500000\n",
      "loss(25): 3425424.750000\n",
      "loss(26): 3075440.750000\n",
      "loss(27): 2760994.250000\n",
      "loss(28): 2506714.250000\n",
      "loss(29): 2313675.750000\n",
      "loss(30): 2161086.000000\n",
      "loss(31): 2033894.750000\n",
      "loss(32): 1921984.375000\n",
      "loss(33): 1822490.750000\n",
      "loss(34): 1741855.250000\n",
      "loss(35): 1677597.625000\n",
      "loss(36): 1622947.625000\n",
      "loss(37): 1572960.000000\n",
      "loss(38): 1524472.375000\n",
      "loss(39): 1476872.375000\n",
      "loss(40): 1431497.250000\n",
      "loss(41): 1389615.250000\n",
      "loss(42): 1350615.500000\n",
      "loss(43): 1312270.375000\n",
      "loss(44): 1273744.125000\n",
      "loss(45): 1234370.125000\n",
      "loss(46): 1192970.500000\n",
      "loss(47): 1151483.500000\n",
      "loss(48): 1113313.875000\n",
      "loss(49): 1081877.000000\n",
      "loss(50): 1055307.500000\n",
      "loss(51): 1031691.625000\n",
      "loss(52): 1012459.687500\n",
      "loss(53): 995420.500000\n",
      "loss(54): 980679.062500\n",
      "loss(55): 967573.687500\n",
      "loss(56): 955993.125000\n",
      "loss(57): 944838.000000\n",
      "loss(58): 935051.500000\n",
      "loss(59): 926491.750000\n",
      "loss(60): 918257.750000\n",
      "loss(61): 909562.000000\n",
      "loss(62): 901083.250000\n",
      "loss(63): 892631.250000\n",
      "loss(64): 885690.500000\n",
      "loss(65): 879798.500000\n",
      "loss(66): 873642.750000\n",
      "loss(67): 867869.625000\n",
      "loss(68): 862541.125000\n",
      "loss(69): 857233.437500\n",
      "loss(70): 851986.187500\n",
      "loss(71): 847512.812500\n",
      "loss(72): 843123.500000\n",
      "loss(73): 838396.250000\n",
      "loss(74): 833839.062500\n",
      "loss(75): 829575.687500\n",
      "loss(76): 825225.500000\n",
      "loss(77): 820998.312500\n",
      "loss(78): 816742.312500\n",
      "loss(79): 812359.750000\n",
      "loss(80): 808338.187500\n",
      "loss(81): 804148.500000\n",
      "loss(82): 800146.500000\n",
      "loss(83): 795869.812500\n",
      "loss(84): 792338.000000\n",
      "loss(85): 787708.500000\n",
      "loss(86): 782427.500000\n",
      "loss(87): 777685.125000\n",
      "loss(88): 773178.187500\n",
      "loss(89): 768529.250000\n",
      "loss(90): 764248.937500\n",
      "loss(91): 759973.437500\n",
      "loss(92): 755600.000000\n",
      "loss(93): 751366.187500\n",
      "loss(94): 747354.062500\n",
      "loss(95): 743255.812500\n",
      "loss(96): 739211.250000\n",
      "loss(97): 734852.812500\n",
      "loss(98): 730398.750000\n",
      "loss(99): 726563.750000\n",
      "loss(100): 722345.125000\n",
      "loss(101): 718292.750000\n",
      "loss(102): 714354.937500\n",
      "loss(103): 710350.875000\n",
      "loss(104): 706419.375000\n",
      "loss(105): 702788.937500\n",
      "loss(106): 699125.437500\n",
      "loss(107): 695100.375000\n",
      "loss(108): 691458.875000\n",
      "loss(109): 688065.937500\n",
      "loss(110): 684429.250000\n",
      "loss(111): 680708.500000\n",
      "loss(112): 677480.875000\n",
      "loss(113): 673839.750000\n",
      "loss(114): 670414.187500\n",
      "loss(115): 666838.937500\n",
      "loss(116): 663455.687500\n",
      "loss(117): 660171.437500\n",
      "loss(118): 656785.375000\n",
      "loss(119): 653253.500000\n",
      "loss(120): 650069.750000\n",
      "loss(121): 647013.250000\n",
      "loss(122): 643504.187500\n",
      "loss(123): 640257.812500\n",
      "loss(124): 637288.312500\n",
      "loss(125): 633882.062500\n",
      "loss(126): 631166.375000\n",
      "loss(127): 627723.250000\n",
      "loss(128): 624711.875000\n",
      "loss(129): 621795.062500\n",
      "loss(130): 618682.875000\n",
      "loss(131): 615450.187500\n",
      "loss(132): 612512.812500\n",
      "loss(133): 609749.812500\n",
      "loss(134): 606741.000000\n",
      "loss(135): 603828.875000\n",
      "loss(136): 600856.062500\n",
      "loss(137): 597990.812500\n",
      "loss(138): 595041.812500\n",
      "loss(139): 592211.625000\n",
      "loss(140): 589524.750000\n",
      "loss(141): 586564.062500\n",
      "loss(142): 583856.187500\n",
      "loss(143): 581189.062500\n",
      "loss(144): 578347.812500\n",
      "loss(145): 575884.187500\n",
      "loss(146): 573062.187500\n",
      "loss(147): 570595.937500\n",
      "loss(148): 567882.937500\n",
      "loss(149): 565270.250000\n",
      "loss(150): 563044.375000\n",
      "loss(151): 560350.687500\n",
      "loss(152): 557768.562500\n",
      "loss(153): 555378.937500\n",
      "loss(154): 552662.375000\n",
      "loss(155): 550214.937500\n",
      "loss(156): 547936.937500\n",
      "loss(157): 545478.562500\n",
      "loss(158): 542848.937500\n",
      "loss(159): 540881.000000\n",
      "loss(160): 538211.812500\n",
      "loss(161): 536062.562500\n",
      "loss(162): 534052.250000\n",
      "loss(163): 531415.562500\n",
      "loss(164): 529102.187500\n",
      "loss(165): 526957.750000\n",
      "loss(166): 524684.250000\n",
      "loss(167): 522370.406250\n",
      "loss(168): 520186.437500\n",
      "loss(169): 517635.875000\n",
      "loss(170): 515371.968750\n",
      "loss(171): 513529.875000\n",
      "loss(172): 511624.156250\n",
      "loss(173): 509401.375000\n",
      "loss(174): 507248.968750\n",
      "loss(175): 505279.906250\n",
      "loss(176): 502961.031250\n",
      "loss(177): 500801.187500\n",
      "loss(178): 499059.250000\n",
      "loss(179): 496788.968750\n",
      "loss(180): 494466.250000\n",
      "loss(181): 492524.687500\n",
      "loss(182): 490392.968750\n",
      "loss(183): 488323.812500\n",
      "loss(184): 486524.187500\n",
      "loss(185): 484391.687500\n",
      "loss(186): 482469.593750\n",
      "loss(187): 480169.968750\n",
      "loss(188): 478479.000000\n",
      "loss(189): 476641.187500\n",
      "loss(190): 474650.843750\n",
      "loss(191): 472706.437500\n",
      "loss(192): 470900.562500\n",
      "loss(193): 468915.156250\n",
      "loss(194): 467189.531250\n",
      "loss(195): 465478.406250\n",
      "loss(196): 463939.968750\n",
      "loss(197): 461967.718750\n",
      "loss(198): 459846.875000\n",
      "loss(199): 458179.750000\n",
      "loss(200): 456535.625000\n",
      "loss(201): 455083.906250\n",
      "loss(202): 453507.593750\n",
      "loss(203): 451697.187500\n",
      "loss(204): 450040.281250\n",
      "loss(205): 448512.000000\n",
      "loss(206): 447063.000000\n",
      "loss(207): 445375.906250\n",
      "loss(208): 443799.156250\n",
      "loss(209): 442535.593750\n",
      "loss(210): 441024.687500\n",
      "loss(211): 439287.531250\n",
      "loss(212): 438326.375000\n",
      "loss(213): 436858.625000\n",
      "loss(214): 435448.875000\n",
      "loss(215): 433994.312500\n",
      "loss(216): 432633.531250\n",
      "loss(217): 431363.968750\n",
      "loss(218): 429590.093750\n",
      "loss(219): 428764.156250\n",
      "loss(220): 427651.531250\n",
      "loss(221): 425396.562500\n",
      "loss(222): 424477.468750\n",
      "loss(223): 423977.250000\n",
      "loss(224): 422349.281250\n",
      "loss(225): 421027.000000\n",
      "loss(226): 419993.687500\n",
      "loss(227): 418177.593750\n",
      "loss(228): 416734.718750\n",
      "loss(229): 415587.406250\n",
      "loss(230): 413981.468750\n",
      "loss(231): 412856.812500\n",
      "loss(232): 411946.437500\n",
      "loss(233): 410846.468750\n",
      "loss(234): 409804.875000\n",
      "loss(235): 408635.718750\n",
      "loss(236): 407222.812500\n",
      "loss(237): 406090.281250\n",
      "loss(238): 405008.156250\n",
      "loss(239): 404048.093750\n",
      "loss(240): 402834.375000\n",
      "loss(241): 402051.843750\n",
      "loss(242): 400933.437500\n",
      "loss(243): 400081.562500\n",
      "loss(244): 399393.031250\n",
      "loss(245): 398204.375000\n",
      "loss(246): 397009.250000\n",
      "loss(247): 396202.750000\n",
      "loss(248): 395282.687500\n",
      "loss(249): 394128.906250\n",
      "loss(250): 394031.000000\n",
      "loss(251): 393109.812500\n",
      "loss(252): 391575.125000\n",
      "loss(253): 390516.906250\n",
      "loss(254): 390050.906250\n",
      "loss(255): 389148.843750\n",
      "loss(256): 387826.687500\n",
      "loss(257): 387607.375000\n",
      "loss(258): 386535.968750\n",
      "loss(259): 385371.000000\n",
      "loss(260): 385068.093750\n",
      "loss(261): 384233.750000\n",
      "loss(262): 383161.437500\n",
      "loss(263): 382145.406250\n",
      "loss(264): 381810.000000\n",
      "loss(265): 381014.312500\n",
      "loss(266): 380082.281250\n",
      "loss(267): 379658.187500\n",
      "loss(268): 378578.593750\n",
      "loss(269): 377798.156250\n",
      "loss(270): 377480.093750\n",
      "loss(271): 376260.875000\n",
      "loss(272): 374948.687500\n",
      "loss(273): 374652.156250\n",
      "loss(274): 374262.406250\n",
      "loss(275): 372759.531250\n",
      "loss(276): 372333.562500\n",
      "loss(277): 371295.843750\n",
      "loss(278): 370261.156250\n",
      "loss(279): 369804.093750\n",
      "loss(280): 368928.531250\n",
      "loss(281): 367872.906250\n",
      "loss(282): 367326.562500\n",
      "loss(283): 366530.250000\n",
      "loss(284): 366144.281250\n",
      "loss(285): 365129.750000\n",
      "loss(286): 363996.031250\n",
      "loss(287): 364030.843750\n",
      "loss(288): 363126.187500\n",
      "loss(289): 361859.843750\n",
      "loss(290): 361160.718750\n",
      "loss(291): 361232.906250\n",
      "loss(292): 360954.375000\n",
      "loss(293): 359368.093750\n",
      "loss(294): 358965.281250\n",
      "loss(295): 358399.687500\n",
      "loss(296): 356978.312500\n",
      "loss(297): 356712.437500\n",
      "loss(298): 355682.437500\n",
      "loss(299): 354886.250000\n",
      "loss(300): 354197.906250\n",
      "loss(301): 353870.031250\n",
      "loss(302): 353259.250000\n",
      "loss(303): 352038.906250\n",
      "loss(304): 351506.531250\n",
      "loss(305): 351503.093750\n",
      "loss(306): 350126.312500\n",
      "loss(307): 349239.812500\n",
      "loss(308): 349213.718750\n",
      "loss(309): 347943.875000\n",
      "loss(310): 346917.437500\n",
      "loss(311): 347116.250000\n",
      "loss(312): 346144.593750\n",
      "loss(313): 345300.000000\n",
      "loss(314): 344789.750000\n",
      "loss(315): 344733.750000\n",
      "loss(316): 343946.031250\n",
      "loss(317): 342909.156250\n",
      "loss(318): 342834.843750\n",
      "loss(319): 341991.531250\n",
      "loss(320): 341045.718750\n",
      "loss(321): 340716.687500\n",
      "loss(322): 340494.531250\n",
      "loss(323): 339755.031250\n",
      "loss(324): 339134.187500\n",
      "loss(325): 338241.000000\n",
      "loss(326): 337997.375000\n",
      "loss(327): 337791.375000\n",
      "loss(328): 336927.406250\n",
      "loss(329): 336690.093750\n",
      "loss(330): 335779.687500\n",
      "loss(331): 335007.906250\n",
      "loss(332): 335663.250000\n",
      "loss(333): 334641.281250\n",
      "loss(334): 333790.687500\n",
      "loss(335): 333706.812500\n",
      "loss(336): 332494.968750\n",
      "loss(337): 331249.656250\n",
      "loss(338): 331900.531250\n",
      "loss(339): 331361.031250\n",
      "loss(340): 330495.125000\n",
      "loss(341): 330237.812500\n",
      "loss(342): 329760.625000\n",
      "loss(343): 329071.562500\n",
      "loss(344): 328470.531250\n",
      "loss(345): 329039.906250\n",
      "loss(346): 328473.156250\n",
      "loss(347): 327884.187500\n",
      "loss(348): 328086.062500\n",
      "loss(349): 326981.250000\n",
      "loss(350): 326755.187500\n",
      "loss(351): 326885.250000\n",
      "loss(352): 325667.031250\n",
      "loss(353): 325352.406250\n",
      "loss(354): 325440.906250\n",
      "loss(355): 324647.687500\n",
      "loss(356): 323703.031250\n",
      "loss(357): 323049.031250\n",
      "loss(358): 322989.468750\n",
      "loss(359): 322001.468750\n",
      "loss(360): 321943.625000\n",
      "loss(361): 321174.843750\n",
      "loss(362): 320119.875000\n",
      "loss(363): 320485.750000\n",
      "loss(364): 320267.281250\n",
      "loss(365): 323336.468750\n",
      "loss(366): 321431.812500\n",
      "loss(367): 321795.437500\n",
      "loss(368): 319896.187500\n",
      "loss(369): 320539.218750\n",
      "loss(370): 318249.750000\n",
      "loss(371): 318611.906250\n",
      "loss(372): 317221.125000\n",
      "loss(373): 319135.531250\n",
      "loss(374): 316417.000000\n",
      "loss(375): 317810.625000\n",
      "loss(376): 316151.093750\n",
      "loss(377): 316896.750000\n",
      "loss(378): 314988.906250\n",
      "loss(379): 315819.843750\n",
      "loss(380): 313400.968750\n",
      "loss(381): 314415.312500\n",
      "loss(382): 313312.187500\n",
      "loss(383): 313920.437500\n",
      "loss(384): 312552.187500\n",
      "loss(385): 311970.437500\n",
      "loss(386): 311832.718750\n",
      "loss(387): 312156.906250\n",
      "loss(388): 311509.875000\n",
      "loss(389): 309579.156250\n",
      "loss(390): 310181.125000\n",
      "loss(391): 309615.156250\n",
      "loss(392): 310398.875000\n",
      "loss(393): 308074.812500\n",
      "loss(394): 308709.031250\n",
      "loss(395): 307309.406250\n",
      "loss(396): 308045.468750\n",
      "loss(397): 307654.343750\n",
      "loss(398): 308014.500000\n",
      "loss(399): 304854.312500\n",
      "loss(400): 306915.187500\n",
      "loss(401): 305036.593750\n",
      "loss(402): 306486.875000\n",
      "loss(403): 303959.500000\n",
      "loss(404): 305687.250000\n",
      "loss(405): 303891.843750\n",
      "loss(406): 304637.312500\n",
      "loss(407): 302743.406250\n",
      "loss(408): 303800.093750\n",
      "loss(409): 302301.875000\n",
      "loss(410): 303066.312500\n",
      "loss(411): 301818.750000\n",
      "loss(412): 301725.687500\n",
      "loss(413): 302396.093750\n",
      "loss(414): 301068.625000\n",
      "loss(415): 300985.031250\n",
      "loss(416): 300728.843750\n",
      "loss(417): 299717.875000\n",
      "loss(418): 299749.875000\n",
      "loss(419): 298817.281250\n",
      "loss(420): 300024.218750\n",
      "loss(421): 297913.031250\n",
      "loss(422): 299371.500000\n",
      "loss(423): 298593.812500\n",
      "loss(424): 299203.625000\n",
      "loss(425): 296923.687500\n",
      "loss(426): 297933.187500\n",
      "loss(427): 297672.250000\n",
      "loss(428): 297113.406250\n",
      "loss(429): 295450.937500\n",
      "loss(430): 297345.468750\n",
      "loss(431): 296346.000000\n",
      "loss(432): 295843.656250\n",
      "loss(433): 295315.218750\n",
      "loss(434): 294869.312500\n",
      "loss(435): 294910.000000\n",
      "loss(436): 294613.093750\n",
      "loss(437): 294176.875000\n",
      "loss(438): 293503.875000\n",
      "loss(439): 294546.812500\n",
      "loss(440): 293783.312500\n",
      "loss(441): 294882.343750\n",
      "loss(442): 291935.375000\n",
      "loss(443): 293222.968750\n",
      "loss(444): 292633.031250\n",
      "loss(445): 292729.906250\n",
      "loss(446): 291585.531250\n",
      "loss(447): 292615.687500\n",
      "loss(448): 291031.656250\n",
      "loss(449): 292037.656250\n",
      "loss(450): 291047.718750\n",
      "loss(451): 291205.406250\n",
      "loss(452): 291584.375000\n",
      "loss(453): 290296.875000\n",
      "loss(454): 290632.187500\n",
      "loss(455): 289586.812500\n",
      "loss(456): 290404.125000\n",
      "loss(457): 288407.468750\n",
      "loss(458): 289969.562500\n",
      "loss(459): 287986.031250\n",
      "loss(460): 290305.281250\n",
      "loss(461): 287990.031250\n",
      "loss(462): 288753.687500\n",
      "loss(463): 287701.187500\n",
      "loss(464): 288183.562500\n",
      "loss(465): 286990.375000\n",
      "loss(466): 287158.406250\n",
      "loss(467): 286812.843750\n",
      "loss(468): 287128.156250\n",
      "loss(469): 286072.156250\n",
      "loss(470): 287032.718750\n",
      "loss(471): 285902.656250\n",
      "loss(472): 286099.000000\n",
      "loss(473): 285346.531250\n",
      "loss(474): 286222.812500\n",
      "loss(475): 284565.875000\n",
      "loss(476): 286457.593750\n",
      "loss(477): 285209.000000\n",
      "loss(478): 286407.093750\n",
      "loss(479): 282885.812500\n",
      "loss(480): 285338.875000\n",
      "loss(481): 283129.218750\n",
      "loss(482): 284310.906250\n",
      "loss(483): 282485.625000\n",
      "loss(484): 283580.687500\n",
      "loss(485): 282506.406250\n",
      "loss(486): 283446.906250\n",
      "loss(487): 282225.625000\n",
      "loss(488): 282526.781250\n",
      "loss(489): 281807.062500\n",
      "loss(490): 282860.125000\n",
      "loss(491): 281057.906250\n",
      "loss(492): 281566.343750\n",
      "loss(493): 281326.406250\n",
      "loss(494): 281146.437500\n",
      "loss(495): 279861.312500\n",
      "loss(496): 280647.781250\n",
      "loss(497): 279443.406250\n",
      "loss(498): 280268.875000\n",
      "loss(499): 279331.031250\n",
      "loss(500): 280519.281250\n",
      "loss(501): 277997.906250\n",
      "loss(502): 280031.031250\n",
      "loss(503): 278121.468750\n",
      "loss(504): 279550.562500\n",
      "loss(505): 277307.187500\n",
      "loss(506): 279246.875000\n",
      "loss(507): 277506.906250\n",
      "loss(508): 277275.937500\n",
      "loss(509): 277005.250000\n",
      "loss(510): 277587.031250\n",
      "loss(511): 276540.750000\n",
      "loss(512): 277167.843750\n",
      "loss(513): 275543.093750\n",
      "loss(514): 276982.468750\n",
      "loss(515): 275552.125000\n",
      "loss(516): 277207.625000\n",
      "loss(517): 274974.687500\n",
      "loss(518): 275671.906250\n",
      "loss(519): 274053.750000\n",
      "loss(520): 275863.593750\n",
      "loss(521): 273274.125000\n",
      "loss(522): 275102.968750\n",
      "loss(523): 273038.625000\n",
      "loss(524): 274102.375000\n",
      "loss(525): 273158.406250\n",
      "loss(526): 274335.218750\n",
      "loss(527): 272006.906250\n",
      "loss(528): 273715.562500\n",
      "loss(529): 272450.875000\n",
      "loss(530): 272967.968750\n",
      "loss(531): 271187.843750\n",
      "loss(532): 272339.906250\n",
      "loss(533): 270576.031250\n",
      "loss(534): 271946.593750\n",
      "loss(535): 270831.531250\n",
      "loss(536): 270595.625000\n",
      "loss(537): 269195.375000\n",
      "loss(538): 270238.250000\n",
      "loss(539): 269003.187500\n",
      "loss(540): 269810.906250\n",
      "loss(541): 268331.468750\n",
      "loss(542): 269696.406250\n",
      "loss(543): 267747.937500\n",
      "loss(544): 268506.625000\n",
      "loss(545): 267598.125000\n",
      "loss(546): 268983.250000\n",
      "loss(547): 267228.656250\n",
      "loss(548): 268820.125000\n",
      "loss(549): 266494.468750\n",
      "loss(550): 266853.406250\n",
      "loss(551): 265779.593750\n",
      "loss(552): 266402.031250\n",
      "loss(553): 266262.968750\n",
      "loss(554): 266867.468750\n",
      "loss(555): 264036.281250\n",
      "loss(556): 266172.406250\n",
      "loss(557): 264133.187500\n",
      "loss(558): 265460.531250\n",
      "loss(559): 263396.875000\n",
      "loss(560): 265228.656250\n",
      "loss(561): 263765.187500\n",
      "loss(562): 263529.312500\n",
      "loss(563): 263351.812500\n",
      "loss(564): 265242.187500\n",
      "loss(565): 262642.906250\n",
      "loss(566): 263302.500000\n",
      "loss(567): 262477.125000\n",
      "loss(568): 262396.718750\n",
      "loss(569): 261386.000000\n",
      "loss(570): 262224.968750\n",
      "loss(571): 260385.843750\n",
      "loss(572): 260462.421875\n",
      "loss(573): 259236.000000\n",
      "loss(574): 261894.640625\n",
      "loss(575): 259068.453125\n",
      "loss(576): 262910.531250\n",
      "loss(577): 258045.062500\n",
      "loss(578): 262306.593750\n",
      "loss(579): 256472.406250\n",
      "loss(580): 262249.125000\n",
      "loss(581): 255818.265625\n",
      "loss(582): 264289.281250\n",
      "loss(583): 255778.703125\n",
      "loss(584): 262254.312500\n",
      "loss(585): 254529.421875\n",
      "loss(586): 262324.687500\n",
      "loss(587): 254926.234375\n",
      "loss(588): 259802.265625\n",
      "loss(589): 253447.296875\n",
      "loss(590): 260642.125000\n",
      "loss(591): 254688.484375\n",
      "loss(592): 258913.500000\n",
      "loss(593): 252833.640625\n",
      "loss(594): 257013.000000\n",
      "loss(595): 252634.875000\n",
      "loss(596): 256349.406250\n",
      "loss(597): 251440.921875\n",
      "loss(598): 256016.375000\n",
      "loss(599): 251922.203125\n",
      "loss(600): 255123.718750\n",
      "loss(601): 250951.796875\n",
      "loss(602): 254201.515625\n",
      "loss(603): 250634.562500\n",
      "loss(604): 254299.281250\n",
      "loss(605): 249303.625000\n",
      "loss(606): 253532.140625\n",
      "loss(607): 249035.359375\n",
      "loss(608): 252083.343750\n",
      "loss(609): 248764.921875\n",
      "loss(610): 252251.218750\n",
      "loss(611): 246834.703125\n",
      "loss(612): 250503.000000\n",
      "loss(613): 247684.000000\n",
      "loss(614): 249887.796875\n",
      "loss(615): 246126.000000\n",
      "loss(616): 249856.078125\n",
      "loss(617): 245509.375000\n",
      "loss(618): 250095.000000\n",
      "loss(619): 243982.281250\n",
      "loss(620): 249059.046875\n",
      "loss(621): 242881.062500\n",
      "loss(622): 248849.546875\n",
      "loss(623): 242277.687500\n",
      "loss(624): 250392.546875\n",
      "loss(625): 241266.484375\n",
      "loss(626): 249050.140625\n",
      "loss(627): 242069.203125\n",
      "loss(628): 246783.625000\n",
      "loss(629): 241376.546875\n",
      "loss(630): 245810.078125\n",
      "loss(631): 241744.359375\n",
      "loss(632): 243864.312500\n",
      "loss(633): 239878.843750\n",
      "loss(634): 242817.000000\n",
      "loss(635): 241103.062500\n",
      "loss(636): 242483.593750\n",
      "loss(637): 239974.484375\n",
      "loss(638): 241255.281250\n",
      "loss(639): 238779.656250\n",
      "loss(640): 241072.859375\n",
      "loss(641): 238165.687500\n",
      "loss(642): 241365.796875\n",
      "loss(643): 237114.796875\n",
      "loss(644): 239954.515625\n",
      "loss(645): 234843.875000\n",
      "loss(646): 240864.765625\n",
      "loss(647): 236263.156250\n",
      "loss(648): 240183.984375\n",
      "loss(649): 233614.156250\n",
      "loss(650): 237832.296875\n",
      "loss(651): 234977.656250\n",
      "loss(652): 237616.312500\n",
      "loss(653): 233266.515625\n",
      "loss(654): 235836.359375\n",
      "loss(655): 232741.359375\n",
      "loss(656): 234876.015625\n",
      "loss(657): 231214.921875\n",
      "loss(658): 235725.921875\n",
      "loss(659): 229933.015625\n",
      "loss(660): 232653.484375\n",
      "loss(661): 227936.078125\n",
      "loss(662): 235634.546875\n",
      "loss(663): 228378.125000\n",
      "loss(664): 233590.953125\n",
      "loss(665): 227682.093750\n",
      "loss(666): 234018.953125\n",
      "loss(667): 226471.234375\n",
      "loss(668): 231623.421875\n",
      "loss(669): 226251.218750\n",
      "loss(670): 229026.718750\n",
      "loss(671): 225037.875000\n",
      "loss(672): 229442.140625\n",
      "loss(673): 226171.734375\n",
      "loss(674): 226859.625000\n",
      "loss(675): 224292.593750\n",
      "loss(676): 226608.687500\n",
      "loss(677): 223654.187500\n",
      "loss(678): 224564.781250\n",
      "loss(679): 225416.203125\n",
      "loss(680): 223938.593750\n",
      "loss(681): 222702.593750\n",
      "loss(682): 222479.546875\n",
      "loss(683): 223752.718750\n",
      "loss(684): 222699.046875\n",
      "loss(685): 221397.265625\n",
      "loss(686): 222182.515625\n",
      "loss(687): 221712.843750\n",
      "loss(688): 219899.000000\n",
      "loss(689): 219735.312500\n",
      "loss(690): 219720.984375\n",
      "loss(691): 220487.703125\n",
      "loss(692): 219647.906250\n",
      "loss(693): 218364.906250\n",
      "loss(694): 219011.859375\n",
      "loss(695): 218055.375000\n",
      "loss(696): 216010.281250\n",
      "loss(697): 217029.484375\n",
      "loss(698): 217481.406250\n",
      "loss(699): 219509.046875\n",
      "loss(700): 215906.453125\n",
      "loss(701): 217117.640625\n",
      "loss(702): 214772.359375\n",
      "loss(703): 220282.046875\n",
      "loss(704): 213691.718750\n",
      "loss(705): 221940.484375\n",
      "loss(706): 210954.687500\n",
      "loss(707): 220460.578125\n",
      "loss(708): 210090.796875\n",
      "loss(709): 222135.984375\n",
      "loss(710): 210241.203125\n",
      "loss(711): 221172.859375\n",
      "loss(712): 209370.718750\n",
      "loss(713): 218174.406250\n",
      "loss(714): 209763.718750\n",
      "loss(715): 213452.937500\n",
      "loss(716): 208959.906250\n",
      "loss(717): 213215.156250\n",
      "loss(718): 211041.234375\n",
      "loss(719): 211655.875000\n",
      "loss(720): 209943.734375\n",
      "loss(721): 212212.546875\n",
      "loss(722): 209587.296875\n",
      "loss(723): 212484.062500\n",
      "loss(724): 208402.156250\n",
      "loss(725): 209420.875000\n",
      "loss(726): 205208.203125\n",
      "loss(727): 213270.046875\n",
      "loss(728): 207912.625000\n",
      "loss(729): 211018.703125\n",
      "loss(730): 205333.562500\n",
      "loss(731): 211197.937500\n",
      "loss(732): 205340.921875\n",
      "loss(733): 209822.796875\n",
      "loss(734): 206000.984375\n",
      "loss(735): 210557.906250\n",
      "loss(736): 204582.187500\n",
      "loss(737): 207420.718750\n",
      "loss(738): 202987.125000\n",
      "loss(739): 210506.953125\n",
      "loss(740): 204949.421875\n",
      "loss(741): 208287.203125\n",
      "loss(742): 204187.796875\n",
      "loss(743): 208906.625000\n",
      "loss(744): 202804.906250\n",
      "loss(745): 206620.343750\n",
      "loss(746): 203505.515625\n",
      "loss(747): 205054.437500\n",
      "loss(748): 202511.687500\n",
      "loss(749): 206581.578125\n",
      "loss(750): 203819.953125\n",
      "loss(751): 205785.625000\n",
      "loss(752): 203419.406250\n",
      "loss(753): 205484.156250\n",
      "loss(754): 201502.953125\n",
      "loss(755): 203900.093750\n",
      "loss(756): 203493.406250\n",
      "loss(757): 204595.937500\n",
      "loss(758): 202664.734375\n",
      "loss(759): 202899.296875\n",
      "loss(760): 201267.500000\n",
      "loss(761): 204570.796875\n",
      "loss(762): 200291.875000\n",
      "loss(763): 204855.156250\n",
      "loss(764): 199995.125000\n",
      "loss(765): 207961.656250\n",
      "loss(766): 199136.125000\n",
      "loss(767): 205812.625000\n",
      "loss(768): 197856.640625\n",
      "loss(769): 207601.375000\n",
      "loss(770): 199037.562500\n",
      "loss(771): 204625.187500\n",
      "loss(772): 198226.203125\n",
      "loss(773): 203893.359375\n",
      "loss(774): 198562.203125\n",
      "loss(775): 205734.796875\n",
      "loss(776): 197519.359375\n",
      "loss(777): 204035.546875\n",
      "loss(778): 198069.000000\n",
      "loss(779): 205547.234375\n",
      "loss(780): 197361.937500\n",
      "loss(781): 204007.406250\n",
      "loss(782): 197274.406250\n",
      "loss(783): 202863.625000\n",
      "loss(784): 195688.500000\n",
      "loss(785): 203660.156250\n",
      "loss(786): 195935.718750\n",
      "loss(787): 203433.312500\n",
      "loss(788): 197238.953125\n",
      "loss(789): 202445.921875\n",
      "loss(790): 197385.937500\n",
      "loss(791): 199637.578125\n",
      "loss(792): 196470.203125\n",
      "loss(793): 200806.437500\n",
      "loss(794): 196772.062500\n",
      "loss(795): 199084.578125\n",
      "loss(796): 195893.765625\n",
      "loss(797): 197002.406250\n",
      "loss(798): 197904.515625\n",
      "loss(799): 197934.734375\n",
      "loss(800): 197686.781250\n",
      "loss(801): 196127.578125\n",
      "loss(802): 195736.812500\n",
      "loss(803): 197240.500000\n",
      "loss(804): 196777.125000\n",
      "loss(805): 197319.437500\n",
      "loss(806): 197454.875000\n",
      "loss(807): 197316.640625\n",
      "loss(808): 195785.359375\n",
      "loss(809): 197715.125000\n",
      "loss(810): 193970.000000\n",
      "loss(811): 198217.703125\n",
      "loss(812): 194325.437500\n",
      "loss(813): 198091.703125\n",
      "loss(814): 195491.906250\n",
      "loss(815): 195001.562500\n",
      "loss(816): 194801.812500\n",
      "loss(817): 197683.921875\n",
      "loss(818): 195788.812500\n",
      "loss(819): 196647.000000\n",
      "loss(820): 194517.859375\n",
      "loss(821): 194184.140625\n",
      "loss(822): 195767.906250\n",
      "loss(823): 196380.843750\n",
      "loss(824): 195183.375000\n",
      "loss(825): 193862.000000\n",
      "loss(826): 194177.859375\n",
      "loss(827): 195506.937500\n",
      "loss(828): 195337.000000\n",
      "loss(829): 195125.593750\n",
      "loss(830): 193021.718750\n",
      "loss(831): 193774.500000\n",
      "loss(832): 196928.203125\n",
      "loss(833): 195043.281250\n",
      "loss(834): 194026.500000\n",
      "loss(835): 193582.296875\n",
      "loss(836): 194159.000000\n",
      "loss(837): 193344.125000\n",
      "loss(838): 196045.015625\n",
      "loss(839): 190963.125000\n",
      "loss(840): 195137.265625\n",
      "loss(841): 190981.484375\n",
      "loss(842): 194916.187500\n",
      "loss(843): 190563.593750\n",
      "loss(844): 196897.234375\n",
      "loss(845): 191436.625000\n",
      "loss(846): 196313.421875\n",
      "loss(847): 190578.718750\n",
      "loss(848): 197110.046875\n",
      "loss(849): 189220.593750\n",
      "loss(850): 195073.375000\n",
      "loss(851): 190488.484375\n",
      "loss(852): 197865.625000\n",
      "loss(853): 190625.562500\n",
      "loss(854): 196144.234375\n",
      "loss(855): 188398.687500\n",
      "loss(856): 194554.453125\n",
      "loss(857): 189368.312500\n",
      "loss(858): 195174.578125\n",
      "loss(859): 189201.515625\n",
      "loss(860): 193074.781250\n",
      "loss(861): 188167.156250\n",
      "loss(862): 193368.203125\n",
      "loss(863): 187513.062500\n",
      "loss(864): 194666.937500\n",
      "loss(865): 187737.781250\n",
      "loss(866): 197319.937500\n",
      "loss(867): 186199.703125\n",
      "loss(868): 195680.984375\n",
      "loss(869): 187141.234375\n",
      "loss(870): 192257.281250\n",
      "loss(871): 187575.000000\n",
      "loss(872): 193586.406250\n",
      "loss(873): 190195.875000\n",
      "loss(874): 190218.093750\n",
      "loss(875): 188878.281250\n",
      "loss(876): 191289.718750\n",
      "loss(877): 188359.484375\n",
      "loss(878): 192185.796875\n",
      "loss(879): 186302.703125\n",
      "loss(880): 192511.718750\n",
      "loss(881): 187067.093750\n",
      "loss(882): 190731.734375\n",
      "loss(883): 188408.875000\n",
      "loss(884): 188079.000000\n",
      "loss(885): 188381.437500\n",
      "loss(886): 188008.484375\n",
      "loss(887): 189143.812500\n",
      "loss(888): 189298.625000\n",
      "loss(889): 187332.578125\n",
      "loss(890): 191177.125000\n",
      "loss(891): 186692.562500\n",
      "loss(892): 189835.078125\n",
      "loss(893): 187398.875000\n",
      "loss(894): 192502.953125\n",
      "loss(895): 186832.656250\n",
      "loss(896): 188253.875000\n",
      "loss(897): 185690.578125\n",
      "loss(898): 188854.078125\n",
      "loss(899): 186587.937500\n",
      "loss(900): 190593.187500\n",
      "loss(901): 184389.875000\n",
      "loss(902): 191827.984375\n",
      "loss(903): 184154.937500\n",
      "loss(904): 189926.203125\n",
      "loss(905): 185768.781250\n",
      "loss(906): 185962.593750\n",
      "loss(907): 186429.593750\n",
      "loss(908): 186877.703125\n",
      "loss(909): 186676.000000\n",
      "loss(910): 188656.625000\n",
      "loss(911): 183584.843750\n",
      "loss(912): 187156.843750\n",
      "loss(913): 185216.515625\n",
      "loss(914): 187850.359375\n",
      "loss(915): 187395.500000\n",
      "loss(916): 184726.453125\n",
      "loss(917): 186588.140625\n",
      "loss(918): 184875.562500\n",
      "loss(919): 188206.421875\n",
      "loss(920): 185184.312500\n",
      "loss(921): 185456.093750\n",
      "loss(922): 185027.187500\n",
      "loss(923): 186044.515625\n",
      "loss(924): 184695.125000\n",
      "loss(925): 189507.593750\n",
      "loss(926): 184229.718750\n",
      "loss(927): 187331.484375\n",
      "loss(928): 183490.953125\n",
      "loss(929): 187757.437500\n",
      "loss(930): 185209.687500\n",
      "loss(931): 183292.281250\n",
      "loss(932): 185763.765625\n",
      "loss(933): 185177.484375\n",
      "loss(934): 185281.593750\n",
      "loss(935): 185152.937500\n",
      "loss(936): 182787.921875\n",
      "loss(937): 186031.093750\n",
      "loss(938): 184023.281250\n",
      "loss(939): 185419.484375\n",
      "loss(940): 184731.062500\n",
      "loss(941): 183073.562500\n",
      "loss(942): 185003.921875\n",
      "loss(943): 182817.515625\n",
      "loss(944): 185977.343750\n",
      "loss(945): 184145.843750\n",
      "loss(946): 182859.296875\n",
      "loss(947): 185563.578125\n",
      "loss(948): 182793.453125\n",
      "loss(949): 184229.984375\n",
      "loss(950): 183539.781250\n",
      "loss(951): 183644.796875\n",
      "loss(952): 185418.921875\n",
      "loss(953): 182273.718750\n",
      "loss(954): 185002.562500\n",
      "loss(955): 183898.562500\n",
      "loss(956): 184137.406250\n",
      "loss(957): 183213.125000\n",
      "loss(958): 183899.859375\n",
      "loss(959): 183836.718750\n",
      "loss(960): 186784.593750\n",
      "loss(961): 181067.562500\n",
      "loss(962): 185393.640625\n",
      "loss(963): 183048.234375\n",
      "loss(964): 187129.156250\n",
      "loss(965): 182960.203125\n",
      "loss(966): 183752.562500\n",
      "loss(967): 181940.218750\n",
      "loss(968): 184438.234375\n",
      "loss(969): 181766.000000\n",
      "loss(970): 185561.203125\n",
      "loss(971): 181133.515625\n",
      "loss(972): 186703.656250\n",
      "loss(973): 181395.484375\n",
      "loss(974): 185011.125000\n",
      "loss(975): 182953.437500\n",
      "loss(976): 182702.453125\n",
      "loss(977): 182536.812500\n",
      "loss(978): 182564.765625\n",
      "loss(979): 183383.015625\n",
      "loss(980): 183308.187500\n",
      "loss(981): 183089.859375\n",
      "loss(982): 186534.359375\n",
      "loss(983): 181148.718750\n",
      "loss(984): 185331.234375\n",
      "loss(985): 181844.015625\n",
      "loss(986): 182681.765625\n",
      "loss(987): 180890.718750\n",
      "loss(988): 183337.312500\n",
      "loss(989): 181216.796875\n",
      "loss(990): 184903.281250\n",
      "loss(991): 180680.062500\n",
      "loss(992): 188766.593750\n",
      "loss(993): 179723.125000\n",
      "loss(994): 188468.765625\n",
      "loss(995): 180098.281250\n",
      "loss(996): 182779.484375\n",
      "loss(997): 179650.203125\n",
      "loss(998): 183894.515625\n",
      "loss(999): 180387.015625\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "learning_rate = .0001\n",
    "epochs = 1000\n",
    "loss_fn = nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# print(list(model.parameters()))\n",
    "for t in range(epochs):\n",
    "    # print(f\"Epoch {t+1}\\n-------------------------------\")\n",
    "    train_loop(train_dataloader, model, loss_fn, optimizer,t)\n",
    "    # test_loop(test_dataloader, model, loss_fn)\n",
    "\n",
    "print(\"Done!\")\n",
    "# print(list(model.parameters()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 346,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\mburs\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\numpy\\core\\shape_base.py:65: FutureWarning: The input object of type 'Tensor' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Tensor', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.\n",
      "  ary = asanyarray(ary)\n",
      "c:\\Users\\mburs\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\numpy\\core\\shape_base.py:65: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  ary = asanyarray(ary)\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "def test_loop(dataloader, model):\n",
    "    yearLow = 2013\n",
    "    yearHigh = 2020\n",
    "    df = pd.read_csv(\"data\\FluViewPhase2Data\\WHO_NREVSS_Combined_prior_to_2015_16.csv\")\n",
    "    df = df[(yearLow <= df[\"YEAR\"]) & (df[\"YEAR\"] < yearHigh)][\"TOTAL\"]\n",
    "    data = np.array(df)\n",
    "    numFeat = 10 #------------------------\n",
    "    numOut = 1\n",
    "\n",
    "    size = len(data)-numFeat-numOut\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for idx in range(4,len(test_data)):\n",
    "            feat = test_data[idx][0]\n",
    "            y = test_data[idx][1]\n",
    "            X = feat[None,:,None]\n",
    "            pred = model(X).squeeze().numpy()\n",
    "            plt.plot(np.append(feat,pred))\n",
    "            plt.plot(np.append(feat,y))\n",
    "            \n",
    "            plt.show()\n",
    "\n",
    "def graph(model):\n",
    "    with torch.no_grad():\n",
    "        predY = []\n",
    "        actY = []\n",
    "        for idx in range(len(test_data)):\n",
    "            feat = test_data[idx][0]\n",
    "            y = test_data[idx][1]\n",
    "            X = feat[None,:]\n",
    "            # print(X.size())\n",
    "            pred = model(X).squeeze().numpy()\n",
    "            # print(pred[0])\n",
    "            predY.append(pred)\n",
    "            actY.append(y)\n",
    "            # plt.plot(np.append(feat,pred))\n",
    "            # plt.plot(np.append(feat,y))\n",
    "            \n",
    "        \n",
    "        plt.plot(actY) \n",
    "        plt.plot(predY)\n",
    "        plt.show()\n",
    "\n",
    "# test_loop(test_dataloader,model)\n",
    "graph(model)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "a148e496c0f49d57628151d2aab378855c5a8a7aaacdf2673cbe18e166795068"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
