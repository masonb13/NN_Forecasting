{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets\n",
    "from torchvision.transforms import ToTensor\n",
    "from torch.utils.data import Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#torch data set\n",
    "class dataSetAll(Dataset):\n",
    "    def __init__(self,numFeat,numOut, location):\n",
    "        #import data from CSV\n",
    "        df1 = pd.read_csv(\"data\\hosp\\hhs_data_weekly.csv\")\n",
    "        df1 = df1[(df1[\"location\"] == location) & (df1[\"yr\"]==2022) & (df1[\"week\"]<52)][\"weekly_hosps\"]\n",
    "\n",
    "        df2 = pd.read_csv(\"data\\hosp\\\\fluview_ili.csv\")\n",
    "        df2 = df2[(df2[\"location\"] == location) & (df2[\"Y\"]==2022)][[\"population\",\"numili\"]]\n",
    "\n",
    "        df1 = df1.reset_index()\n",
    "        df2 = df2.reset_index()\n",
    "\n",
    "        df3 = pd.concat([df1,df2],axis=1)[[\"weekly_hosps\",\"population\",\"numili\"]]\n",
    "\n",
    "        self.numFeat = numFeat #------------------------\n",
    "        self.numOut = numOut\n",
    "        \n",
    "        self.data = np.asarray(df3,dtype=np.float32)\n",
    "        # print(self.data)\n",
    "        self.norm = np.linalg.norm(self.data, axis=0)\n",
    "        # print(self.norm)\n",
    "        self.data = self.data / self.norm\n",
    "        self.data = torch.as_tensor(self.data)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)-self.numFeat-self.numOut\n",
    "    \n",
    "    def __getitem__(self,idx):\n",
    "        # idx = 0\n",
    "        return self.data[idx:idx+self.numFeat],self.data[idx+self.numFeat:idx+self.numFeat+self.numOut]\n",
    "    \n",
    "    def getNorm(self):\n",
    "        return self.norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create data loaders\n",
    "numFeat = 10\n",
    "numOut = 4\n",
    "batchSize = 64\n",
    "\n",
    "locations = [\"01\",\"02\",\"04\",\"05\",\"06\",\"08\",\"09\",\"10\",\"11\",\"12\",\"13\",\"15\",\"16\",\"17\",\"18\",\"19\",\"20\",\n",
    "            \"21\",\"22\",\"23\",\"24\",\"25\",\"26\",\"27\",\"28\",\"29\",\"30\"]\n",
    "\n",
    "datSetLoc = [dataSetAll(numFeat,numOut,loc) for loc in locations]\n",
    "\n",
    "train_data = torch.utils.data.ConcatDataset(datSetLoc)\n",
    "train_dataloader = DataLoader(train_data, batch_size=batchSize,drop_last=False,shuffle=True)\n",
    "\n",
    "test_data = dataSetAll(numFeat,numOut,\"US\")\n",
    "test_dataloader = DataLoader(test_data, batch_size=batchSize,drop_last=False,shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#test\n",
    "# pd.set_option('display.max_rows', 500)\n",
    "# df1 = pd.read_csv(\"data\\hosp\\hhs_data_weekly.csv\")\n",
    "# df1 = df1[(df1[\"location\"] == \"US\") & (df1[\"yr\"]==2022) & (df1[\"week\"]<52)][\"weekly_hosps\"]\n",
    "\n",
    "# df2 = pd.read_csv(\"data\\hosp\\\\fluview_ili.csv\")\n",
    "# df2 = df2[(df2[\"location\"] == \"US\") & (df2[\"Y\"]==2022)][[\"population\",\"numili\"]]\n",
    "\n",
    "# df1 = df1.reset_index()\n",
    "# df2 = df2.reset_index()\n",
    "\n",
    "# df3 = pd.concat([df1,df2],axis=1)[[\"weekly_hosps\",\"population\",\"numili\"]]\n",
    "# print(df3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create our RNN based network with an RNN followed by a linear layer\n",
    "inputSize = 3\n",
    "sequenceLength = numFeat\n",
    "numLayers = 1\n",
    "hiddenSize = 32\n",
    "\n",
    "class RNN(nn.Module):\n",
    "    def __init__(self,inputSize,hiddenSize,numLayers,numOut,sequenceLength,future=0):\n",
    "        super(RNN, self).__init__()\n",
    "        self.inputSize = inputSize\n",
    "        self.hiddenSize = hiddenSize\n",
    "        self.numLayers = numLayers\n",
    "        self.numOut = numOut\n",
    "        self.future = future\n",
    "        self.inputSize = inputSize\n",
    "        # print(batchSize,sequenceLength,inputSize)\n",
    "        # self.LSTM = nn.LSTM(inputSize,hiddenSize,numLayers,batch_first=True)\n",
    "        self.rnn1 = nn.RNNCell(inputSize,hiddenSize)\n",
    "        self.rnn2 = nn.RNNCell(inputSize,hiddenSize)\n",
    "        self.rnn3 = nn.RNNCell(inputSize,hiddenSize)\n",
    "        self.rnn4 = nn.RNNCell(inputSize,hiddenSize)\n",
    "        # self.rnn2 = nn.RNNCell(1,hiddenSize,nonlinearity='tanh')\n",
    "        self.fc1 = nn.Linear(hiddenSize,inputSize)\n",
    "        self.fc2 = nn.Linear(hiddenSize,inputSize)\n",
    "        self.fc3 = nn.Linear(hiddenSize,inputSize)\n",
    "        self.fc4 = nn.Linear(hiddenSize,inputSize)\n",
    "        \n",
    "    def forward(self,x):\n",
    "        nSamples = x.size(0)\n",
    "        outputs = torch.zeros([nSamples,self.numOut,self.inputSize])\n",
    "        h_1 = torch.zeros(nSamples, self.hiddenSize, dtype=torch.float32)\n",
    "        h_2 = torch.zeros(nSamples, self.hiddenSize, dtype=torch.float32)\n",
    "\n",
    "        for input in x.split(1,dim=1):\n",
    "            h_1 = self.rnn1(input[:,0,:], h_1)\n",
    "            out = self.fc1(h_1)\n",
    "        outputs[:,0] = out\n",
    "\n",
    "        h_1 = self.rnn2(out, h_1)\n",
    "        out = self.fc2(h_1)\n",
    "        outputs[:,1] = out\n",
    "\n",
    "        h_1 = self.rnn3(out, h_1)\n",
    "        out = self.fc3(h_1)\n",
    "        outputs[:,2] = out\n",
    "\n",
    "        h_1 = self.rnn4(out, h_1)\n",
    "        out = self.fc4(h_1)\n",
    "        outputs[:,3] = out\n",
    "\n",
    "        # outputs = torch.as_tensor(outputs)\n",
    "        return outputs\n",
    "\n",
    "model = RNN(inputSize,hiddenSize,numLayers,numOut,sequenceLength)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train/test loop\n",
    "def train_loop(dataloader, model, loss_fn, optimizer,t):\n",
    "    size = len(dataloader.dataset)\n",
    "    for batch, (X, y) in enumerate(dataloader):\n",
    "        # Compute prediction and loss\n",
    "        # print(X.size())\n",
    "        # X = X[:,:,None]\n",
    "        # print(X.size())\n",
    "        pred = model(X)\n",
    "        # break\n",
    "        # print(pred.size())\n",
    "        # print(y.size())\n",
    "        # print(\"pred\",pred.size())\n",
    "        # print(\"Y\",y.size())\n",
    "        loss = loss_fn(pred, y)\n",
    "        \n",
    "        # Backpropagation\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if batch % size == 0:\n",
    "            loss, current = loss.item(), batch * len(X)\n",
    "            print(f\"loss({t}): {loss:>7f}  [{current:>5d}/{size:>5d}]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss(0): 0.080794  [    0/  999]\n",
      "loss(1): 0.012430  [    0/  999]\n",
      "loss(2): 0.008211  [    0/  999]\n",
      "loss(3): 0.008317  [    0/  999]\n",
      "loss(4): 0.006074  [    0/  999]\n",
      "loss(5): 0.005907  [    0/  999]\n",
      "loss(6): 0.004049  [    0/  999]\n",
      "loss(7): 0.006589  [    0/  999]\n",
      "loss(8): 0.004689  [    0/  999]\n",
      "loss(9): 0.004796  [    0/  999]\n",
      "loss(10): 0.003752  [    0/  999]\n",
      "loss(11): 0.004158  [    0/  999]\n",
      "loss(12): 0.002355  [    0/  999]\n",
      "loss(13): 0.002851  [    0/  999]\n",
      "loss(14): 0.002153  [    0/  999]\n",
      "loss(15): 0.001571  [    0/  999]\n",
      "loss(16): 0.002882  [    0/  999]\n",
      "loss(17): 0.002189  [    0/  999]\n",
      "loss(18): 0.002564  [    0/  999]\n",
      "loss(19): 0.002957  [    0/  999]\n",
      "loss(20): 0.002805  [    0/  999]\n",
      "loss(21): 0.003584  [    0/  999]\n",
      "loss(22): 0.003432  [    0/  999]\n",
      "loss(23): 0.002501  [    0/  999]\n",
      "loss(24): 0.001879  [    0/  999]\n",
      "loss(25): 0.001439  [    0/  999]\n",
      "loss(26): 0.002049  [    0/  999]\n",
      "loss(27): 0.001930  [    0/  999]\n",
      "loss(28): 0.002793  [    0/  999]\n",
      "loss(29): 0.001505  [    0/  999]\n",
      "loss(30): 0.002068  [    0/  999]\n",
      "loss(31): 0.001884  [    0/  999]\n",
      "loss(32): 0.002300  [    0/  999]\n",
      "loss(33): 0.001268  [    0/  999]\n",
      "loss(34): 0.001243  [    0/  999]\n",
      "loss(35): 0.001771  [    0/  999]\n",
      "loss(36): 0.002379  [    0/  999]\n",
      "loss(37): 0.001891  [    0/  999]\n",
      "loss(38): 0.002137  [    0/  999]\n",
      "loss(39): 0.001300  [    0/  999]\n",
      "loss(40): 0.002779  [    0/  999]\n",
      "loss(41): 0.001894  [    0/  999]\n",
      "loss(42): 0.001991  [    0/  999]\n",
      "loss(43): 0.002326  [    0/  999]\n",
      "loss(44): 0.002346  [    0/  999]\n",
      "loss(45): 0.001774  [    0/  999]\n",
      "loss(46): 0.003145  [    0/  999]\n",
      "loss(47): 0.002917  [    0/  999]\n",
      "loss(48): 0.002168  [    0/  999]\n",
      "loss(49): 0.002059  [    0/  999]\n",
      "loss(50): 0.001762  [    0/  999]\n",
      "loss(51): 0.002278  [    0/  999]\n",
      "loss(52): 0.001479  [    0/  999]\n",
      "loss(53): 0.001896  [    0/  999]\n",
      "loss(54): 0.002276  [    0/  999]\n",
      "loss(55): 0.002464  [    0/  999]\n",
      "loss(56): 0.002327  [    0/  999]\n",
      "loss(57): 0.002359  [    0/  999]\n",
      "loss(58): 0.001463  [    0/  999]\n",
      "loss(59): 0.001644  [    0/  999]\n",
      "loss(60): 0.001849  [    0/  999]\n",
      "loss(61): 0.003035  [    0/  999]\n",
      "loss(62): 0.001237  [    0/  999]\n",
      "loss(63): 0.001935  [    0/  999]\n",
      "loss(64): 0.001863  [    0/  999]\n",
      "loss(65): 0.001987  [    0/  999]\n",
      "loss(66): 0.001593  [    0/  999]\n",
      "loss(67): 0.002022  [    0/  999]\n",
      "loss(68): 0.002004  [    0/  999]\n",
      "loss(69): 0.001978  [    0/  999]\n",
      "loss(70): 0.002237  [    0/  999]\n",
      "loss(71): 0.001616  [    0/  999]\n",
      "loss(72): 0.001972  [    0/  999]\n",
      "loss(73): 0.002249  [    0/  999]\n",
      "loss(74): 0.001579  [    0/  999]\n",
      "loss(75): 0.002237  [    0/  999]\n",
      "loss(76): 0.001561  [    0/  999]\n",
      "loss(77): 0.001889  [    0/  999]\n",
      "loss(78): 0.002046  [    0/  999]\n",
      "loss(79): 0.001620  [    0/  999]\n",
      "loss(80): 0.001537  [    0/  999]\n",
      "loss(81): 0.001586  [    0/  999]\n",
      "loss(82): 0.001894  [    0/  999]\n",
      "loss(83): 0.001590  [    0/  999]\n",
      "loss(84): 0.001953  [    0/  999]\n",
      "loss(85): 0.001945  [    0/  999]\n",
      "loss(86): 0.001490  [    0/  999]\n",
      "loss(87): 0.001448  [    0/  999]\n",
      "loss(88): 0.002216  [    0/  999]\n",
      "loss(89): 0.001794  [    0/  999]\n",
      "loss(90): 0.001273  [    0/  999]\n",
      "loss(91): 0.001726  [    0/  999]\n",
      "loss(92): 0.001747  [    0/  999]\n",
      "loss(93): 0.001218  [    0/  999]\n",
      "loss(94): 0.002079  [    0/  999]\n",
      "loss(95): 0.001582  [    0/  999]\n",
      "loss(96): 0.002230  [    0/  999]\n",
      "loss(97): 0.001407  [    0/  999]\n",
      "loss(98): 0.001912  [    0/  999]\n",
      "loss(99): 0.002309  [    0/  999]\n",
      "loss(100): 0.001194  [    0/  999]\n",
      "loss(101): 0.001586  [    0/  999]\n",
      "loss(102): 0.001583  [    0/  999]\n",
      "loss(103): 0.001361  [    0/  999]\n",
      "loss(104): 0.002293  [    0/  999]\n",
      "loss(105): 0.001106  [    0/  999]\n",
      "loss(106): 0.001730  [    0/  999]\n",
      "loss(107): 0.001839  [    0/  999]\n",
      "loss(108): 0.002137  [    0/  999]\n",
      "loss(109): 0.001872  [    0/  999]\n",
      "loss(110): 0.001498  [    0/  999]\n",
      "loss(111): 0.001509  [    0/  999]\n",
      "loss(112): 0.001989  [    0/  999]\n",
      "loss(113): 0.001541  [    0/  999]\n",
      "loss(114): 0.001320  [    0/  999]\n",
      "loss(115): 0.001075  [    0/  999]\n",
      "loss(116): 0.001890  [    0/  999]\n",
      "loss(117): 0.001175  [    0/  999]\n",
      "loss(118): 0.001632  [    0/  999]\n",
      "loss(119): 0.002985  [    0/  999]\n",
      "loss(120): 0.001797  [    0/  999]\n",
      "loss(121): 0.001973  [    0/  999]\n",
      "loss(122): 0.002141  [    0/  999]\n",
      "loss(123): 0.001911  [    0/  999]\n",
      "loss(124): 0.001724  [    0/  999]\n",
      "loss(125): 0.001932  [    0/  999]\n",
      "loss(126): 0.001975  [    0/  999]\n",
      "loss(127): 0.001848  [    0/  999]\n",
      "loss(128): 0.001676  [    0/  999]\n",
      "loss(129): 0.001924  [    0/  999]\n",
      "loss(130): 0.001112  [    0/  999]\n",
      "loss(131): 0.001584  [    0/  999]\n",
      "loss(132): 0.001443  [    0/  999]\n",
      "loss(133): 0.002101  [    0/  999]\n",
      "loss(134): 0.001785  [    0/  999]\n",
      "loss(135): 0.001425  [    0/  999]\n",
      "loss(136): 0.002063  [    0/  999]\n",
      "loss(137): 0.001732  [    0/  999]\n",
      "loss(138): 0.001436  [    0/  999]\n",
      "loss(139): 0.001753  [    0/  999]\n",
      "loss(140): 0.001674  [    0/  999]\n",
      "loss(141): 0.001020  [    0/  999]\n",
      "loss(142): 0.001337  [    0/  999]\n",
      "loss(143): 0.002465  [    0/  999]\n",
      "loss(144): 0.001988  [    0/  999]\n",
      "loss(145): 0.002364  [    0/  999]\n",
      "loss(146): 0.002514  [    0/  999]\n",
      "loss(147): 0.001910  [    0/  999]\n",
      "loss(148): 0.002161  [    0/  999]\n",
      "loss(149): 0.001389  [    0/  999]\n",
      "loss(150): 0.001692  [    0/  999]\n",
      "loss(151): 0.002181  [    0/  999]\n",
      "loss(152): 0.001026  [    0/  999]\n",
      "loss(153): 0.001676  [    0/  999]\n",
      "loss(154): 0.001753  [    0/  999]\n",
      "loss(155): 0.001902  [    0/  999]\n",
      "loss(156): 0.000975  [    0/  999]\n",
      "loss(157): 0.002064  [    0/  999]\n",
      "loss(158): 0.002158  [    0/  999]\n",
      "loss(159): 0.002342  [    0/  999]\n",
      "loss(160): 0.002149  [    0/  999]\n",
      "loss(161): 0.001242  [    0/  999]\n",
      "loss(162): 0.001215  [    0/  999]\n",
      "loss(163): 0.001839  [    0/  999]\n",
      "loss(164): 0.001296  [    0/  999]\n",
      "loss(165): 0.001783  [    0/  999]\n",
      "loss(166): 0.001331  [    0/  999]\n",
      "loss(167): 0.001253  [    0/  999]\n",
      "loss(168): 0.001930  [    0/  999]\n",
      "loss(169): 0.002162  [    0/  999]\n",
      "loss(170): 0.002325  [    0/  999]\n",
      "loss(171): 0.001829  [    0/  999]\n",
      "loss(172): 0.001804  [    0/  999]\n",
      "loss(173): 0.001241  [    0/  999]\n",
      "loss(174): 0.002080  [    0/  999]\n",
      "loss(175): 0.001654  [    0/  999]\n",
      "loss(176): 0.001889  [    0/  999]\n",
      "loss(177): 0.001370  [    0/  999]\n",
      "loss(178): 0.001042  [    0/  999]\n",
      "loss(179): 0.001566  [    0/  999]\n",
      "loss(180): 0.001478  [    0/  999]\n",
      "loss(181): 0.001575  [    0/  999]\n",
      "loss(182): 0.001267  [    0/  999]\n",
      "loss(183): 0.001737  [    0/  999]\n",
      "loss(184): 0.001514  [    0/  999]\n",
      "loss(185): 0.001075  [    0/  999]\n",
      "loss(186): 0.001897  [    0/  999]\n",
      "loss(187): 0.001943  [    0/  999]\n",
      "loss(188): 0.001616  [    0/  999]\n",
      "loss(189): 0.001451  [    0/  999]\n",
      "loss(190): 0.001883  [    0/  999]\n",
      "loss(191): 0.001342  [    0/  999]\n",
      "loss(192): 0.001144  [    0/  999]\n",
      "loss(193): 0.001406  [    0/  999]\n",
      "loss(194): 0.002085  [    0/  999]\n",
      "loss(195): 0.001654  [    0/  999]\n",
      "loss(196): 0.000866  [    0/  999]\n",
      "loss(197): 0.001462  [    0/  999]\n",
      "loss(198): 0.001802  [    0/  999]\n",
      "loss(199): 0.001429  [    0/  999]\n",
      "loss(200): 0.001102  [    0/  999]\n",
      "loss(201): 0.001121  [    0/  999]\n",
      "loss(202): 0.001770  [    0/  999]\n",
      "loss(203): 0.001490  [    0/  999]\n",
      "loss(204): 0.001516  [    0/  999]\n",
      "loss(205): 0.001734  [    0/  999]\n",
      "loss(206): 0.001302  [    0/  999]\n",
      "loss(207): 0.001857  [    0/  999]\n",
      "loss(208): 0.000772  [    0/  999]\n",
      "loss(209): 0.000887  [    0/  999]\n",
      "loss(210): 0.002128  [    0/  999]\n",
      "loss(211): 0.001564  [    0/  999]\n",
      "loss(212): 0.002048  [    0/  999]\n",
      "loss(213): 0.001482  [    0/  999]\n",
      "loss(214): 0.001622  [    0/  999]\n",
      "loss(215): 0.001122  [    0/  999]\n",
      "loss(216): 0.001058  [    0/  999]\n",
      "loss(217): 0.001816  [    0/  999]\n",
      "loss(218): 0.001495  [    0/  999]\n",
      "loss(219): 0.001897  [    0/  999]\n",
      "loss(220): 0.001298  [    0/  999]\n",
      "loss(221): 0.001012  [    0/  999]\n",
      "loss(222): 0.001260  [    0/  999]\n",
      "loss(223): 0.001184  [    0/  999]\n",
      "loss(224): 0.001053  [    0/  999]\n",
      "loss(225): 0.001308  [    0/  999]\n",
      "loss(226): 0.002251  [    0/  999]\n",
      "loss(227): 0.001857  [    0/  999]\n",
      "loss(228): 0.000802  [    0/  999]\n",
      "loss(229): 0.001238  [    0/  999]\n",
      "loss(230): 0.001909  [    0/  999]\n",
      "loss(231): 0.001626  [    0/  999]\n",
      "loss(232): 0.002069  [    0/  999]\n",
      "loss(233): 0.001816  [    0/  999]\n",
      "loss(234): 0.001059  [    0/  999]\n",
      "loss(235): 0.001536  [    0/  999]\n",
      "loss(236): 0.001439  [    0/  999]\n",
      "loss(237): 0.001207  [    0/  999]\n",
      "loss(238): 0.001335  [    0/  999]\n",
      "loss(239): 0.001218  [    0/  999]\n",
      "loss(240): 0.001043  [    0/  999]\n",
      "loss(241): 0.001648  [    0/  999]\n",
      "loss(242): 0.001636  [    0/  999]\n",
      "loss(243): 0.001707  [    0/  999]\n",
      "loss(244): 0.001043  [    0/  999]\n",
      "loss(245): 0.001575  [    0/  999]\n",
      "loss(246): 0.000782  [    0/  999]\n",
      "loss(247): 0.001374  [    0/  999]\n",
      "loss(248): 0.001434  [    0/  999]\n",
      "loss(249): 0.001298  [    0/  999]\n",
      "loss(250): 0.000908  [    0/  999]\n",
      "loss(251): 0.001201  [    0/  999]\n",
      "loss(252): 0.001916  [    0/  999]\n",
      "loss(253): 0.001522  [    0/  999]\n",
      "loss(254): 0.001220  [    0/  999]\n",
      "loss(255): 0.001794  [    0/  999]\n",
      "loss(256): 0.001140  [    0/  999]\n",
      "loss(257): 0.001554  [    0/  999]\n",
      "loss(258): 0.001257  [    0/  999]\n",
      "loss(259): 0.001100  [    0/  999]\n",
      "loss(260): 0.001378  [    0/  999]\n",
      "loss(261): 0.001661  [    0/  999]\n",
      "loss(262): 0.001028  [    0/  999]\n",
      "loss(263): 0.000975  [    0/  999]\n",
      "loss(264): 0.001714  [    0/  999]\n",
      "loss(265): 0.001854  [    0/  999]\n",
      "loss(266): 0.001459  [    0/  999]\n",
      "loss(267): 0.001443  [    0/  999]\n",
      "loss(268): 0.001893  [    0/  999]\n",
      "loss(269): 0.001640  [    0/  999]\n",
      "loss(270): 0.000995  [    0/  999]\n",
      "loss(271): 0.001366  [    0/  999]\n",
      "loss(272): 0.001908  [    0/  999]\n",
      "loss(273): 0.001416  [    0/  999]\n",
      "loss(274): 0.000857  [    0/  999]\n",
      "loss(275): 0.001075  [    0/  999]\n",
      "loss(276): 0.000951  [    0/  999]\n",
      "loss(277): 0.001410  [    0/  999]\n",
      "loss(278): 0.001053  [    0/  999]\n",
      "loss(279): 0.000600  [    0/  999]\n",
      "loss(280): 0.002401  [    0/  999]\n",
      "loss(281): 0.000834  [    0/  999]\n",
      "loss(282): 0.001406  [    0/  999]\n",
      "loss(283): 0.001603  [    0/  999]\n",
      "loss(284): 0.001437  [    0/  999]\n",
      "loss(285): 0.001543  [    0/  999]\n",
      "loss(286): 0.000982  [    0/  999]\n",
      "loss(287): 0.002547  [    0/  999]\n",
      "loss(288): 0.001422  [    0/  999]\n",
      "loss(289): 0.001563  [    0/  999]\n",
      "loss(290): 0.001500  [    0/  999]\n",
      "loss(291): 0.002009  [    0/  999]\n",
      "loss(292): 0.001335  [    0/  999]\n",
      "loss(293): 0.001179  [    0/  999]\n",
      "loss(294): 0.001264  [    0/  999]\n",
      "loss(295): 0.001281  [    0/  999]\n",
      "loss(296): 0.001776  [    0/  999]\n",
      "loss(297): 0.001459  [    0/  999]\n",
      "loss(298): 0.001518  [    0/  999]\n",
      "loss(299): 0.001594  [    0/  999]\n",
      "loss(300): 0.000865  [    0/  999]\n",
      "loss(301): 0.001964  [    0/  999]\n",
      "loss(302): 0.001508  [    0/  999]\n",
      "loss(303): 0.001671  [    0/  999]\n",
      "loss(304): 0.001875  [    0/  999]\n",
      "loss(305): 0.001225  [    0/  999]\n",
      "loss(306): 0.001284  [    0/  999]\n",
      "loss(307): 0.001246  [    0/  999]\n",
      "loss(308): 0.001066  [    0/  999]\n",
      "loss(309): 0.001443  [    0/  999]\n",
      "loss(310): 0.001148  [    0/  999]\n",
      "loss(311): 0.001143  [    0/  999]\n",
      "loss(312): 0.001803  [    0/  999]\n",
      "loss(313): 0.001608  [    0/  999]\n",
      "loss(314): 0.001553  [    0/  999]\n",
      "loss(315): 0.001520  [    0/  999]\n",
      "loss(316): 0.000570  [    0/  999]\n",
      "loss(317): 0.000943  [    0/  999]\n",
      "loss(318): 0.001883  [    0/  999]\n",
      "loss(319): 0.001008  [    0/  999]\n",
      "loss(320): 0.001954  [    0/  999]\n",
      "loss(321): 0.001163  [    0/  999]\n",
      "loss(322): 0.001535  [    0/  999]\n",
      "loss(323): 0.000893  [    0/  999]\n",
      "loss(324): 0.000950  [    0/  999]\n",
      "loss(325): 0.000918  [    0/  999]\n",
      "loss(326): 0.001223  [    0/  999]\n",
      "loss(327): 0.002142  [    0/  999]\n",
      "loss(328): 0.001595  [    0/  999]\n",
      "loss(329): 0.001858  [    0/  999]\n",
      "loss(330): 0.000789  [    0/  999]\n",
      "loss(331): 0.001492  [    0/  999]\n",
      "loss(332): 0.001275  [    0/  999]\n",
      "loss(333): 0.000890  [    0/  999]\n",
      "loss(334): 0.001617  [    0/  999]\n",
      "loss(335): 0.001191  [    0/  999]\n",
      "loss(336): 0.001714  [    0/  999]\n",
      "loss(337): 0.001559  [    0/  999]\n",
      "loss(338): 0.001040  [    0/  999]\n",
      "loss(339): 0.001018  [    0/  999]\n",
      "loss(340): 0.000993  [    0/  999]\n",
      "loss(341): 0.001043  [    0/  999]\n",
      "loss(342): 0.001186  [    0/  999]\n",
      "loss(343): 0.001029  [    0/  999]\n",
      "loss(344): 0.001344  [    0/  999]\n",
      "loss(345): 0.001839  [    0/  999]\n",
      "loss(346): 0.001115  [    0/  999]\n",
      "loss(347): 0.001136  [    0/  999]\n",
      "loss(348): 0.001438  [    0/  999]\n",
      "loss(349): 0.001317  [    0/  999]\n",
      "loss(350): 0.000983  [    0/  999]\n",
      "loss(351): 0.001551  [    0/  999]\n",
      "loss(352): 0.001256  [    0/  999]\n",
      "loss(353): 0.001508  [    0/  999]\n",
      "loss(354): 0.001326  [    0/  999]\n",
      "loss(355): 0.001293  [    0/  999]\n",
      "loss(356): 0.001338  [    0/  999]\n",
      "loss(357): 0.000711  [    0/  999]\n",
      "loss(358): 0.001239  [    0/  999]\n",
      "loss(359): 0.002089  [    0/  999]\n",
      "loss(360): 0.001116  [    0/  999]\n",
      "loss(361): 0.001152  [    0/  999]\n",
      "loss(362): 0.001199  [    0/  999]\n",
      "loss(363): 0.001431  [    0/  999]\n",
      "loss(364): 0.000802  [    0/  999]\n",
      "loss(365): 0.001373  [    0/  999]\n",
      "loss(366): 0.000862  [    0/  999]\n",
      "loss(367): 0.001687  [    0/  999]\n",
      "loss(368): 0.001781  [    0/  999]\n",
      "loss(369): 0.000763  [    0/  999]\n",
      "loss(370): 0.001748  [    0/  999]\n",
      "loss(371): 0.000925  [    0/  999]\n",
      "loss(372): 0.000793  [    0/  999]\n",
      "loss(373): 0.000572  [    0/  999]\n",
      "loss(374): 0.000784  [    0/  999]\n",
      "loss(375): 0.001393  [    0/  999]\n",
      "loss(376): 0.000803  [    0/  999]\n",
      "loss(377): 0.001641  [    0/  999]\n",
      "loss(378): 0.001174  [    0/  999]\n",
      "loss(379): 0.001254  [    0/  999]\n",
      "loss(380): 0.001178  [    0/  999]\n",
      "loss(381): 0.001147  [    0/  999]\n",
      "loss(382): 0.001152  [    0/  999]\n",
      "loss(383): 0.001013  [    0/  999]\n",
      "loss(384): 0.000660  [    0/  999]\n",
      "loss(385): 0.000836  [    0/  999]\n",
      "loss(386): 0.001327  [    0/  999]\n",
      "loss(387): 0.000847  [    0/  999]\n",
      "loss(388): 0.001194  [    0/  999]\n",
      "loss(389): 0.001416  [    0/  999]\n",
      "loss(390): 0.001155  [    0/  999]\n",
      "loss(391): 0.001006  [    0/  999]\n",
      "loss(392): 0.001141  [    0/  999]\n",
      "loss(393): 0.001295  [    0/  999]\n",
      "loss(394): 0.001257  [    0/  999]\n",
      "loss(395): 0.001526  [    0/  999]\n",
      "loss(396): 0.000877  [    0/  999]\n",
      "loss(397): 0.000933  [    0/  999]\n",
      "loss(398): 0.001283  [    0/  999]\n",
      "loss(399): 0.001016  [    0/  999]\n",
      "loss(400): 0.001460  [    0/  999]\n",
      "loss(401): 0.001181  [    0/  999]\n",
      "loss(402): 0.000938  [    0/  999]\n",
      "loss(403): 0.001335  [    0/  999]\n",
      "loss(404): 0.000815  [    0/  999]\n",
      "loss(405): 0.001082  [    0/  999]\n",
      "loss(406): 0.000943  [    0/  999]\n",
      "loss(407): 0.001149  [    0/  999]\n",
      "loss(408): 0.001472  [    0/  999]\n",
      "loss(409): 0.000824  [    0/  999]\n",
      "loss(410): 0.000584  [    0/  999]\n",
      "loss(411): 0.000866  [    0/  999]\n",
      "loss(412): 0.000985  [    0/  999]\n",
      "loss(413): 0.001110  [    0/  999]\n",
      "loss(414): 0.001502  [    0/  999]\n",
      "loss(415): 0.001085  [    0/  999]\n",
      "loss(416): 0.001274  [    0/  999]\n",
      "loss(417): 0.001414  [    0/  999]\n",
      "loss(418): 0.001141  [    0/  999]\n",
      "loss(419): 0.000969  [    0/  999]\n",
      "loss(420): 0.000999  [    0/  999]\n",
      "loss(421): 0.001442  [    0/  999]\n",
      "loss(422): 0.001001  [    0/  999]\n",
      "loss(423): 0.001600  [    0/  999]\n",
      "loss(424): 0.001356  [    0/  999]\n",
      "loss(425): 0.001234  [    0/  999]\n",
      "loss(426): 0.001964  [    0/  999]\n",
      "loss(427): 0.000832  [    0/  999]\n",
      "loss(428): 0.000933  [    0/  999]\n",
      "loss(429): 0.000665  [    0/  999]\n",
      "loss(430): 0.000627  [    0/  999]\n",
      "loss(431): 0.000835  [    0/  999]\n",
      "loss(432): 0.000751  [    0/  999]\n",
      "loss(433): 0.001779  [    0/  999]\n",
      "loss(434): 0.000763  [    0/  999]\n",
      "loss(435): 0.000915  [    0/  999]\n",
      "loss(436): 0.001406  [    0/  999]\n",
      "loss(437): 0.000494  [    0/  999]\n",
      "loss(438): 0.000976  [    0/  999]\n",
      "loss(439): 0.001017  [    0/  999]\n",
      "loss(440): 0.001576  [    0/  999]\n",
      "loss(441): 0.000823  [    0/  999]\n",
      "loss(442): 0.001149  [    0/  999]\n",
      "loss(443): 0.001134  [    0/  999]\n",
      "loss(444): 0.001006  [    0/  999]\n",
      "loss(445): 0.000678  [    0/  999]\n",
      "loss(446): 0.001473  [    0/  999]\n",
      "loss(447): 0.000887  [    0/  999]\n",
      "loss(448): 0.001696  [    0/  999]\n",
      "loss(449): 0.001214  [    0/  999]\n",
      "loss(450): 0.001073  [    0/  999]\n",
      "loss(451): 0.001127  [    0/  999]\n",
      "loss(452): 0.001457  [    0/  999]\n",
      "loss(453): 0.001628  [    0/  999]\n",
      "loss(454): 0.000927  [    0/  999]\n",
      "loss(455): 0.001084  [    0/  999]\n",
      "loss(456): 0.000773  [    0/  999]\n",
      "loss(457): 0.001323  [    0/  999]\n",
      "loss(458): 0.001260  [    0/  999]\n",
      "loss(459): 0.001192  [    0/  999]\n",
      "loss(460): 0.001745  [    0/  999]\n",
      "loss(461): 0.001336  [    0/  999]\n",
      "loss(462): 0.002251  [    0/  999]\n",
      "loss(463): 0.001274  [    0/  999]\n",
      "loss(464): 0.001512  [    0/  999]\n",
      "loss(465): 0.001422  [    0/  999]\n",
      "loss(466): 0.001192  [    0/  999]\n",
      "loss(467): 0.001757  [    0/  999]\n",
      "loss(468): 0.001213  [    0/  999]\n",
      "loss(469): 0.000889  [    0/  999]\n",
      "loss(470): 0.001091  [    0/  999]\n",
      "loss(471): 0.000977  [    0/  999]\n",
      "loss(472): 0.000822  [    0/  999]\n",
      "loss(473): 0.000801  [    0/  999]\n",
      "loss(474): 0.001147  [    0/  999]\n",
      "loss(475): 0.000811  [    0/  999]\n",
      "loss(476): 0.001318  [    0/  999]\n",
      "loss(477): 0.001331  [    0/  999]\n",
      "loss(478): 0.001480  [    0/  999]\n",
      "loss(479): 0.001028  [    0/  999]\n",
      "loss(480): 0.000930  [    0/  999]\n",
      "loss(481): 0.000857  [    0/  999]\n",
      "loss(482): 0.001042  [    0/  999]\n",
      "loss(483): 0.001086  [    0/  999]\n",
      "loss(484): 0.001367  [    0/  999]\n",
      "loss(485): 0.000993  [    0/  999]\n",
      "loss(486): 0.000892  [    0/  999]\n",
      "loss(487): 0.001200  [    0/  999]\n",
      "loss(488): 0.001223  [    0/  999]\n",
      "loss(489): 0.001246  [    0/  999]\n",
      "loss(490): 0.001377  [    0/  999]\n",
      "loss(491): 0.001253  [    0/  999]\n",
      "loss(492): 0.000979  [    0/  999]\n",
      "loss(493): 0.000741  [    0/  999]\n",
      "loss(494): 0.001193  [    0/  999]\n",
      "loss(495): 0.000701  [    0/  999]\n",
      "loss(496): 0.000987  [    0/  999]\n",
      "loss(497): 0.001237  [    0/  999]\n",
      "loss(498): 0.001656  [    0/  999]\n",
      "loss(499): 0.001007  [    0/  999]\n",
      "loss(500): 0.001174  [    0/  999]\n",
      "loss(501): 0.001536  [    0/  999]\n",
      "loss(502): 0.000945  [    0/  999]\n",
      "loss(503): 0.001134  [    0/  999]\n",
      "loss(504): 0.001174  [    0/  999]\n",
      "loss(505): 0.000718  [    0/  999]\n",
      "loss(506): 0.000893  [    0/  999]\n",
      "loss(507): 0.000829  [    0/  999]\n",
      "loss(508): 0.000965  [    0/  999]\n",
      "loss(509): 0.001440  [    0/  999]\n",
      "loss(510): 0.001068  [    0/  999]\n",
      "loss(511): 0.000807  [    0/  999]\n",
      "loss(512): 0.001520  [    0/  999]\n",
      "loss(513): 0.001094  [    0/  999]\n",
      "loss(514): 0.001396  [    0/  999]\n",
      "loss(515): 0.000842  [    0/  999]\n",
      "loss(516): 0.000725  [    0/  999]\n",
      "loss(517): 0.001201  [    0/  999]\n",
      "loss(518): 0.001379  [    0/  999]\n",
      "loss(519): 0.001504  [    0/  999]\n",
      "loss(520): 0.001078  [    0/  999]\n",
      "loss(521): 0.001514  [    0/  999]\n",
      "loss(522): 0.000844  [    0/  999]\n",
      "loss(523): 0.001096  [    0/  999]\n",
      "loss(524): 0.000783  [    0/  999]\n",
      "loss(525): 0.000628  [    0/  999]\n",
      "loss(526): 0.001258  [    0/  999]\n",
      "loss(527): 0.000902  [    0/  999]\n",
      "loss(528): 0.000763  [    0/  999]\n",
      "loss(529): 0.001636  [    0/  999]\n",
      "loss(530): 0.000867  [    0/  999]\n",
      "loss(531): 0.001367  [    0/  999]\n",
      "loss(532): 0.001328  [    0/  999]\n",
      "loss(533): 0.001121  [    0/  999]\n",
      "loss(534): 0.000649  [    0/  999]\n",
      "loss(535): 0.001565  [    0/  999]\n",
      "loss(536): 0.001959  [    0/  999]\n",
      "loss(537): 0.000911  [    0/  999]\n",
      "loss(538): 0.001200  [    0/  999]\n",
      "loss(539): 0.001200  [    0/  999]\n",
      "loss(540): 0.000624  [    0/  999]\n",
      "loss(541): 0.000639  [    0/  999]\n",
      "loss(542): 0.001055  [    0/  999]\n",
      "loss(543): 0.001333  [    0/  999]\n",
      "loss(544): 0.001211  [    0/  999]\n",
      "loss(545): 0.001276  [    0/  999]\n",
      "loss(546): 0.000676  [    0/  999]\n",
      "loss(547): 0.000898  [    0/  999]\n",
      "loss(548): 0.001497  [    0/  999]\n",
      "loss(549): 0.001179  [    0/  999]\n",
      "loss(550): 0.000980  [    0/  999]\n",
      "loss(551): 0.001476  [    0/  999]\n",
      "loss(552): 0.001456  [    0/  999]\n",
      "loss(553): 0.001236  [    0/  999]\n",
      "loss(554): 0.001246  [    0/  999]\n",
      "loss(555): 0.001170  [    0/  999]\n",
      "loss(556): 0.000837  [    0/  999]\n",
      "loss(557): 0.001025  [    0/  999]\n",
      "loss(558): 0.001570  [    0/  999]\n",
      "loss(559): 0.000797  [    0/  999]\n",
      "loss(560): 0.000891  [    0/  999]\n",
      "loss(561): 0.000936  [    0/  999]\n",
      "loss(562): 0.000701  [    0/  999]\n",
      "loss(563): 0.000725  [    0/  999]\n",
      "loss(564): 0.001476  [    0/  999]\n",
      "loss(565): 0.000868  [    0/  999]\n",
      "loss(566): 0.001052  [    0/  999]\n",
      "loss(567): 0.001264  [    0/  999]\n",
      "loss(568): 0.001035  [    0/  999]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\mburs\\OneDrive - Lehigh University\\Opportunities\\Research\\NN - Git\\_EncDecMulti.ipynb Cell 7\u001b[0m in \u001b[0;36m<cell line: 8>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/mburs/OneDrive%20-%20Lehigh%20University/Opportunities/Research/NN%20-%20Git/_EncDecMulti.ipynb#W6sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m \u001b[39m# print(list(model.parameters()))\u001b[39;00m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/mburs/OneDrive%20-%20Lehigh%20University/Opportunities/Research/NN%20-%20Git/_EncDecMulti.ipynb#W6sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m \n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/mburs/OneDrive%20-%20Lehigh%20University/Opportunities/Research/NN%20-%20Git/_EncDecMulti.ipynb#W6sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m \u001b[39m# print(list(model.parameters()))\u001b[39;00m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/mburs/OneDrive%20-%20Lehigh%20University/Opportunities/Research/NN%20-%20Git/_EncDecMulti.ipynb#W6sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m \u001b[39mfor\u001b[39;00m t \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(epochs):\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/mburs/OneDrive%20-%20Lehigh%20University/Opportunities/Research/NN%20-%20Git/_EncDecMulti.ipynb#W6sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m     \u001b[39m# print(f\"Epoch {t+1}\\n-------------------------------\")\u001b[39;00m\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/mburs/OneDrive%20-%20Lehigh%20University/Opportunities/Research/NN%20-%20Git/_EncDecMulti.ipynb#W6sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m     train_loop(train_dataloader, model, loss_fn, optimizer,t)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/mburs/OneDrive%20-%20Lehigh%20University/Opportunities/Research/NN%20-%20Git/_EncDecMulti.ipynb#W6sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m     \u001b[39m# break\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/mburs/OneDrive%20-%20Lehigh%20University/Opportunities/Research/NN%20-%20Git/_EncDecMulti.ipynb#W6sZmlsZQ%3D%3D?line=11'>12</a>\u001b[0m     \u001b[39m# test_loop(test_dataloader, model, loss_fn)\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/mburs/OneDrive%20-%20Lehigh%20University/Opportunities/Research/NN%20-%20Git/_EncDecMulti.ipynb#W6sZmlsZQ%3D%3D?line=13'>14</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mDone!\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "\u001b[1;32mc:\\Users\\mburs\\OneDrive - Lehigh University\\Opportunities\\Research\\NN - Git\\_EncDecMulti.ipynb Cell 7\u001b[0m in \u001b[0;36mtrain_loop\u001b[1;34m(dataloader, model, loss_fn, optimizer, t)\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/mburs/OneDrive%20-%20Lehigh%20University/Opportunities/Research/NN%20-%20Git/_EncDecMulti.ipynb#W6sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m pred \u001b[39m=\u001b[39m model(X)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/mburs/OneDrive%20-%20Lehigh%20University/Opportunities/Research/NN%20-%20Git/_EncDecMulti.ipynb#W6sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m \u001b[39m# break\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/mburs/OneDrive%20-%20Lehigh%20University/Opportunities/Research/NN%20-%20Git/_EncDecMulti.ipynb#W6sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m \u001b[39m# print(pred.size())\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/mburs/OneDrive%20-%20Lehigh%20University/Opportunities/Research/NN%20-%20Git/_EncDecMulti.ipynb#W6sZmlsZQ%3D%3D?line=11'>12</a>\u001b[0m \u001b[39m# print(y.size())\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/mburs/OneDrive%20-%20Lehigh%20University/Opportunities/Research/NN%20-%20Git/_EncDecMulti.ipynb#W6sZmlsZQ%3D%3D?line=12'>13</a>\u001b[0m \u001b[39m# print(\"pred\",pred.size())\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/mburs/OneDrive%20-%20Lehigh%20University/Opportunities/Research/NN%20-%20Git/_EncDecMulti.ipynb#W6sZmlsZQ%3D%3D?line=13'>14</a>\u001b[0m \u001b[39m# print(\"Y\",y.size())\u001b[39;00m\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/mburs/OneDrive%20-%20Lehigh%20University/Opportunities/Research/NN%20-%20Git/_EncDecMulti.ipynb#W6sZmlsZQ%3D%3D?line=14'>15</a>\u001b[0m loss \u001b[39m=\u001b[39m loss_fn(pred, y)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/mburs/OneDrive%20-%20Lehigh%20University/Opportunities/Research/NN%20-%20Git/_EncDecMulti.ipynb#W6sZmlsZQ%3D%3D?line=16'>17</a>\u001b[0m \u001b[39m# Backpropagation\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/mburs/OneDrive%20-%20Lehigh%20University/Opportunities/Research/NN%20-%20Git/_EncDecMulti.ipynb#W6sZmlsZQ%3D%3D?line=17'>18</a>\u001b[0m optimizer\u001b[39m.\u001b[39mzero_grad()\n",
      "File \u001b[1;32mc:\\Users\\mburs\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\nn\\modules\\module.py:1188\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1185\u001b[0m             tracing_state\u001b[39m.\u001b[39mpop_scope()\n\u001b[0;32m   1186\u001b[0m     \u001b[39mreturn\u001b[39;00m result\n\u001b[1;32m-> 1188\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_call_impl\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39m*\u001b[39m\u001b[39minput\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[0;32m   1189\u001b[0m     forward_call \u001b[39m=\u001b[39m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_slow_forward \u001b[39mif\u001b[39;00m torch\u001b[39m.\u001b[39m_C\u001b[39m.\u001b[39m_get_tracing_state() \u001b[39melse\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mforward)\n\u001b[0;32m   1190\u001b[0m     \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1191\u001b[0m     \u001b[39m# this function, and just call forward.\u001b[39;00m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "learning_rate = .001\n",
    "epochs = 1000\n",
    "loss_fn = nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "# print(list(model.parameters()))\n",
    "\n",
    "# print(list(model.parameters()))\n",
    "for t in range(epochs):\n",
    "    # print(f\"Epoch {t+1}\\n-------------------------------\")\n",
    "    train_loop(train_dataloader, model, loss_fn, optimizer,t)\n",
    "    # break\n",
    "    # test_loop(test_dataloader, model, loss_fn)\n",
    "\n",
    "print(\"Done!\")\n",
    "# print(list(model.parameters()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Error:\n",
      "Avg loss: 0.000489 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "def test_loop(dataloader, model, loss_fn):\n",
    "    size = len(dataloader.dataset)\n",
    "    num_batches = len(dataloader)\n",
    "    test_loss= 0\n",
    "    with torch.no_grad():\n",
    "        for X, y in dataloader:\n",
    "            # print(X)\n",
    "            pred = model(X)\n",
    "            # print(pred)\n",
    "            test_loss += loss_fn(pred, y).item()\n",
    "\n",
    "    test_loss /= num_batches\n",
    "    print(f\"Test Error:\\nAvg loss: {test_loss:>8f} \\n\")\n",
    "\n",
    "def test(model):\n",
    "    norm = test_data.getNorm()\n",
    "    with torch.no_grad():\n",
    "        for idx in range(len(test_data)):\n",
    "            X = test_data[idx][0]\n",
    "            Y = test_data[idx][1]*norm\n",
    "            feat = X[None,:]\n",
    "            pred = model(feat)[0]*norm\n",
    "            pred = pred[:,0] \n",
    "            Y = Y[:,0]  \n",
    "            plt.plot(Y)\n",
    "            plt.plot(pred)\n",
    "            \n",
    "            plt.show()\n",
    "\n",
    "\n",
    "def graph(model):\n",
    "    norm = test_data.getNorm()\n",
    "    with torch.no_grad():\n",
    "        predY = []\n",
    "        actY = []\n",
    "        for idx in range(len(test_data)):\n",
    "            X = test_data[idx][0]\n",
    "            y = test_data[idx][1]*norm\n",
    "            feat = X[None,:]\n",
    "\n",
    "            pred = model(feat)[0]*norm\n",
    "            future = 3\n",
    "\n",
    "            # predY.append(pred[numFeat-1::][future])\n",
    "            # actY.append(y[numFeat-1::][future]) \n",
    "            pred = pred[:,0] \n",
    "            y = y[:,0]  \n",
    "            # print(pred.size())\n",
    "            # print(float(pred[3]))\n",
    "            predY.append(pred[future])\n",
    "            actY.append(y[future])    \n",
    "            \n",
    "        plt.plot(actY) \n",
    "        plt.plot(predY)\n",
    "        plt.show()\n",
    "\n",
    "graph(model)\n",
    "# test(model)\n",
    "test_loop(test_dataloader,model,loss_fn)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "a148e496c0f49d57628151d2aab378855c5a8a7aaacdf2673cbe18e166795068"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
