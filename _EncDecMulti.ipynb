{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets\n",
    "from torchvision.transforms import ToTensor\n",
    "from torch.utils.data import Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "#torch data set\n",
    "class dataSetAll(Dataset):\n",
    "    def __init__(self,numFeat,numOut, location):\n",
    "        #import data from CSV\n",
    "        df1 = pd.read_csv(\"data\\hosp\\hhs_data_weekly.csv\")\n",
    "        df1 = df1[(df1[\"location\"] == location) & (df1[\"yr\"]==2022) & (df1[\"week\"]<52)][\"weekly_hosps\"]\n",
    "\n",
    "        df2 = pd.read_csv(\"data\\hosp\\\\fluview_ili.csv\")\n",
    "        df2 = df2[(df2[\"location\"] == location) & (df2[\"Y\"]==2022)][[\"population\",\"numili\"]]\n",
    "\n",
    "        df1 = df1.reset_index()\n",
    "        df2 = df2.reset_index()\n",
    "\n",
    "        df3 = pd.concat([df1,df2],axis=1)[[\"weekly_hosps\",\"population\",\"numili\"]]\n",
    "\n",
    "        self.numFeat = numFeat #------------------------\n",
    "        self.numOut = numOut\n",
    "        \n",
    "        self.data = np.asarray(df3,dtype=np.float32)\n",
    "        # print(self.data)\n",
    "        self.norm = np.linalg.norm(self.data, axis=0)\n",
    "        # print(self.norm)\n",
    "        self.data = self.data / self.norm\n",
    "        self.data = torch.as_tensor(self.data)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)-self.numFeat-self.numOut\n",
    "    \n",
    "    def __getitem__(self,idx):\n",
    "        # idx = 0\n",
    "        return self.data[idx:idx+self.numFeat],self.data[idx+self.numFeat:idx+self.numFeat+self.numOut]\n",
    "    \n",
    "    def getNorm(self):\n",
    "        return self.norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create data loaders\n",
    "numFeat = 10\n",
    "numOut = 4\n",
    "batchSize = 64\n",
    "\n",
    "train_data_1 = dataSetAll(numFeat,numOut,\"01\")\n",
    "train_data_2 = dataSetAll(numFeat,numOut,\"02\")\n",
    "train_data_4 = dataSetAll(numFeat,numOut,\"04\")\n",
    "train_data_5 = dataSetAll(numFeat,numOut,\"05\")\n",
    "train_data_6 = dataSetAll(numFeat,numOut,\"06\")\n",
    "\n",
    "train_data = torch.utils.data.ConcatDataset([train_data_1,train_data_2,train_data_4,train_data_5,train_data_6])\n",
    "train_dataloader = DataLoader(train_data, batch_size=batchSize,drop_last=False,shuffle=True)\n",
    "\n",
    "test_data = dataSetAll(numFeat,numOut,\"US\")\n",
    "test_dataloader = DataLoader(test_data, batch_size=batchSize,drop_last=False,shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "#test\n",
    "# pd.set_option('display.max_rows', 500)\n",
    "# df1 = pd.read_csv(\"data\\hosp\\hhs_data_weekly.csv\")\n",
    "# df1 = df1[(df1[\"location\"] == \"US\") & (df1[\"yr\"]==2022) & (df1[\"week\"]<52)][\"weekly_hosps\"]\n",
    "\n",
    "# df2 = pd.read_csv(\"data\\hosp\\\\fluview_ili.csv\")\n",
    "# df2 = df2[(df2[\"location\"] == \"US\") & (df2[\"Y\"]==2022)][[\"population\",\"numili\"]]\n",
    "\n",
    "# df1 = df1.reset_index()\n",
    "# df2 = df2.reset_index()\n",
    "\n",
    "# df3 = pd.concat([df1,df2],axis=1)[[\"weekly_hosps\",\"population\",\"numili\"]]\n",
    "# print(df3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create our RNN based network with an RNN followed by a linear layer\n",
    "inputSize = 3\n",
    "sequenceLength = numFeat\n",
    "numLayers = 1\n",
    "hiddenSize = 32\n",
    "\n",
    "class RNN(nn.Module):\n",
    "    def __init__(self,inputSize,hiddenSize,numLayers,numOut,sequenceLength,future=0):\n",
    "        super(RNN, self).__init__()\n",
    "        self.inputSize = inputSize\n",
    "        self.hiddenSize = hiddenSize\n",
    "        self.numLayers = numLayers\n",
    "        self.numOut = numOut\n",
    "        self.future = future\n",
    "        self.inputSize = inputSize\n",
    "        # print(batchSize,sequenceLength,inputSize)\n",
    "        # self.LSTM = nn.LSTM(inputSize,hiddenSize,numLayers,batch_first=True)\n",
    "        self.rnn1 = nn.RNNCell(inputSize,hiddenSize)\n",
    "        self.rnn2 = nn.RNNCell(inputSize,hiddenSize)\n",
    "        self.rnn3 = nn.RNNCell(inputSize,hiddenSize)\n",
    "        self.rnn4 = nn.RNNCell(inputSize,hiddenSize)\n",
    "        # self.rnn2 = nn.RNNCell(1,hiddenSize,nonlinearity='tanh')\n",
    "        self.fc1 = nn.Linear(hiddenSize,inputSize)\n",
    "        self.fc2 = nn.Linear(hiddenSize,inputSize)\n",
    "        self.fc3 = nn.Linear(hiddenSize,inputSize)\n",
    "        self.fc4 = nn.Linear(hiddenSize,inputSize)\n",
    "        \n",
    "    def forward(self,x):\n",
    "        nSamples = x.size(0)\n",
    "        outputs = torch.zeros([nSamples,self.numOut,self.inputSize])\n",
    "        h_1 = torch.zeros(nSamples, self.hiddenSize, dtype=torch.float32)\n",
    "        h_2 = torch.zeros(nSamples, self.hiddenSize, dtype=torch.float32)\n",
    "\n",
    "        for input in x.split(1,dim=1):\n",
    "            h_1 = self.rnn1(input[:,0,:], h_1)\n",
    "            out = self.fc1(h_1)\n",
    "        outputs[:,0] = out\n",
    "\n",
    "        h_1 = self.rnn2(out, h_1)\n",
    "        out = self.fc2(h_1)\n",
    "        outputs[:,1] = out\n",
    "\n",
    "        h_1 = self.rnn3(out, h_1)\n",
    "        out = self.fc3(h_1)\n",
    "        outputs[:,2] = out\n",
    "\n",
    "        h_1 = self.rnn4(out, h_1)\n",
    "        out = self.fc4(h_1)\n",
    "        outputs[:,3] = out\n",
    "\n",
    "        # outputs = torch.as_tensor(outputs)\n",
    "        return outputs\n",
    "\n",
    "model = RNN(inputSize,hiddenSize,numLayers,numOut,sequenceLength)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train/test loop\n",
    "def train_loop(dataloader, model, loss_fn, optimizer,t):\n",
    "    size = len(dataloader.dataset)\n",
    "    for batch, (X, y) in enumerate(dataloader):\n",
    "        # Compute prediction and loss\n",
    "        # print(X.size())\n",
    "        # X = X[:,:,None]\n",
    "        # print(X.size())\n",
    "        pred = model(X)\n",
    "        # break\n",
    "        # print(pred.size())\n",
    "        # print(y.size())\n",
    "        # print(\"pred\",pred.size())\n",
    "        # print(\"Y\",y.size())\n",
    "        loss = loss_fn(pred, y)\n",
    "        \n",
    "        # Backpropagation\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if batch % size == 0:\n",
    "            loss, current = loss.item(), batch * len(X)\n",
    "            print(f\"loss({t}): {loss:>7f}  [{current:>5d}/{size:>5d}]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss(0): 0.044311  [    0/  185]\n",
      "loss(1): 0.027992  [    0/  185]\n",
      "loss(2): 0.022164  [    0/  185]\n",
      "loss(3): 0.015034  [    0/  185]\n",
      "loss(4): 0.008346  [    0/  185]\n",
      "loss(5): 0.008873  [    0/  185]\n",
      "loss(6): 0.010778  [    0/  185]\n",
      "loss(7): 0.006174  [    0/  185]\n",
      "loss(8): 0.006735  [    0/  185]\n",
      "loss(9): 0.007294  [    0/  185]\n",
      "loss(10): 0.006956  [    0/  185]\n",
      "loss(11): 0.008298  [    0/  185]\n",
      "loss(12): 0.008304  [    0/  185]\n",
      "loss(13): 0.007324  [    0/  185]\n",
      "loss(14): 0.006081  [    0/  185]\n",
      "loss(15): 0.009013  [    0/  185]\n",
      "loss(16): 0.007342  [    0/  185]\n",
      "loss(17): 0.006489  [    0/  185]\n",
      "loss(18): 0.006575  [    0/  185]\n",
      "loss(19): 0.006906  [    0/  185]\n",
      "loss(20): 0.007236  [    0/  185]\n",
      "loss(21): 0.006767  [    0/  185]\n",
      "loss(22): 0.007265  [    0/  185]\n",
      "loss(23): 0.006497  [    0/  185]\n",
      "loss(24): 0.004843  [    0/  185]\n",
      "loss(25): 0.006731  [    0/  185]\n",
      "loss(26): 0.004881  [    0/  185]\n",
      "loss(27): 0.004091  [    0/  185]\n",
      "loss(28): 0.005225  [    0/  185]\n",
      "loss(29): 0.003768  [    0/  185]\n",
      "loss(30): 0.004632  [    0/  185]\n",
      "loss(31): 0.003397  [    0/  185]\n",
      "loss(32): 0.003323  [    0/  185]\n",
      "loss(33): 0.004995  [    0/  185]\n",
      "loss(34): 0.004292  [    0/  185]\n",
      "loss(35): 0.003321  [    0/  185]\n",
      "loss(36): 0.003309  [    0/  185]\n",
      "loss(37): 0.002997  [    0/  185]\n",
      "loss(38): 0.003110  [    0/  185]\n",
      "loss(39): 0.003263  [    0/  185]\n",
      "loss(40): 0.003232  [    0/  185]\n",
      "loss(41): 0.003268  [    0/  185]\n",
      "loss(42): 0.003390  [    0/  185]\n",
      "loss(43): 0.002792  [    0/  185]\n",
      "loss(44): 0.003965  [    0/  185]\n",
      "loss(45): 0.004623  [    0/  185]\n",
      "loss(46): 0.003850  [    0/  185]\n",
      "loss(47): 0.001643  [    0/  185]\n",
      "loss(48): 0.003029  [    0/  185]\n",
      "loss(49): 0.002908  [    0/  185]\n",
      "loss(50): 0.003036  [    0/  185]\n",
      "loss(51): 0.002212  [    0/  185]\n",
      "loss(52): 0.004086  [    0/  185]\n",
      "loss(53): 0.003732  [    0/  185]\n",
      "loss(54): 0.002861  [    0/  185]\n",
      "loss(55): 0.003211  [    0/  185]\n",
      "loss(56): 0.001542  [    0/  185]\n",
      "loss(57): 0.002547  [    0/  185]\n",
      "loss(58): 0.002558  [    0/  185]\n",
      "loss(59): 0.003212  [    0/  185]\n",
      "loss(60): 0.002636  [    0/  185]\n",
      "loss(61): 0.002824  [    0/  185]\n",
      "loss(62): 0.002683  [    0/  185]\n",
      "loss(63): 0.003143  [    0/  185]\n",
      "loss(64): 0.003331  [    0/  185]\n",
      "loss(65): 0.002998  [    0/  185]\n",
      "loss(66): 0.003045  [    0/  185]\n",
      "loss(67): 0.002423  [    0/  185]\n",
      "loss(68): 0.003509  [    0/  185]\n",
      "loss(69): 0.002329  [    0/  185]\n",
      "loss(70): 0.002197  [    0/  185]\n",
      "loss(71): 0.002601  [    0/  185]\n",
      "loss(72): 0.002481  [    0/  185]\n",
      "loss(73): 0.002014  [    0/  185]\n",
      "loss(74): 0.001554  [    0/  185]\n",
      "loss(75): 0.002556  [    0/  185]\n",
      "loss(76): 0.002978  [    0/  185]\n",
      "loss(77): 0.002582  [    0/  185]\n",
      "loss(78): 0.003231  [    0/  185]\n",
      "loss(79): 0.003198  [    0/  185]\n",
      "loss(80): 0.002556  [    0/  185]\n",
      "loss(81): 0.001696  [    0/  185]\n",
      "loss(82): 0.002564  [    0/  185]\n",
      "loss(83): 0.002908  [    0/  185]\n",
      "loss(84): 0.002789  [    0/  185]\n",
      "loss(85): 0.002621  [    0/  185]\n",
      "loss(86): 0.003022  [    0/  185]\n",
      "loss(87): 0.002582  [    0/  185]\n",
      "loss(88): 0.002592  [    0/  185]\n",
      "loss(89): 0.002691  [    0/  185]\n",
      "loss(90): 0.002238  [    0/  185]\n",
      "loss(91): 0.002416  [    0/  185]\n",
      "loss(92): 0.002440  [    0/  185]\n",
      "loss(93): 0.002594  [    0/  185]\n",
      "loss(94): 0.002273  [    0/  185]\n",
      "loss(95): 0.002094  [    0/  185]\n",
      "loss(96): 0.002713  [    0/  185]\n",
      "loss(97): 0.001699  [    0/  185]\n",
      "loss(98): 0.001871  [    0/  185]\n",
      "loss(99): 0.002353  [    0/  185]\n",
      "loss(100): 0.002987  [    0/  185]\n",
      "loss(101): 0.003056  [    0/  185]\n",
      "loss(102): 0.003150  [    0/  185]\n",
      "loss(103): 0.001947  [    0/  185]\n",
      "loss(104): 0.001720  [    0/  185]\n",
      "loss(105): 0.001293  [    0/  185]\n",
      "loss(106): 0.001689  [    0/  185]\n",
      "loss(107): 0.001840  [    0/  185]\n",
      "loss(108): 0.001857  [    0/  185]\n",
      "loss(109): 0.002873  [    0/  185]\n",
      "loss(110): 0.003055  [    0/  185]\n",
      "loss(111): 0.002429  [    0/  185]\n",
      "loss(112): 0.003001  [    0/  185]\n",
      "loss(113): 0.003150  [    0/  185]\n",
      "loss(114): 0.003012  [    0/  185]\n",
      "loss(115): 0.002836  [    0/  185]\n",
      "loss(116): 0.002995  [    0/  185]\n",
      "loss(117): 0.002193  [    0/  185]\n",
      "loss(118): 0.003348  [    0/  185]\n",
      "loss(119): 0.002105  [    0/  185]\n",
      "loss(120): 0.002225  [    0/  185]\n",
      "loss(121): 0.001820  [    0/  185]\n",
      "loss(122): 0.002028  [    0/  185]\n",
      "loss(123): 0.001551  [    0/  185]\n",
      "loss(124): 0.002203  [    0/  185]\n",
      "loss(125): 0.002536  [    0/  185]\n",
      "loss(126): 0.002217  [    0/  185]\n",
      "loss(127): 0.002106  [    0/  185]\n",
      "loss(128): 0.002251  [    0/  185]\n",
      "loss(129): 0.002699  [    0/  185]\n",
      "loss(130): 0.002600  [    0/  185]\n",
      "loss(131): 0.001763  [    0/  185]\n",
      "loss(132): 0.001850  [    0/  185]\n",
      "loss(133): 0.002479  [    0/  185]\n",
      "loss(134): 0.002639  [    0/  185]\n",
      "loss(135): 0.002059  [    0/  185]\n",
      "loss(136): 0.002701  [    0/  185]\n",
      "loss(137): 0.001502  [    0/  185]\n",
      "loss(138): 0.002306  [    0/  185]\n",
      "loss(139): 0.002396  [    0/  185]\n",
      "loss(140): 0.001825  [    0/  185]\n",
      "loss(141): 0.002477  [    0/  185]\n",
      "loss(142): 0.002756  [    0/  185]\n",
      "loss(143): 0.002910  [    0/  185]\n",
      "loss(144): 0.002779  [    0/  185]\n",
      "loss(145): 0.002491  [    0/  185]\n",
      "loss(146): 0.002456  [    0/  185]\n",
      "loss(147): 0.001687  [    0/  185]\n",
      "loss(148): 0.001445  [    0/  185]\n",
      "loss(149): 0.003485  [    0/  185]\n",
      "loss(150): 0.001267  [    0/  185]\n",
      "loss(151): 0.003050  [    0/  185]\n",
      "loss(152): 0.003143  [    0/  185]\n",
      "loss(153): 0.002176  [    0/  185]\n",
      "loss(154): 0.001813  [    0/  185]\n",
      "loss(155): 0.002449  [    0/  185]\n",
      "loss(156): 0.001534  [    0/  185]\n",
      "loss(157): 0.001934  [    0/  185]\n",
      "loss(158): 0.002139  [    0/  185]\n",
      "loss(159): 0.002500  [    0/  185]\n",
      "loss(160): 0.001632  [    0/  185]\n",
      "loss(161): 0.001667  [    0/  185]\n",
      "loss(162): 0.001932  [    0/  185]\n",
      "loss(163): 0.002127  [    0/  185]\n",
      "loss(164): 0.002389  [    0/  185]\n",
      "loss(165): 0.002550  [    0/  185]\n",
      "loss(166): 0.001502  [    0/  185]\n",
      "loss(167): 0.002088  [    0/  185]\n",
      "loss(168): 0.003521  [    0/  185]\n",
      "loss(169): 0.002246  [    0/  185]\n",
      "loss(170): 0.002715  [    0/  185]\n",
      "loss(171): 0.002216  [    0/  185]\n",
      "loss(172): 0.001331  [    0/  185]\n",
      "loss(173): 0.002126  [    0/  185]\n",
      "loss(174): 0.002123  [    0/  185]\n",
      "loss(175): 0.001710  [    0/  185]\n",
      "loss(176): 0.002627  [    0/  185]\n",
      "loss(177): 0.002750  [    0/  185]\n",
      "loss(178): 0.001372  [    0/  185]\n",
      "loss(179): 0.001815  [    0/  185]\n",
      "loss(180): 0.002169  [    0/  185]\n",
      "loss(181): 0.002388  [    0/  185]\n",
      "loss(182): 0.002573  [    0/  185]\n",
      "loss(183): 0.001681  [    0/  185]\n",
      "loss(184): 0.003004  [    0/  185]\n",
      "loss(185): 0.002565  [    0/  185]\n",
      "loss(186): 0.002129  [    0/  185]\n",
      "loss(187): 0.002286  [    0/  185]\n",
      "loss(188): 0.001899  [    0/  185]\n",
      "loss(189): 0.002355  [    0/  185]\n",
      "loss(190): 0.001799  [    0/  185]\n",
      "loss(191): 0.002240  [    0/  185]\n",
      "loss(192): 0.002036  [    0/  185]\n",
      "loss(193): 0.001821  [    0/  185]\n",
      "loss(194): 0.001826  [    0/  185]\n",
      "loss(195): 0.002071  [    0/  185]\n",
      "loss(196): 0.002604  [    0/  185]\n",
      "loss(197): 0.002414  [    0/  185]\n",
      "loss(198): 0.001983  [    0/  185]\n",
      "loss(199): 0.002746  [    0/  185]\n",
      "loss(200): 0.001991  [    0/  185]\n",
      "loss(201): 0.002181  [    0/  185]\n",
      "loss(202): 0.002158  [    0/  185]\n",
      "loss(203): 0.002051  [    0/  185]\n",
      "loss(204): 0.002661  [    0/  185]\n",
      "loss(205): 0.002325  [    0/  185]\n",
      "loss(206): 0.002340  [    0/  185]\n",
      "loss(207): 0.002685  [    0/  185]\n",
      "loss(208): 0.002736  [    0/  185]\n",
      "loss(209): 0.001750  [    0/  185]\n",
      "loss(210): 0.001946  [    0/  185]\n",
      "loss(211): 0.002041  [    0/  185]\n",
      "loss(212): 0.001779  [    0/  185]\n",
      "loss(213): 0.001357  [    0/  185]\n",
      "loss(214): 0.002308  [    0/  185]\n",
      "loss(215): 0.002318  [    0/  185]\n",
      "loss(216): 0.002426  [    0/  185]\n",
      "loss(217): 0.002542  [    0/  185]\n",
      "loss(218): 0.002382  [    0/  185]\n",
      "loss(219): 0.002624  [    0/  185]\n",
      "loss(220): 0.002045  [    0/  185]\n",
      "loss(221): 0.001915  [    0/  185]\n",
      "loss(222): 0.001612  [    0/  185]\n",
      "loss(223): 0.001855  [    0/  185]\n",
      "loss(224): 0.003204  [    0/  185]\n",
      "loss(225): 0.002170  [    0/  185]\n",
      "loss(226): 0.001453  [    0/  185]\n",
      "loss(227): 0.001811  [    0/  185]\n",
      "loss(228): 0.001464  [    0/  185]\n",
      "loss(229): 0.001963  [    0/  185]\n",
      "loss(230): 0.002249  [    0/  185]\n",
      "loss(231): 0.001863  [    0/  185]\n",
      "loss(232): 0.001523  [    0/  185]\n",
      "loss(233): 0.002339  [    0/  185]\n",
      "loss(234): 0.002153  [    0/  185]\n",
      "loss(235): 0.002093  [    0/  185]\n",
      "loss(236): 0.002072  [    0/  185]\n",
      "loss(237): 0.001534  [    0/  185]\n",
      "loss(238): 0.001418  [    0/  185]\n",
      "loss(239): 0.002224  [    0/  185]\n",
      "loss(240): 0.002150  [    0/  185]\n",
      "loss(241): 0.002034  [    0/  185]\n",
      "loss(242): 0.001865  [    0/  185]\n",
      "loss(243): 0.001553  [    0/  185]\n",
      "loss(244): 0.002401  [    0/  185]\n",
      "loss(245): 0.002169  [    0/  185]\n",
      "loss(246): 0.001913  [    0/  185]\n",
      "loss(247): 0.001500  [    0/  185]\n",
      "loss(248): 0.001544  [    0/  185]\n",
      "loss(249): 0.002081  [    0/  185]\n",
      "loss(250): 0.001824  [    0/  185]\n",
      "loss(251): 0.001685  [    0/  185]\n",
      "loss(252): 0.001960  [    0/  185]\n",
      "loss(253): 0.002203  [    0/  185]\n",
      "loss(254): 0.001662  [    0/  185]\n",
      "loss(255): 0.001964  [    0/  185]\n",
      "loss(256): 0.001519  [    0/  185]\n",
      "loss(257): 0.001662  [    0/  185]\n",
      "loss(258): 0.001807  [    0/  185]\n",
      "loss(259): 0.001733  [    0/  185]\n",
      "loss(260): 0.001488  [    0/  185]\n",
      "loss(261): 0.001287  [    0/  185]\n",
      "loss(262): 0.001920  [    0/  185]\n",
      "loss(263): 0.002360  [    0/  185]\n",
      "loss(264): 0.002272  [    0/  185]\n",
      "loss(265): 0.001210  [    0/  185]\n",
      "loss(266): 0.001660  [    0/  185]\n",
      "loss(267): 0.001503  [    0/  185]\n",
      "loss(268): 0.002597  [    0/  185]\n",
      "loss(269): 0.001655  [    0/  185]\n",
      "loss(270): 0.002079  [    0/  185]\n",
      "loss(271): 0.001527  [    0/  185]\n",
      "loss(272): 0.002270  [    0/  185]\n",
      "loss(273): 0.002504  [    0/  185]\n",
      "loss(274): 0.001727  [    0/  185]\n",
      "loss(275): 0.002263  [    0/  185]\n",
      "loss(276): 0.002274  [    0/  185]\n",
      "loss(277): 0.002852  [    0/  185]\n",
      "loss(278): 0.001932  [    0/  185]\n",
      "loss(279): 0.001737  [    0/  185]\n",
      "loss(280): 0.001456  [    0/  185]\n",
      "loss(281): 0.001787  [    0/  185]\n",
      "loss(282): 0.001743  [    0/  185]\n",
      "loss(283): 0.001800  [    0/  185]\n",
      "loss(284): 0.001585  [    0/  185]\n",
      "loss(285): 0.002140  [    0/  185]\n",
      "loss(286): 0.002289  [    0/  185]\n",
      "loss(287): 0.001882  [    0/  185]\n",
      "loss(288): 0.002250  [    0/  185]\n",
      "loss(289): 0.001370  [    0/  185]\n",
      "loss(290): 0.001954  [    0/  185]\n",
      "loss(291): 0.002074  [    0/  185]\n",
      "loss(292): 0.002489  [    0/  185]\n",
      "loss(293): 0.001499  [    0/  185]\n",
      "loss(294): 0.002258  [    0/  185]\n",
      "loss(295): 0.001947  [    0/  185]\n",
      "loss(296): 0.002091  [    0/  185]\n",
      "loss(297): 0.001886  [    0/  185]\n",
      "loss(298): 0.002191  [    0/  185]\n",
      "loss(299): 0.002102  [    0/  185]\n",
      "loss(300): 0.002214  [    0/  185]\n",
      "loss(301): 0.001733  [    0/  185]\n",
      "loss(302): 0.001703  [    0/  185]\n",
      "loss(303): 0.002109  [    0/  185]\n",
      "loss(304): 0.001980  [    0/  185]\n",
      "loss(305): 0.001855  [    0/  185]\n",
      "loss(306): 0.002244  [    0/  185]\n",
      "loss(307): 0.001074  [    0/  185]\n",
      "loss(308): 0.002357  [    0/  185]\n",
      "loss(309): 0.001897  [    0/  185]\n",
      "loss(310): 0.002050  [    0/  185]\n",
      "loss(311): 0.001602  [    0/  185]\n",
      "loss(312): 0.002098  [    0/  185]\n",
      "loss(313): 0.001387  [    0/  185]\n",
      "loss(314): 0.002076  [    0/  185]\n",
      "loss(315): 0.002342  [    0/  185]\n",
      "loss(316): 0.001340  [    0/  185]\n",
      "loss(317): 0.002014  [    0/  185]\n",
      "loss(318): 0.002021  [    0/  185]\n",
      "loss(319): 0.001866  [    0/  185]\n",
      "loss(320): 0.002345  [    0/  185]\n",
      "loss(321): 0.001842  [    0/  185]\n",
      "loss(322): 0.002171  [    0/  185]\n",
      "loss(323): 0.001551  [    0/  185]\n",
      "loss(324): 0.001601  [    0/  185]\n",
      "loss(325): 0.002126  [    0/  185]\n",
      "loss(326): 0.001535  [    0/  185]\n",
      "loss(327): 0.001479  [    0/  185]\n",
      "loss(328): 0.001991  [    0/  185]\n",
      "loss(329): 0.001860  [    0/  185]\n",
      "loss(330): 0.001578  [    0/  185]\n",
      "loss(331): 0.001911  [    0/  185]\n",
      "loss(332): 0.001780  [    0/  185]\n",
      "loss(333): 0.002258  [    0/  185]\n",
      "loss(334): 0.001554  [    0/  185]\n",
      "loss(335): 0.001188  [    0/  185]\n",
      "loss(336): 0.002139  [    0/  185]\n",
      "loss(337): 0.001349  [    0/  185]\n",
      "loss(338): 0.001357  [    0/  185]\n",
      "loss(339): 0.001502  [    0/  185]\n",
      "loss(340): 0.002055  [    0/  185]\n",
      "loss(341): 0.001800  [    0/  185]\n",
      "loss(342): 0.001260  [    0/  185]\n",
      "loss(343): 0.001455  [    0/  185]\n",
      "loss(344): 0.001459  [    0/  185]\n",
      "loss(345): 0.001075  [    0/  185]\n",
      "loss(346): 0.002178  [    0/  185]\n",
      "loss(347): 0.001914  [    0/  185]\n",
      "loss(348): 0.002689  [    0/  185]\n",
      "loss(349): 0.001668  [    0/  185]\n",
      "loss(350): 0.001909  [    0/  185]\n",
      "loss(351): 0.001487  [    0/  185]\n",
      "loss(352): 0.001686  [    0/  185]\n",
      "loss(353): 0.001538  [    0/  185]\n",
      "loss(354): 0.001574  [    0/  185]\n",
      "loss(355): 0.001203  [    0/  185]\n",
      "loss(356): 0.002240  [    0/  185]\n",
      "loss(357): 0.001776  [    0/  185]\n",
      "loss(358): 0.001369  [    0/  185]\n",
      "loss(359): 0.001740  [    0/  185]\n",
      "loss(360): 0.001196  [    0/  185]\n",
      "loss(361): 0.001580  [    0/  185]\n",
      "loss(362): 0.002011  [    0/  185]\n",
      "loss(363): 0.000972  [    0/  185]\n",
      "loss(364): 0.001710  [    0/  185]\n",
      "loss(365): 0.002170  [    0/  185]\n",
      "loss(366): 0.001903  [    0/  185]\n",
      "loss(367): 0.001734  [    0/  185]\n",
      "loss(368): 0.001215  [    0/  185]\n",
      "loss(369): 0.001664  [    0/  185]\n",
      "loss(370): 0.001979  [    0/  185]\n",
      "loss(371): 0.001938  [    0/  185]\n",
      "loss(372): 0.001592  [    0/  185]\n",
      "loss(373): 0.002019  [    0/  185]\n",
      "loss(374): 0.001498  [    0/  185]\n",
      "loss(375): 0.001966  [    0/  185]\n",
      "loss(376): 0.002226  [    0/  185]\n",
      "loss(377): 0.001732  [    0/  185]\n",
      "loss(378): 0.001985  [    0/  185]\n",
      "loss(379): 0.001484  [    0/  185]\n",
      "loss(380): 0.001686  [    0/  185]\n",
      "loss(381): 0.001663  [    0/  185]\n",
      "loss(382): 0.001615  [    0/  185]\n",
      "loss(383): 0.002132  [    0/  185]\n",
      "loss(384): 0.001640  [    0/  185]\n",
      "loss(385): 0.001881  [    0/  185]\n",
      "loss(386): 0.001594  [    0/  185]\n",
      "loss(387): 0.002088  [    0/  185]\n",
      "loss(388): 0.001367  [    0/  185]\n",
      "loss(389): 0.001278  [    0/  185]\n",
      "loss(390): 0.001772  [    0/  185]\n",
      "loss(391): 0.001708  [    0/  185]\n",
      "loss(392): 0.001349  [    0/  185]\n",
      "loss(393): 0.001392  [    0/  185]\n",
      "loss(394): 0.002248  [    0/  185]\n",
      "loss(395): 0.001920  [    0/  185]\n",
      "loss(396): 0.001816  [    0/  185]\n",
      "loss(397): 0.001556  [    0/  185]\n",
      "loss(398): 0.001787  [    0/  185]\n",
      "loss(399): 0.001189  [    0/  185]\n",
      "loss(400): 0.001727  [    0/  185]\n",
      "loss(401): 0.001880  [    0/  185]\n",
      "loss(402): 0.001919  [    0/  185]\n",
      "loss(403): 0.001282  [    0/  185]\n",
      "loss(404): 0.002134  [    0/  185]\n",
      "loss(405): 0.002201  [    0/  185]\n",
      "loss(406): 0.001856  [    0/  185]\n",
      "loss(407): 0.002294  [    0/  185]\n",
      "loss(408): 0.001231  [    0/  185]\n",
      "loss(409): 0.001909  [    0/  185]\n",
      "loss(410): 0.001440  [    0/  185]\n",
      "loss(411): 0.001680  [    0/  185]\n",
      "loss(412): 0.001974  [    0/  185]\n",
      "loss(413): 0.001985  [    0/  185]\n",
      "loss(414): 0.001542  [    0/  185]\n",
      "loss(415): 0.001921  [    0/  185]\n",
      "loss(416): 0.001269  [    0/  185]\n",
      "loss(417): 0.001967  [    0/  185]\n",
      "loss(418): 0.001688  [    0/  185]\n",
      "loss(419): 0.001908  [    0/  185]\n",
      "loss(420): 0.001377  [    0/  185]\n",
      "loss(421): 0.001738  [    0/  185]\n",
      "loss(422): 0.001793  [    0/  185]\n",
      "loss(423): 0.002201  [    0/  185]\n",
      "loss(424): 0.001690  [    0/  185]\n",
      "loss(425): 0.001340  [    0/  185]\n",
      "loss(426): 0.001486  [    0/  185]\n",
      "loss(427): 0.001830  [    0/  185]\n",
      "loss(428): 0.001360  [    0/  185]\n",
      "loss(429): 0.002056  [    0/  185]\n",
      "loss(430): 0.001971  [    0/  185]\n",
      "loss(431): 0.001744  [    0/  185]\n",
      "loss(432): 0.000872  [    0/  185]\n",
      "loss(433): 0.001404  [    0/  185]\n",
      "loss(434): 0.001038  [    0/  185]\n",
      "loss(435): 0.002080  [    0/  185]\n",
      "loss(436): 0.001776  [    0/  185]\n",
      "loss(437): 0.001866  [    0/  185]\n",
      "loss(438): 0.001624  [    0/  185]\n",
      "loss(439): 0.001879  [    0/  185]\n",
      "loss(440): 0.001594  [    0/  185]\n",
      "loss(441): 0.002040  [    0/  185]\n",
      "loss(442): 0.001999  [    0/  185]\n",
      "loss(443): 0.002097  [    0/  185]\n",
      "loss(444): 0.001513  [    0/  185]\n",
      "loss(445): 0.001509  [    0/  185]\n",
      "loss(446): 0.001761  [    0/  185]\n",
      "loss(447): 0.001956  [    0/  185]\n",
      "loss(448): 0.001313  [    0/  185]\n",
      "loss(449): 0.002350  [    0/  185]\n",
      "loss(450): 0.001225  [    0/  185]\n",
      "loss(451): 0.001716  [    0/  185]\n",
      "loss(452): 0.002331  [    0/  185]\n",
      "loss(453): 0.002532  [    0/  185]\n",
      "loss(454): 0.001966  [    0/  185]\n",
      "loss(455): 0.001394  [    0/  185]\n",
      "loss(456): 0.001885  [    0/  185]\n",
      "loss(457): 0.001815  [    0/  185]\n",
      "loss(458): 0.001889  [    0/  185]\n",
      "loss(459): 0.001028  [    0/  185]\n",
      "loss(460): 0.001153  [    0/  185]\n",
      "loss(461): 0.001903  [    0/  185]\n",
      "loss(462): 0.001598  [    0/  185]\n",
      "loss(463): 0.001612  [    0/  185]\n",
      "loss(464): 0.001843  [    0/  185]\n",
      "loss(465): 0.001517  [    0/  185]\n",
      "loss(466): 0.001389  [    0/  185]\n",
      "loss(467): 0.001201  [    0/  185]\n",
      "loss(468): 0.001369  [    0/  185]\n",
      "loss(469): 0.001256  [    0/  185]\n",
      "loss(470): 0.001103  [    0/  185]\n",
      "loss(471): 0.002018  [    0/  185]\n",
      "loss(472): 0.001423  [    0/  185]\n",
      "loss(473): 0.001933  [    0/  185]\n",
      "loss(474): 0.001111  [    0/  185]\n",
      "loss(475): 0.001578  [    0/  185]\n",
      "loss(476): 0.001917  [    0/  185]\n",
      "loss(477): 0.001663  [    0/  185]\n",
      "loss(478): 0.001310  [    0/  185]\n",
      "loss(479): 0.001528  [    0/  185]\n",
      "loss(480): 0.001200  [    0/  185]\n",
      "loss(481): 0.001864  [    0/  185]\n",
      "loss(482): 0.002029  [    0/  185]\n",
      "loss(483): 0.001079  [    0/  185]\n",
      "loss(484): 0.001845  [    0/  185]\n",
      "loss(485): 0.001528  [    0/  185]\n",
      "loss(486): 0.001500  [    0/  185]\n",
      "loss(487): 0.001355  [    0/  185]\n",
      "loss(488): 0.001091  [    0/  185]\n",
      "loss(489): 0.001771  [    0/  185]\n",
      "loss(490): 0.002007  [    0/  185]\n",
      "loss(491): 0.001405  [    0/  185]\n",
      "loss(492): 0.000957  [    0/  185]\n",
      "loss(493): 0.000997  [    0/  185]\n",
      "loss(494): 0.001348  [    0/  185]\n",
      "loss(495): 0.001599  [    0/  185]\n",
      "loss(496): 0.001452  [    0/  185]\n",
      "loss(497): 0.001675  [    0/  185]\n",
      "loss(498): 0.001410  [    0/  185]\n",
      "loss(499): 0.001338  [    0/  185]\n",
      "loss(500): 0.001855  [    0/  185]\n",
      "loss(501): 0.001461  [    0/  185]\n",
      "loss(502): 0.001282  [    0/  185]\n",
      "loss(503): 0.001484  [    0/  185]\n",
      "loss(504): 0.001080  [    0/  185]\n",
      "loss(505): 0.001289  [    0/  185]\n",
      "loss(506): 0.001245  [    0/  185]\n",
      "loss(507): 0.001980  [    0/  185]\n",
      "loss(508): 0.001065  [    0/  185]\n",
      "loss(509): 0.001787  [    0/  185]\n",
      "loss(510): 0.001481  [    0/  185]\n",
      "loss(511): 0.001512  [    0/  185]\n",
      "loss(512): 0.001387  [    0/  185]\n",
      "loss(513): 0.001297  [    0/  185]\n",
      "loss(514): 0.001408  [    0/  185]\n",
      "loss(515): 0.001701  [    0/  185]\n",
      "loss(516): 0.001220  [    0/  185]\n",
      "loss(517): 0.001331  [    0/  185]\n",
      "loss(518): 0.001248  [    0/  185]\n",
      "loss(519): 0.001585  [    0/  185]\n",
      "loss(520): 0.001358  [    0/  185]\n",
      "loss(521): 0.002057  [    0/  185]\n",
      "loss(522): 0.002047  [    0/  185]\n",
      "loss(523): 0.001433  [    0/  185]\n",
      "loss(524): 0.001211  [    0/  185]\n",
      "loss(525): 0.001282  [    0/  185]\n",
      "loss(526): 0.001026  [    0/  185]\n",
      "loss(527): 0.000999  [    0/  185]\n",
      "loss(528): 0.001238  [    0/  185]\n",
      "loss(529): 0.002039  [    0/  185]\n",
      "loss(530): 0.002255  [    0/  185]\n",
      "loss(531): 0.001784  [    0/  185]\n",
      "loss(532): 0.002134  [    0/  185]\n",
      "loss(533): 0.001616  [    0/  185]\n",
      "loss(534): 0.000899  [    0/  185]\n",
      "loss(535): 0.001219  [    0/  185]\n",
      "loss(536): 0.001340  [    0/  185]\n",
      "loss(537): 0.001558  [    0/  185]\n",
      "loss(538): 0.001164  [    0/  185]\n",
      "loss(539): 0.001659  [    0/  185]\n",
      "loss(540): 0.001991  [    0/  185]\n",
      "loss(541): 0.001385  [    0/  185]\n",
      "loss(542): 0.001829  [    0/  185]\n",
      "loss(543): 0.001280  [    0/  185]\n",
      "loss(544): 0.001336  [    0/  185]\n",
      "loss(545): 0.001807  [    0/  185]\n",
      "loss(546): 0.000892  [    0/  185]\n",
      "loss(547): 0.001117  [    0/  185]\n",
      "loss(548): 0.001656  [    0/  185]\n",
      "loss(549): 0.001405  [    0/  185]\n",
      "loss(550): 0.001006  [    0/  185]\n",
      "loss(551): 0.001327  [    0/  185]\n",
      "loss(552): 0.001738  [    0/  185]\n",
      "loss(553): 0.001034  [    0/  185]\n",
      "loss(554): 0.001634  [    0/  185]\n",
      "loss(555): 0.001221  [    0/  185]\n",
      "loss(556): 0.001104  [    0/  185]\n",
      "loss(557): 0.001079  [    0/  185]\n",
      "loss(558): 0.001658  [    0/  185]\n",
      "loss(559): 0.002105  [    0/  185]\n",
      "loss(560): 0.001029  [    0/  185]\n",
      "loss(561): 0.001164  [    0/  185]\n",
      "loss(562): 0.001465  [    0/  185]\n",
      "loss(563): 0.000842  [    0/  185]\n",
      "loss(564): 0.001422  [    0/  185]\n",
      "loss(565): 0.000927  [    0/  185]\n",
      "loss(566): 0.001377  [    0/  185]\n",
      "loss(567): 0.001236  [    0/  185]\n",
      "loss(568): 0.000612  [    0/  185]\n",
      "loss(569): 0.002302  [    0/  185]\n",
      "loss(570): 0.002276  [    0/  185]\n",
      "loss(571): 0.001615  [    0/  185]\n",
      "loss(572): 0.001062  [    0/  185]\n",
      "loss(573): 0.001384  [    0/  185]\n",
      "loss(574): 0.000996  [    0/  185]\n",
      "loss(575): 0.001235  [    0/  185]\n",
      "loss(576): 0.001095  [    0/  185]\n",
      "loss(577): 0.000962  [    0/  185]\n",
      "loss(578): 0.001150  [    0/  185]\n",
      "loss(579): 0.001324  [    0/  185]\n",
      "loss(580): 0.001661  [    0/  185]\n",
      "loss(581): 0.001280  [    0/  185]\n",
      "loss(582): 0.001241  [    0/  185]\n",
      "loss(583): 0.001305  [    0/  185]\n",
      "loss(584): 0.000930  [    0/  185]\n",
      "loss(585): 0.000912  [    0/  185]\n",
      "loss(586): 0.001563  [    0/  185]\n",
      "loss(587): 0.000678  [    0/  185]\n",
      "loss(588): 0.001104  [    0/  185]\n",
      "loss(589): 0.000748  [    0/  185]\n",
      "loss(590): 0.001519  [    0/  185]\n",
      "loss(591): 0.000613  [    0/  185]\n",
      "loss(592): 0.001654  [    0/  185]\n",
      "loss(593): 0.001029  [    0/  185]\n",
      "loss(594): 0.000692  [    0/  185]\n",
      "loss(595): 0.001352  [    0/  185]\n",
      "loss(596): 0.001544  [    0/  185]\n",
      "loss(597): 0.001186  [    0/  185]\n",
      "loss(598): 0.001360  [    0/  185]\n",
      "loss(599): 0.001406  [    0/  185]\n",
      "loss(600): 0.001428  [    0/  185]\n",
      "loss(601): 0.001092  [    0/  185]\n",
      "loss(602): 0.002071  [    0/  185]\n",
      "loss(603): 0.001293  [    0/  185]\n",
      "loss(604): 0.001751  [    0/  185]\n",
      "loss(605): 0.001328  [    0/  185]\n",
      "loss(606): 0.001027  [    0/  185]\n",
      "loss(607): 0.001069  [    0/  185]\n",
      "loss(608): 0.001071  [    0/  185]\n",
      "loss(609): 0.001091  [    0/  185]\n",
      "loss(610): 0.001464  [    0/  185]\n",
      "loss(611): 0.001256  [    0/  185]\n",
      "loss(612): 0.001355  [    0/  185]\n",
      "loss(613): 0.002319  [    0/  185]\n",
      "loss(614): 0.000926  [    0/  185]\n",
      "loss(615): 0.002047  [    0/  185]\n",
      "loss(616): 0.001072  [    0/  185]\n",
      "loss(617): 0.001062  [    0/  185]\n",
      "loss(618): 0.001969  [    0/  185]\n",
      "loss(619): 0.001565  [    0/  185]\n",
      "loss(620): 0.001830  [    0/  185]\n",
      "loss(621): 0.001051  [    0/  185]\n",
      "loss(622): 0.001346  [    0/  185]\n",
      "loss(623): 0.001281  [    0/  185]\n",
      "loss(624): 0.001235  [    0/  185]\n",
      "loss(625): 0.001808  [    0/  185]\n",
      "loss(626): 0.001796  [    0/  185]\n",
      "loss(627): 0.000974  [    0/  185]\n",
      "loss(628): 0.001509  [    0/  185]\n",
      "loss(629): 0.001847  [    0/  185]\n",
      "loss(630): 0.001191  [    0/  185]\n",
      "loss(631): 0.000847  [    0/  185]\n",
      "loss(632): 0.001512  [    0/  185]\n",
      "loss(633): 0.000854  [    0/  185]\n",
      "loss(634): 0.000864  [    0/  185]\n",
      "loss(635): 0.001326  [    0/  185]\n",
      "loss(636): 0.001484  [    0/  185]\n",
      "loss(637): 0.002227  [    0/  185]\n",
      "loss(638): 0.001653  [    0/  185]\n",
      "loss(639): 0.001075  [    0/  185]\n",
      "loss(640): 0.001905  [    0/  185]\n",
      "loss(641): 0.001038  [    0/  185]\n",
      "loss(642): 0.001326  [    0/  185]\n",
      "loss(643): 0.000902  [    0/  185]\n",
      "loss(644): 0.001048  [    0/  185]\n",
      "loss(645): 0.001209  [    0/  185]\n",
      "loss(646): 0.000979  [    0/  185]\n",
      "loss(647): 0.000705  [    0/  185]\n",
      "loss(648): 0.001289  [    0/  185]\n",
      "loss(649): 0.000958  [    0/  185]\n",
      "loss(650): 0.001358  [    0/  185]\n",
      "loss(651): 0.001053  [    0/  185]\n",
      "loss(652): 0.000939  [    0/  185]\n",
      "loss(653): 0.001797  [    0/  185]\n",
      "loss(654): 0.001605  [    0/  185]\n",
      "loss(655): 0.001314  [    0/  185]\n",
      "loss(656): 0.001676  [    0/  185]\n",
      "loss(657): 0.001135  [    0/  185]\n",
      "loss(658): 0.001674  [    0/  185]\n",
      "loss(659): 0.000864  [    0/  185]\n",
      "loss(660): 0.000798  [    0/  185]\n",
      "loss(661): 0.000946  [    0/  185]\n",
      "loss(662): 0.001786  [    0/  185]\n",
      "loss(663): 0.001562  [    0/  185]\n",
      "loss(664): 0.001276  [    0/  185]\n",
      "loss(665): 0.000863  [    0/  185]\n",
      "loss(666): 0.000866  [    0/  185]\n",
      "loss(667): 0.001264  [    0/  185]\n",
      "loss(668): 0.001057  [    0/  185]\n",
      "loss(669): 0.001084  [    0/  185]\n",
      "loss(670): 0.000597  [    0/  185]\n",
      "loss(671): 0.001178  [    0/  185]\n",
      "loss(672): 0.001455  [    0/  185]\n",
      "loss(673): 0.001428  [    0/  185]\n",
      "loss(674): 0.002197  [    0/  185]\n",
      "loss(675): 0.001328  [    0/  185]\n",
      "loss(676): 0.002004  [    0/  185]\n",
      "loss(677): 0.000869  [    0/  185]\n",
      "loss(678): 0.001405  [    0/  185]\n",
      "loss(679): 0.001775  [    0/  185]\n",
      "loss(680): 0.000662  [    0/  185]\n",
      "loss(681): 0.001625  [    0/  185]\n",
      "loss(682): 0.001432  [    0/  185]\n",
      "loss(683): 0.000785  [    0/  185]\n",
      "loss(684): 0.001108  [    0/  185]\n",
      "loss(685): 0.001784  [    0/  185]\n",
      "loss(686): 0.001455  [    0/  185]\n",
      "loss(687): 0.002038  [    0/  185]\n",
      "loss(688): 0.001609  [    0/  185]\n",
      "loss(689): 0.000668  [    0/  185]\n",
      "loss(690): 0.000697  [    0/  185]\n",
      "loss(691): 0.000763  [    0/  185]\n",
      "loss(692): 0.001814  [    0/  185]\n",
      "loss(693): 0.001179  [    0/  185]\n",
      "loss(694): 0.000637  [    0/  185]\n",
      "loss(695): 0.001199  [    0/  185]\n",
      "loss(696): 0.001595  [    0/  185]\n",
      "loss(697): 0.001062  [    0/  185]\n",
      "loss(698): 0.001231  [    0/  185]\n",
      "loss(699): 0.001429  [    0/  185]\n",
      "loss(700): 0.001789  [    0/  185]\n",
      "loss(701): 0.001200  [    0/  185]\n",
      "loss(702): 0.001694  [    0/  185]\n",
      "loss(703): 0.000826  [    0/  185]\n",
      "loss(704): 0.001288  [    0/  185]\n",
      "loss(705): 0.000877  [    0/  185]\n",
      "loss(706): 0.001703  [    0/  185]\n",
      "loss(707): 0.001523  [    0/  185]\n",
      "loss(708): 0.002050  [    0/  185]\n",
      "loss(709): 0.001833  [    0/  185]\n",
      "loss(710): 0.001312  [    0/  185]\n",
      "loss(711): 0.000868  [    0/  185]\n",
      "loss(712): 0.000962  [    0/  185]\n",
      "loss(713): 0.000954  [    0/  185]\n",
      "loss(714): 0.001518  [    0/  185]\n",
      "loss(715): 0.001176  [    0/  185]\n",
      "loss(716): 0.000991  [    0/  185]\n",
      "loss(717): 0.000936  [    0/  185]\n",
      "loss(718): 0.001334  [    0/  185]\n",
      "loss(719): 0.001007  [    0/  185]\n",
      "loss(720): 0.001606  [    0/  185]\n",
      "loss(721): 0.001199  [    0/  185]\n",
      "loss(722): 0.001075  [    0/  185]\n",
      "loss(723): 0.001802  [    0/  185]\n",
      "loss(724): 0.001272  [    0/  185]\n",
      "loss(725): 0.000639  [    0/  185]\n",
      "loss(726): 0.001260  [    0/  185]\n",
      "loss(727): 0.000984  [    0/  185]\n",
      "loss(728): 0.001158  [    0/  185]\n",
      "loss(729): 0.001233  [    0/  185]\n",
      "loss(730): 0.001276  [    0/  185]\n",
      "loss(731): 0.000679  [    0/  185]\n",
      "loss(732): 0.001301  [    0/  185]\n",
      "loss(733): 0.000974  [    0/  185]\n",
      "loss(734): 0.001024  [    0/  185]\n",
      "loss(735): 0.000896  [    0/  185]\n",
      "loss(736): 0.001528  [    0/  185]\n",
      "loss(737): 0.001173  [    0/  185]\n",
      "loss(738): 0.001309  [    0/  185]\n",
      "loss(739): 0.000834  [    0/  185]\n",
      "loss(740): 0.001774  [    0/  185]\n",
      "loss(741): 0.001174  [    0/  185]\n",
      "loss(742): 0.001578  [    0/  185]\n",
      "loss(743): 0.002043  [    0/  185]\n",
      "loss(744): 0.001004  [    0/  185]\n",
      "loss(745): 0.001866  [    0/  185]\n",
      "loss(746): 0.001142  [    0/  185]\n",
      "loss(747): 0.000760  [    0/  185]\n",
      "loss(748): 0.001221  [    0/  185]\n",
      "loss(749): 0.001429  [    0/  185]\n",
      "loss(750): 0.001470  [    0/  185]\n",
      "loss(751): 0.001142  [    0/  185]\n",
      "loss(752): 0.001285  [    0/  185]\n",
      "loss(753): 0.001760  [    0/  185]\n",
      "loss(754): 0.001486  [    0/  185]\n",
      "loss(755): 0.001397  [    0/  185]\n",
      "loss(756): 0.000883  [    0/  185]\n",
      "loss(757): 0.000860  [    0/  185]\n",
      "loss(758): 0.001242  [    0/  185]\n",
      "loss(759): 0.001449  [    0/  185]\n",
      "loss(760): 0.001235  [    0/  185]\n",
      "loss(761): 0.001248  [    0/  185]\n",
      "loss(762): 0.001182  [    0/  185]\n",
      "loss(763): 0.001230  [    0/  185]\n",
      "loss(764): 0.001253  [    0/  185]\n",
      "loss(765): 0.001291  [    0/  185]\n",
      "loss(766): 0.000735  [    0/  185]\n",
      "loss(767): 0.001398  [    0/  185]\n",
      "loss(768): 0.001514  [    0/  185]\n",
      "loss(769): 0.001160  [    0/  185]\n",
      "loss(770): 0.001410  [    0/  185]\n",
      "loss(771): 0.001154  [    0/  185]\n",
      "loss(772): 0.001452  [    0/  185]\n",
      "loss(773): 0.001157  [    0/  185]\n",
      "loss(774): 0.001271  [    0/  185]\n",
      "loss(775): 0.001815  [    0/  185]\n",
      "loss(776): 0.001489  [    0/  185]\n",
      "loss(777): 0.000834  [    0/  185]\n",
      "loss(778): 0.001666  [    0/  185]\n",
      "loss(779): 0.000825  [    0/  185]\n",
      "loss(780): 0.000846  [    0/  185]\n",
      "loss(781): 0.001423  [    0/  185]\n",
      "loss(782): 0.000977  [    0/  185]\n",
      "loss(783): 0.001591  [    0/  185]\n",
      "loss(784): 0.000967  [    0/  185]\n",
      "loss(785): 0.000983  [    0/  185]\n",
      "loss(786): 0.000919  [    0/  185]\n",
      "loss(787): 0.001220  [    0/  185]\n",
      "loss(788): 0.001166  [    0/  185]\n",
      "loss(789): 0.001117  [    0/  185]\n",
      "loss(790): 0.001484  [    0/  185]\n",
      "loss(791): 0.001066  [    0/  185]\n",
      "loss(792): 0.001090  [    0/  185]\n",
      "loss(793): 0.001146  [    0/  185]\n",
      "loss(794): 0.001209  [    0/  185]\n",
      "loss(795): 0.001065  [    0/  185]\n",
      "loss(796): 0.001141  [    0/  185]\n",
      "loss(797): 0.001859  [    0/  185]\n",
      "loss(798): 0.001227  [    0/  185]\n",
      "loss(799): 0.001053  [    0/  185]\n",
      "loss(800): 0.001247  [    0/  185]\n",
      "loss(801): 0.000703  [    0/  185]\n",
      "loss(802): 0.001079  [    0/  185]\n",
      "loss(803): 0.000994  [    0/  185]\n",
      "loss(804): 0.001384  [    0/  185]\n",
      "loss(805): 0.000762  [    0/  185]\n",
      "loss(806): 0.000793  [    0/  185]\n",
      "loss(807): 0.000806  [    0/  185]\n",
      "loss(808): 0.000817  [    0/  185]\n",
      "loss(809): 0.000787  [    0/  185]\n",
      "loss(810): 0.000721  [    0/  185]\n",
      "loss(811): 0.001578  [    0/  185]\n",
      "loss(812): 0.000684  [    0/  185]\n",
      "loss(813): 0.001003  [    0/  185]\n",
      "loss(814): 0.001433  [    0/  185]\n",
      "loss(815): 0.000924  [    0/  185]\n",
      "loss(816): 0.001205  [    0/  185]\n",
      "loss(817): 0.001146  [    0/  185]\n",
      "loss(818): 0.001549  [    0/  185]\n",
      "loss(819): 0.000642  [    0/  185]\n",
      "loss(820): 0.001543  [    0/  185]\n",
      "loss(821): 0.001351  [    0/  185]\n",
      "loss(822): 0.001109  [    0/  185]\n",
      "loss(823): 0.001218  [    0/  185]\n",
      "loss(824): 0.000815  [    0/  185]\n",
      "loss(825): 0.000612  [    0/  185]\n",
      "loss(826): 0.000871  [    0/  185]\n",
      "loss(827): 0.001130  [    0/  185]\n",
      "loss(828): 0.000779  [    0/  185]\n",
      "loss(829): 0.000790  [    0/  185]\n",
      "loss(830): 0.001353  [    0/  185]\n",
      "loss(831): 0.001054  [    0/  185]\n",
      "loss(832): 0.001328  [    0/  185]\n",
      "loss(833): 0.001190  [    0/  185]\n",
      "loss(834): 0.001557  [    0/  185]\n",
      "loss(835): 0.001471  [    0/  185]\n",
      "loss(836): 0.001179  [    0/  185]\n",
      "loss(837): 0.001545  [    0/  185]\n",
      "loss(838): 0.001458  [    0/  185]\n",
      "loss(839): 0.000484  [    0/  185]\n",
      "loss(840): 0.000751  [    0/  185]\n",
      "loss(841): 0.001226  [    0/  185]\n",
      "loss(842): 0.001017  [    0/  185]\n",
      "loss(843): 0.001277  [    0/  185]\n",
      "loss(844): 0.001461  [    0/  185]\n",
      "loss(845): 0.000930  [    0/  185]\n",
      "loss(846): 0.001172  [    0/  185]\n",
      "loss(847): 0.000891  [    0/  185]\n",
      "loss(848): 0.001266  [    0/  185]\n",
      "loss(849): 0.001224  [    0/  185]\n",
      "loss(850): 0.000914  [    0/  185]\n",
      "loss(851): 0.000819  [    0/  185]\n",
      "loss(852): 0.001268  [    0/  185]\n",
      "loss(853): 0.000973  [    0/  185]\n",
      "loss(854): 0.001389  [    0/  185]\n",
      "loss(855): 0.001246  [    0/  185]\n",
      "loss(856): 0.000852  [    0/  185]\n",
      "loss(857): 0.000490  [    0/  185]\n",
      "loss(858): 0.001489  [    0/  185]\n",
      "loss(859): 0.001509  [    0/  185]\n",
      "loss(860): 0.001063  [    0/  185]\n",
      "loss(861): 0.000773  [    0/  185]\n",
      "loss(862): 0.001336  [    0/  185]\n",
      "loss(863): 0.000855  [    0/  185]\n",
      "loss(864): 0.001329  [    0/  185]\n",
      "loss(865): 0.001626  [    0/  185]\n",
      "loss(866): 0.001005  [    0/  185]\n",
      "loss(867): 0.001031  [    0/  185]\n",
      "loss(868): 0.001340  [    0/  185]\n",
      "loss(869): 0.001154  [    0/  185]\n",
      "loss(870): 0.000993  [    0/  185]\n",
      "loss(871): 0.000917  [    0/  185]\n",
      "loss(872): 0.001050  [    0/  185]\n",
      "loss(873): 0.001159  [    0/  185]\n",
      "loss(874): 0.001153  [    0/  185]\n",
      "loss(875): 0.001403  [    0/  185]\n",
      "loss(876): 0.000804  [    0/  185]\n",
      "loss(877): 0.001445  [    0/  185]\n",
      "loss(878): 0.000969  [    0/  185]\n",
      "loss(879): 0.001130  [    0/  185]\n",
      "loss(880): 0.000828  [    0/  185]\n",
      "loss(881): 0.000573  [    0/  185]\n",
      "loss(882): 0.000814  [    0/  185]\n",
      "loss(883): 0.000968  [    0/  185]\n",
      "loss(884): 0.001477  [    0/  185]\n",
      "loss(885): 0.001196  [    0/  185]\n",
      "loss(886): 0.001348  [    0/  185]\n",
      "loss(887): 0.001498  [    0/  185]\n",
      "loss(888): 0.001282  [    0/  185]\n",
      "loss(889): 0.000709  [    0/  185]\n",
      "loss(890): 0.000744  [    0/  185]\n",
      "loss(891): 0.001309  [    0/  185]\n",
      "loss(892): 0.000706  [    0/  185]\n",
      "loss(893): 0.000850  [    0/  185]\n",
      "loss(894): 0.001001  [    0/  185]\n",
      "loss(895): 0.001069  [    0/  185]\n",
      "loss(896): 0.001162  [    0/  185]\n",
      "loss(897): 0.000987  [    0/  185]\n",
      "loss(898): 0.001078  [    0/  185]\n",
      "loss(899): 0.001345  [    0/  185]\n",
      "loss(900): 0.001106  [    0/  185]\n",
      "loss(901): 0.000514  [    0/  185]\n",
      "loss(902): 0.001286  [    0/  185]\n",
      "loss(903): 0.001252  [    0/  185]\n",
      "loss(904): 0.001636  [    0/  185]\n",
      "loss(905): 0.001204  [    0/  185]\n",
      "loss(906): 0.001351  [    0/  185]\n",
      "loss(907): 0.001154  [    0/  185]\n",
      "loss(908): 0.001593  [    0/  185]\n",
      "loss(909): 0.000668  [    0/  185]\n",
      "loss(910): 0.001134  [    0/  185]\n",
      "loss(911): 0.001445  [    0/  185]\n",
      "loss(912): 0.001525  [    0/  185]\n",
      "loss(913): 0.000711  [    0/  185]\n",
      "loss(914): 0.001316  [    0/  185]\n",
      "loss(915): 0.001647  [    0/  185]\n",
      "loss(916): 0.001285  [    0/  185]\n",
      "loss(917): 0.001332  [    0/  185]\n",
      "loss(918): 0.000670  [    0/  185]\n",
      "loss(919): 0.001043  [    0/  185]\n",
      "loss(920): 0.000826  [    0/  185]\n",
      "loss(921): 0.000871  [    0/  185]\n",
      "loss(922): 0.001040  [    0/  185]\n",
      "loss(923): 0.000985  [    0/  185]\n",
      "loss(924): 0.000687  [    0/  185]\n",
      "loss(925): 0.001024  [    0/  185]\n",
      "loss(926): 0.000683  [    0/  185]\n",
      "loss(927): 0.000805  [    0/  185]\n",
      "loss(928): 0.001001  [    0/  185]\n",
      "loss(929): 0.001032  [    0/  185]\n",
      "loss(930): 0.001004  [    0/  185]\n",
      "loss(931): 0.000973  [    0/  185]\n",
      "loss(932): 0.000710  [    0/  185]\n",
      "loss(933): 0.001488  [    0/  185]\n",
      "loss(934): 0.001083  [    0/  185]\n",
      "loss(935): 0.001164  [    0/  185]\n",
      "loss(936): 0.000726  [    0/  185]\n",
      "loss(937): 0.000999  [    0/  185]\n",
      "loss(938): 0.001195  [    0/  185]\n",
      "loss(939): 0.001146  [    0/  185]\n",
      "loss(940): 0.000697  [    0/  185]\n",
      "loss(941): 0.001175  [    0/  185]\n",
      "loss(942): 0.001022  [    0/  185]\n",
      "loss(943): 0.000707  [    0/  185]\n",
      "loss(944): 0.001616  [    0/  185]\n",
      "loss(945): 0.001081  [    0/  185]\n",
      "loss(946): 0.001095  [    0/  185]\n",
      "loss(947): 0.000775  [    0/  185]\n",
      "loss(948): 0.000845  [    0/  185]\n",
      "loss(949): 0.001003  [    0/  185]\n",
      "loss(950): 0.000729  [    0/  185]\n",
      "loss(951): 0.001108  [    0/  185]\n",
      "loss(952): 0.001274  [    0/  185]\n",
      "loss(953): 0.000949  [    0/  185]\n",
      "loss(954): 0.001416  [    0/  185]\n",
      "loss(955): 0.001261  [    0/  185]\n",
      "loss(956): 0.000900  [    0/  185]\n",
      "loss(957): 0.000904  [    0/  185]\n",
      "loss(958): 0.001442  [    0/  185]\n",
      "loss(959): 0.000792  [    0/  185]\n",
      "loss(960): 0.001394  [    0/  185]\n",
      "loss(961): 0.001071  [    0/  185]\n",
      "loss(962): 0.001014  [    0/  185]\n",
      "loss(963): 0.000734  [    0/  185]\n",
      "loss(964): 0.001228  [    0/  185]\n",
      "loss(965): 0.001388  [    0/  185]\n",
      "loss(966): 0.000831  [    0/  185]\n",
      "loss(967): 0.001057  [    0/  185]\n",
      "loss(968): 0.000960  [    0/  185]\n",
      "loss(969): 0.001388  [    0/  185]\n",
      "loss(970): 0.001228  [    0/  185]\n",
      "loss(971): 0.000884  [    0/  185]\n",
      "loss(972): 0.001034  [    0/  185]\n",
      "loss(973): 0.001443  [    0/  185]\n",
      "loss(974): 0.000807  [    0/  185]\n",
      "loss(975): 0.000837  [    0/  185]\n",
      "loss(976): 0.000969  [    0/  185]\n",
      "loss(977): 0.001030  [    0/  185]\n",
      "loss(978): 0.000655  [    0/  185]\n",
      "loss(979): 0.001218  [    0/  185]\n",
      "loss(980): 0.001369  [    0/  185]\n",
      "loss(981): 0.001152  [    0/  185]\n",
      "loss(982): 0.001198  [    0/  185]\n",
      "loss(983): 0.001043  [    0/  185]\n",
      "loss(984): 0.001078  [    0/  185]\n",
      "loss(985): 0.000724  [    0/  185]\n",
      "loss(986): 0.000851  [    0/  185]\n",
      "loss(987): 0.001050  [    0/  185]\n",
      "loss(988): 0.000830  [    0/  185]\n",
      "loss(989): 0.001146  [    0/  185]\n",
      "loss(990): 0.000830  [    0/  185]\n",
      "loss(991): 0.000710  [    0/  185]\n",
      "loss(992): 0.000716  [    0/  185]\n",
      "loss(993): 0.000977  [    0/  185]\n",
      "loss(994): 0.001125  [    0/  185]\n",
      "loss(995): 0.001131  [    0/  185]\n",
      "loss(996): 0.001006  [    0/  185]\n",
      "loss(997): 0.000766  [    0/  185]\n",
      "loss(998): 0.000971  [    0/  185]\n",
      "loss(999): 0.001168  [    0/  185]\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "learning_rate = .001\n",
    "epochs = 1000\n",
    "loss_fn = nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "# print(list(model.parameters()))\n",
    "\n",
    "# print(list(model.parameters()))\n",
    "for t in range(epochs):\n",
    "    # print(f\"Epoch {t+1}\\n-------------------------------\")\n",
    "    train_loop(train_dataloader, model, loss_fn, optimizer,t)\n",
    "    # break\n",
    "    # test_loop(test_dataloader, model, loss_fn)\n",
    "\n",
    "print(\"Done!\")\n",
    "# print(list(model.parameters()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Error:\n",
      "Avg loss: 0.000809 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "def test_loop(dataloader, model, loss_fn):\n",
    "    size = len(dataloader.dataset)\n",
    "    num_batches = len(dataloader)\n",
    "    test_loss= 0\n",
    "    with torch.no_grad():\n",
    "        for X, y in dataloader:\n",
    "            # print(X)\n",
    "            pred = model(X)\n",
    "            # print(pred)\n",
    "            test_loss += loss_fn(pred, y).item()\n",
    "\n",
    "    test_loss /= num_batches\n",
    "    print(f\"Test Error:\\nAvg loss: {test_loss:>8f} \\n\")\n",
    "\n",
    "def test(model):\n",
    "    norm = test_data.getNorm()\n",
    "    with torch.no_grad():\n",
    "        for idx in range(len(test_data)):\n",
    "            X = test_data[idx][0]\n",
    "            Y = test_data[idx][1]*norm\n",
    "            feat = X[None,:]\n",
    "            pred = model(feat)[0]*norm\n",
    "            pred = pred[:,0] \n",
    "            Y = Y[:,0]  \n",
    "            plt.plot(Y)\n",
    "            plt.plot(pred)\n",
    "            \n",
    "            plt.show()\n",
    "\n",
    "\n",
    "def graph(model):\n",
    "    norm = test_data.getNorm()\n",
    "    with torch.no_grad():\n",
    "        predY = []\n",
    "        actY = []\n",
    "        for idx in range(len(test_data)):\n",
    "            X = test_data[idx][0]\n",
    "            y = test_data[idx][1]*norm\n",
    "            feat = X[None,:]\n",
    "\n",
    "            pred = model(feat)[0]*norm\n",
    "            future = 3\n",
    "\n",
    "            # predY.append(pred[numFeat-1::][future])\n",
    "            # actY.append(y[numFeat-1::][future]) \n",
    "            pred = pred[:,0] \n",
    "            y = y[:,0]  \n",
    "            # print(pred.size())\n",
    "            # print(float(pred[3]))\n",
    "            predY.append(pred[future])\n",
    "            actY.append(y[future])    \n",
    "            \n",
    "        plt.plot(actY) \n",
    "        plt.plot(predY)\n",
    "        plt.show()\n",
    "\n",
    "graph(model)\n",
    "# test(model)\n",
    "test_loop(test_dataloader,model,loss_fn)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "a148e496c0f49d57628151d2aab378855c5a8a7aaacdf2673cbe18e166795068"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
