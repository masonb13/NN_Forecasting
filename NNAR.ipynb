{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 293,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets\n",
    "from torchvision.transforms import ToTensor\n",
    "from torch.utils.data import Dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 294,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "995\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# #import data from CDC\n",
    "# df = pd.read_csv(\"data\\FluViewPhase2Data\\WHO_NREVSS_Combined_prior_to_2015_16.csv\")\n",
    "\n",
    "# #split into training data and test data\n",
    "# train = df\n",
    "# # test = df[df[\"YEAR\"]>2012]\n",
    "\n",
    "# #turn data into features and output\n",
    "# #features: 5 previous + one from last year for predicted\n",
    "# #output: prediction for next time\n",
    "\n",
    "# #create test data\n",
    "# shift = 52\n",
    "# train_X = np.empty([5,len(train)-shift],dtype=int)\n",
    "# train_y = np.empty([1,len(train)-shift],dtype=int)\n",
    "# train_Data = []\n",
    "# for i in range(shift,len(train)):\n",
    "#     train_X[:,i-shift] = np.asarray(train.iloc[[i-1,i-2,i-3,i-4,i-52]][\"TOTAL\"]).T\n",
    "#     train_y[:,i-shift] = train.iloc[[i]][\"TOTAL\"]\n",
    "#     train_Data.append((train_X,train_y))\n",
    "# print(train_X[:,0])\n",
    "# print(train_y[:,0])\n",
    "#import data from CDC\n",
    "# df = pd.read_csv(\"data\\FluViewPhase2Data\\WHO_NREVSS_Combined_prior_to_2015_16(1).csv\")\n",
    "# series = df[\"TOTAL\"]\n",
    "# #turn data into features and output\n",
    "# #features: 5 previous + one from last year for predicted\n",
    "# #output: prediction for next time\n",
    "\n",
    "# #create test data\n",
    "\n",
    "\n",
    "# from pandas import read_csv\n",
    "# from matplotlib import pyplot\n",
    "# from statsmodels.graphics.tsaplots import plot_acf\n",
    "# plot_acf(series, lags=31)\n",
    "# pyplot.show()\n",
    "\n",
    "series = np.sin(.1*np.arange(1000))\n",
    "plt.plot(series)\n",
    "T=5\n",
    "X = np.empty([5,len(series)-T])\n",
    "Y = np.empty([1,len(series)-T])\n",
    "for i in range(len(series)-T):\n",
    "    X[:,i] = np.asarray(series[i:i+T])\n",
    "    Y[:,i] = series[i+T]\n",
    "print(Y.shape[1])\n",
    "# TRY WITH SINE\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Try with sin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 295,
   "metadata": {},
   "outputs": [],
   "source": [
    "class sinDataset(Dataset):\n",
    "    def __init__(self):\n",
    "        series = np.sin(.1*np.arange(1000))\n",
    "        T=5\n",
    "        X = np.empty([5,len(series)-T],dtype=np.float32)\n",
    "        Y = np.empty([1,len(series)-T],dtype=np.float32)\n",
    "        for i in range(len(series)-T):\n",
    "            X[:,i] = np.asarray(series[i:i+T])\n",
    "            Y[:,i] = series[i+T]\n",
    "        self.X=torch.from_numpy(X)\n",
    "        self.Y=torch.from_numpy(Y)\n",
    "    def __len__(self):\n",
    "        return self.Y.shape[1]\n",
    "    def __getitem__(self,idx):\n",
    "        return self.X[:,idx].T, self.Y[:,idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 296,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = sinDataset()\n",
    "training_data_sin, testing_data_sin = torch.utils.data.random_split(data,[.7,.3])\n",
    "train_dataloader_sin = DataLoader(training_data_sin,batch_size=64,shuffle=True)\n",
    "test_dataloader_sin = DataLoader(testing_data_sin,batch_size=64,shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 297,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create neural netowrk\n",
    "class NeuralNetworkSin(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(NeuralNetworkSin, self).__init__()\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.linear_relu_stack = nn.Sequential(\n",
    "            nn.Linear(5, 3),\n",
    "            nn.Sigmoid(),\n",
    "            nn.Linear(3, 1),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.flatten(x)\n",
    "        logits = self.linear_relu_stack(x)\n",
    "        return logits\n",
    "\n",
    "model_sin = NeuralNetworkSin()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 298,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train/test loop\n",
    "def train_loop(dataloader, model, loss_fn, optimizer):\n",
    "    size = len(dataloader.dataset)\n",
    "    for batch, (X, y) in enumerate(dataloader):\n",
    "        # Compute prediction and loss\n",
    "        pred = model(X)\n",
    "        loss = loss_fn(pred, y)\n",
    "        print(pred)\n",
    "        # print(y)\n",
    "        # Backpropagation\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if batch % size == 0:\n",
    "            loss, current = loss.item(), batch * len(X)\n",
    "            print(f\"loss: {loss:>7f}  [{current:>5d}/{size:>5d}]\")\n",
    "\n",
    "def test_loop(dataloader, model, loss_fn):\n",
    "    size = len(dataloader.dataset)\n",
    "    num_batches = len(dataloader)\n",
    "    test_loss, correct = 0, 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for X, y in dataloader:\n",
    "            # print(y)\n",
    "            pred = model(X)\n",
    "            # print(pred)\n",
    "            test_loss += loss_fn(pred, y).item()\n",
    "            correct += (pred.argmax(1) == y).type(torch.float).sum().item()\n",
    "\n",
    "    test_loss /= num_batches\n",
    "    correct /= size\n",
    "    print(f\"Test Error: Avg loss: {test_loss:>8f} \\n\")\n",
    "    # print(pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 299,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.6179],\n",
      "        [-0.4860],\n",
      "        [-0.4941],\n",
      "        [-0.6436],\n",
      "        [-0.6328],\n",
      "        [-0.6934],\n",
      "        [-0.6376],\n",
      "        [-0.6452],\n",
      "        [-0.5859],\n",
      "        [-0.6685],\n",
      "        [-0.6360],\n",
      "        [-0.6112],\n",
      "        [-0.6887],\n",
      "        [-0.4914],\n",
      "        [-0.5575],\n",
      "        [-0.6406],\n",
      "        [-0.6697],\n",
      "        [-0.4919],\n",
      "        [-0.5156],\n",
      "        [-0.5096],\n",
      "        [-0.5006],\n",
      "        [-0.5615],\n",
      "        [-0.6665],\n",
      "        [-0.5566],\n",
      "        [-0.6907],\n",
      "        [-0.6936],\n",
      "        [-0.6294],\n",
      "        [-0.6930],\n",
      "        [-0.6055],\n",
      "        [-0.5969],\n",
      "        [-0.4940],\n",
      "        [-0.6677],\n",
      "        [-0.4846],\n",
      "        [-0.5400],\n",
      "        [-0.5946],\n",
      "        [-0.6421],\n",
      "        [-0.5023],\n",
      "        [-0.6662],\n",
      "        [-0.5364],\n",
      "        [-0.6915],\n",
      "        [-0.5773],\n",
      "        [-0.6146],\n",
      "        [-0.6077],\n",
      "        [-0.4939],\n",
      "        [-0.5147],\n",
      "        [-0.6810],\n",
      "        [-0.4877],\n",
      "        [-0.4848],\n",
      "        [-0.6481],\n",
      "        [-0.4848],\n",
      "        [-0.4979],\n",
      "        [-0.4866],\n",
      "        [-0.5755],\n",
      "        [-0.6054],\n",
      "        [-0.5947],\n",
      "        [-0.6125],\n",
      "        [-0.4902],\n",
      "        [-0.6789],\n",
      "        [-0.5370],\n",
      "        [-0.4933],\n",
      "        [-0.6926],\n",
      "        [-0.5864],\n",
      "        [-0.4847],\n",
      "        [-0.6899]], grad_fn=<AddmmBackward0>)\n",
      "loss: 0.914937  [    0/  697]\n",
      "tensor([[-0.5476],\n",
      "        [-0.6085],\n",
      "        [-0.6907],\n",
      "        [-0.5893],\n",
      "        [-0.5659],\n",
      "        [-0.6692],\n",
      "        [-0.6271],\n",
      "        [-0.6527],\n",
      "        [-0.6891],\n",
      "        [-0.6104],\n",
      "        [-0.5646],\n",
      "        [-0.6171],\n",
      "        [-0.6874],\n",
      "        [-0.6733],\n",
      "        [-0.6867],\n",
      "        [-0.6172],\n",
      "        [-0.4838],\n",
      "        [-0.5559],\n",
      "        [-0.6910],\n",
      "        [-0.6631],\n",
      "        [-0.6369],\n",
      "        [-0.6736],\n",
      "        [-0.4827],\n",
      "        [-0.6304],\n",
      "        [-0.5926],\n",
      "        [-0.6911],\n",
      "        [-0.4927],\n",
      "        [-0.4828],\n",
      "        [-0.5911],\n",
      "        [-0.4888],\n",
      "        [-0.5561],\n",
      "        [-0.6603],\n",
      "        [-0.6335],\n",
      "        [-0.6224],\n",
      "        [-0.4841],\n",
      "        [-0.6848],\n",
      "        [-0.4833],\n",
      "        [-0.5454],\n",
      "        [-0.6614],\n",
      "        [-0.6626],\n",
      "        [-0.4829],\n",
      "        [-0.4830],\n",
      "        [-0.6555],\n",
      "        [-0.4985],\n",
      "        [-0.4912],\n",
      "        [-0.5165],\n",
      "        [-0.5284],\n",
      "        [-0.5363],\n",
      "        [-0.4928],\n",
      "        [-0.6905],\n",
      "        [-0.4888],\n",
      "        [-0.6713],\n",
      "        [-0.4928],\n",
      "        [-0.6336],\n",
      "        [-0.6911],\n",
      "        [-0.4866],\n",
      "        [-0.5079],\n",
      "        [-0.6018],\n",
      "        [-0.6864],\n",
      "        [-0.5819],\n",
      "        [-0.6842],\n",
      "        [-0.5313],\n",
      "        [-0.6272],\n",
      "        [-0.4837]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[-0.6025],\n",
      "        [-0.4849],\n",
      "        [-0.4840],\n",
      "        [-0.5003],\n",
      "        [-0.5218],\n",
      "        [-0.6659],\n",
      "        [-0.5351],\n",
      "        [-0.6432],\n",
      "        [-0.6766],\n",
      "        [-0.6875],\n",
      "        [-0.6095],\n",
      "        [-0.6773],\n",
      "        [-0.5139],\n",
      "        [-0.6802],\n",
      "        [-0.5077],\n",
      "        [-0.6882],\n",
      "        [-0.5208],\n",
      "        [-0.6460],\n",
      "        [-0.4819],\n",
      "        [-0.6061],\n",
      "        [-0.6774],\n",
      "        [-0.6841],\n",
      "        [-0.4807],\n",
      "        [-0.6857],\n",
      "        [-0.6007],\n",
      "        [-0.4921],\n",
      "        [-0.5480],\n",
      "        [-0.6499],\n",
      "        [-0.5012],\n",
      "        [-0.6027],\n",
      "        [-0.5191],\n",
      "        [-0.5002],\n",
      "        [-0.4912],\n",
      "        [-0.4806],\n",
      "        [-0.4811],\n",
      "        [-0.5464],\n",
      "        [-0.4891],\n",
      "        [-0.5710],\n",
      "        [-0.5180],\n",
      "        [-0.5670],\n",
      "        [-0.4807],\n",
      "        [-0.5830],\n",
      "        [-0.6883],\n",
      "        [-0.4929],\n",
      "        [-0.6844],\n",
      "        [-0.6228],\n",
      "        [-0.4840],\n",
      "        [-0.5000],\n",
      "        [-0.6113],\n",
      "        [-0.6848],\n",
      "        [-0.5013],\n",
      "        [-0.5704],\n",
      "        [-0.5138],\n",
      "        [-0.6278],\n",
      "        [-0.6854],\n",
      "        [-0.4897],\n",
      "        [-0.6095],\n",
      "        [-0.5151],\n",
      "        [-0.6457],\n",
      "        [-0.5101],\n",
      "        [-0.6446],\n",
      "        [-0.6880],\n",
      "        [-0.6812],\n",
      "        [-0.6471]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[-0.4905],\n",
      "        [-0.6683],\n",
      "        [-0.5704],\n",
      "        [-0.6654],\n",
      "        [-0.4973],\n",
      "        [-0.5006],\n",
      "        [-0.6160],\n",
      "        [-0.6851],\n",
      "        [-0.4828],\n",
      "        [-0.4838],\n",
      "        [-0.6422],\n",
      "        [-0.6616],\n",
      "        [-0.6764],\n",
      "        [-0.6351],\n",
      "        [-0.6861],\n",
      "        [-0.6855],\n",
      "        [-0.5050],\n",
      "        [-0.6494],\n",
      "        [-0.6771],\n",
      "        [-0.6337],\n",
      "        [-0.4867],\n",
      "        [-0.6707],\n",
      "        [-0.5204],\n",
      "        [-0.5954],\n",
      "        [-0.5483],\n",
      "        [-0.4805],\n",
      "        [-0.5434],\n",
      "        [-0.5530],\n",
      "        [-0.4789],\n",
      "        [-0.4966],\n",
      "        [-0.6463],\n",
      "        [-0.4850],\n",
      "        [-0.5920],\n",
      "        [-0.5215],\n",
      "        [-0.5568],\n",
      "        [-0.5120],\n",
      "        [-0.6727],\n",
      "        [-0.6367],\n",
      "        [-0.6546],\n",
      "        [-0.4808],\n",
      "        [-0.6463],\n",
      "        [-0.6108],\n",
      "        [-0.6338],\n",
      "        [-0.6393],\n",
      "        [-0.5072],\n",
      "        [-0.5191],\n",
      "        [-0.6503],\n",
      "        [-0.4838],\n",
      "        [-0.4819],\n",
      "        [-0.6275],\n",
      "        [-0.6719],\n",
      "        [-0.5062],\n",
      "        [-0.6814],\n",
      "        [-0.5291],\n",
      "        [-0.6827],\n",
      "        [-0.6006],\n",
      "        [-0.4797],\n",
      "        [-0.6848],\n",
      "        [-0.5050],\n",
      "        [-0.5865],\n",
      "        [-0.6091],\n",
      "        [-0.6507],\n",
      "        [-0.5099],\n",
      "        [-0.5721]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[-0.6798],\n",
      "        [-0.6797],\n",
      "        [-0.5181],\n",
      "        [-0.5226],\n",
      "        [-0.4954],\n",
      "        [-0.5088],\n",
      "        [-0.5717],\n",
      "        [-0.5364],\n",
      "        [-0.5156],\n",
      "        [-0.6747],\n",
      "        [-0.4769],\n",
      "        [-0.4964],\n",
      "        [-0.5665],\n",
      "        [-0.4926],\n",
      "        [-0.6752],\n",
      "        [-0.6834],\n",
      "        [-0.5446],\n",
      "        [-0.4984],\n",
      "        [-0.6516],\n",
      "        [-0.6718],\n",
      "        [-0.5861],\n",
      "        [-0.5166],\n",
      "        [-0.6783],\n",
      "        [-0.5721],\n",
      "        [-0.6690],\n",
      "        [-0.5297],\n",
      "        [-0.6759],\n",
      "        [-0.5063],\n",
      "        [-0.4936],\n",
      "        [-0.6168],\n",
      "        [-0.6817],\n",
      "        [-0.5880],\n",
      "        [-0.6217],\n",
      "        [-0.5612],\n",
      "        [-0.5197],\n",
      "        [-0.6782],\n",
      "        [-0.5930],\n",
      "        [-0.6673],\n",
      "        [-0.4778],\n",
      "        [-0.5984],\n",
      "        [-0.5841],\n",
      "        [-0.6806],\n",
      "        [-0.4917],\n",
      "        [-0.6673],\n",
      "        [-0.4944],\n",
      "        [-0.4835],\n",
      "        [-0.5363],\n",
      "        [-0.5393],\n",
      "        [-0.4769],\n",
      "        [-0.5982],\n",
      "        [-0.6711],\n",
      "        [-0.4804],\n",
      "        [-0.6820],\n",
      "        [-0.5030],\n",
      "        [-0.5090],\n",
      "        [-0.5348],\n",
      "        [-0.6788],\n",
      "        [-0.6823],\n",
      "        [-0.6639],\n",
      "        [-0.4996],\n",
      "        [-0.6281],\n",
      "        [-0.6778],\n",
      "        [-0.6534],\n",
      "        [-0.5212]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[-0.5978],\n",
      "        [-0.5766],\n",
      "        [-0.6642],\n",
      "        [-0.5092],\n",
      "        [-0.4756],\n",
      "        [-0.5250],\n",
      "        [-0.6810],\n",
      "        [-0.5995],\n",
      "        [-0.5423],\n",
      "        [-0.4758],\n",
      "        [-0.6802],\n",
      "        [-0.4856],\n",
      "        [-0.6582],\n",
      "        [-0.5081],\n",
      "        [-0.6583],\n",
      "        [-0.6562],\n",
      "        [-0.4863],\n",
      "        [-0.6598],\n",
      "        [-0.6571],\n",
      "        [-0.4896],\n",
      "        [-0.4754],\n",
      "        [-0.5925],\n",
      "        [-0.5662],\n",
      "        [-0.5231],\n",
      "        [-0.6560],\n",
      "        [-0.5572],\n",
      "        [-0.4752],\n",
      "        [-0.6484],\n",
      "        [-0.4888],\n",
      "        [-0.6358],\n",
      "        [-0.6348],\n",
      "        [-0.4758],\n",
      "        [-0.4803],\n",
      "        [-0.6484],\n",
      "        [-0.6129],\n",
      "        [-0.5387],\n",
      "        [-0.5786],\n",
      "        [-0.4815],\n",
      "        [-0.5055],\n",
      "        [-0.5162],\n",
      "        [-0.6432],\n",
      "        [-0.6804],\n",
      "        [-0.6440],\n",
      "        [-0.6638],\n",
      "        [-0.6669],\n",
      "        [-0.6567],\n",
      "        [-0.5489],\n",
      "        [-0.6784],\n",
      "        [-0.6521],\n",
      "        [-0.5821],\n",
      "        [-0.5307],\n",
      "        [-0.6788],\n",
      "        [-0.6063],\n",
      "        [-0.6319],\n",
      "        [-0.5217],\n",
      "        [-0.6802],\n",
      "        [-0.4821],\n",
      "        [-0.6145],\n",
      "        [-0.6728],\n",
      "        [-0.4749],\n",
      "        [-0.5322],\n",
      "        [-0.6722],\n",
      "        [-0.5854],\n",
      "        [-0.4749]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[-0.6770],\n",
      "        [-0.6248],\n",
      "        [-0.6782],\n",
      "        [-0.6039],\n",
      "        [-0.6573],\n",
      "        [-0.6782],\n",
      "        [-0.5569],\n",
      "        [-0.5304],\n",
      "        [-0.6616],\n",
      "        [-0.6750],\n",
      "        [-0.6593],\n",
      "        [-0.6603],\n",
      "        [-0.6702],\n",
      "        [-0.5430],\n",
      "        [-0.4730],\n",
      "        [-0.4828],\n",
      "        [-0.4739],\n",
      "        [-0.4836],\n",
      "        [-0.4975],\n",
      "        [-0.5448],\n",
      "        [-0.6768],\n",
      "        [-0.6753],\n",
      "        [-0.4729],\n",
      "        [-0.6308],\n",
      "        [-0.6782],\n",
      "        [-0.6657],\n",
      "        [-0.5516],\n",
      "        [-0.4733],\n",
      "        [-0.5045],\n",
      "        [-0.6713],\n",
      "        [-0.4729],\n",
      "        [-0.4789],\n",
      "        [-0.5352],\n",
      "        [-0.5181],\n",
      "        [-0.5212],\n",
      "        [-0.5553],\n",
      "        [-0.4745],\n",
      "        [-0.4752],\n",
      "        [-0.5098],\n",
      "        [-0.5848],\n",
      "        [-0.5320],\n",
      "        [-0.6672],\n",
      "        [-0.4965],\n",
      "        [-0.6524],\n",
      "        [-0.5316],\n",
      "        [-0.6215],\n",
      "        [-0.5332],\n",
      "        [-0.6123],\n",
      "        [-0.5725],\n",
      "        [-0.6365],\n",
      "        [-0.5048],\n",
      "        [-0.6761],\n",
      "        [-0.5035],\n",
      "        [-0.5866],\n",
      "        [-0.4867],\n",
      "        [-0.6765],\n",
      "        [-0.6568],\n",
      "        [-0.6761],\n",
      "        [-0.4876],\n",
      "        [-0.6634],\n",
      "        [-0.5656],\n",
      "        [-0.5954],\n",
      "        [-0.6021],\n",
      "        [-0.6072]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[-0.6312],\n",
      "        [-0.6457],\n",
      "        [-0.4830],\n",
      "        [-0.6682],\n",
      "        [-0.6389],\n",
      "        [-0.6445],\n",
      "        [-0.4845],\n",
      "        [-0.6550],\n",
      "        [-0.5475],\n",
      "        [-0.5561],\n",
      "        [-0.6655],\n",
      "        [-0.6221],\n",
      "        [-0.6032],\n",
      "        [-0.4762],\n",
      "        [-0.5001],\n",
      "        [-0.6048],\n",
      "        [-0.6408],\n",
      "        [-0.5145],\n",
      "        [-0.6757],\n",
      "        [-0.5376],\n",
      "        [-0.6114],\n",
      "        [-0.5877],\n",
      "        [-0.4954],\n",
      "        [-0.5202],\n",
      "        [-0.6698],\n",
      "        [-0.6681],\n",
      "        [-0.6049],\n",
      "        [-0.5492],\n",
      "        [-0.5144],\n",
      "        [-0.5131],\n",
      "        [-0.4714],\n",
      "        [-0.5580],\n",
      "        [-0.6687],\n",
      "        [-0.6662],\n",
      "        [-0.6525],\n",
      "        [-0.6250],\n",
      "        [-0.4815],\n",
      "        [-0.5649],\n",
      "        [-0.4768],\n",
      "        [-0.6630],\n",
      "        [-0.6754],\n",
      "        [-0.6486],\n",
      "        [-0.5945],\n",
      "        [-0.6493],\n",
      "        [-0.6111],\n",
      "        [-0.6293],\n",
      "        [-0.6470],\n",
      "        [-0.6047],\n",
      "        [-0.5359],\n",
      "        [-0.5343],\n",
      "        [-0.5392],\n",
      "        [-0.4757],\n",
      "        [-0.6082],\n",
      "        [-0.4728],\n",
      "        [-0.6401],\n",
      "        [-0.5894],\n",
      "        [-0.6746],\n",
      "        [-0.4715],\n",
      "        [-0.6474],\n",
      "        [-0.5160],\n",
      "        [-0.5596],\n",
      "        [-0.6605],\n",
      "        [-0.5065],\n",
      "        [-0.5946]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[-0.6342],\n",
      "        [-0.4713],\n",
      "        [-0.5938],\n",
      "        [-0.6178],\n",
      "        [-0.6610],\n",
      "        [-0.6400],\n",
      "        [-0.4726],\n",
      "        [-0.5041],\n",
      "        [-0.6072],\n",
      "        [-0.5124],\n",
      "        [-0.4911],\n",
      "        [-0.5081],\n",
      "        [-0.6728],\n",
      "        [-0.6704],\n",
      "        [-0.6666],\n",
      "        [-0.6349],\n",
      "        [-0.6037],\n",
      "        [-0.6336],\n",
      "        [-0.4954],\n",
      "        [-0.5592],\n",
      "        [-0.4860],\n",
      "        [-0.5368],\n",
      "        [-0.6394],\n",
      "        [-0.5800],\n",
      "        [-0.6147],\n",
      "        [-0.5256],\n",
      "        [-0.4695],\n",
      "        [-0.5195],\n",
      "        [-0.5067],\n",
      "        [-0.6713],\n",
      "        [-0.6713],\n",
      "        [-0.4870],\n",
      "        [-0.5869],\n",
      "        [-0.5834],\n",
      "        [-0.6102],\n",
      "        [-0.6342],\n",
      "        [-0.5589],\n",
      "        [-0.4890],\n",
      "        [-0.5902],\n",
      "        [-0.6719],\n",
      "        [-0.4731],\n",
      "        [-0.6633],\n",
      "        [-0.4954],\n",
      "        [-0.4703],\n",
      "        [-0.5642],\n",
      "        [-0.5385],\n",
      "        [-0.4687],\n",
      "        [-0.6068],\n",
      "        [-0.5678],\n",
      "        [-0.5818],\n",
      "        [-0.6728],\n",
      "        [-0.6687],\n",
      "        [-0.4943],\n",
      "        [-0.5799],\n",
      "        [-0.6725],\n",
      "        [-0.5731],\n",
      "        [-0.5714],\n",
      "        [-0.4687],\n",
      "        [-0.6552],\n",
      "        [-0.6052],\n",
      "        [-0.6677],\n",
      "        [-0.4870],\n",
      "        [-0.5288],\n",
      "        [-0.5486]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[-0.5829],\n",
      "        [-0.4688],\n",
      "        [-0.4666],\n",
      "        [-0.6488],\n",
      "        [-0.6484],\n",
      "        [-0.5949],\n",
      "        [-0.4682],\n",
      "        [-0.6081],\n",
      "        [-0.4899],\n",
      "        [-0.5019],\n",
      "        [-0.6015],\n",
      "        [-0.5516],\n",
      "        [-0.4701],\n",
      "        [-0.4848],\n",
      "        [-0.5995],\n",
      "        [-0.5690],\n",
      "        [-0.5250],\n",
      "        [-0.5654],\n",
      "        [-0.6656],\n",
      "        [-0.6681],\n",
      "        [-0.6398],\n",
      "        [-0.6127],\n",
      "        [-0.6423],\n",
      "        [-0.6096],\n",
      "        [-0.6323],\n",
      "        [-0.5158],\n",
      "        [-0.5793],\n",
      "        [-0.6627],\n",
      "        [-0.5019],\n",
      "        [-0.5585],\n",
      "        [-0.6579],\n",
      "        [-0.5966],\n",
      "        [-0.5980],\n",
      "        [-0.6303],\n",
      "        [-0.5190],\n",
      "        [-0.6275],\n",
      "        [-0.4795],\n",
      "        [-0.6684],\n",
      "        [-0.5688],\n",
      "        [-0.5219],\n",
      "        [-0.6283],\n",
      "        [-0.6571],\n",
      "        [-0.6213],\n",
      "        [-0.6600],\n",
      "        [-0.6703],\n",
      "        [-0.5088],\n",
      "        [-0.6269],\n",
      "        [-0.6233],\n",
      "        [-0.5447],\n",
      "        [-0.4771],\n",
      "        [-0.6452],\n",
      "        [-0.5413],\n",
      "        [-0.4945],\n",
      "        [-0.5281],\n",
      "        [-0.5912],\n",
      "        [-0.5059],\n",
      "        [-0.6227],\n",
      "        [-0.6477],\n",
      "        [-0.5637],\n",
      "        [-0.4679],\n",
      "        [-0.6337],\n",
      "        [-0.6362],\n",
      "        [-0.4714],\n",
      "        [-0.4830]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[-0.6633],\n",
      "        [-0.6477],\n",
      "        [-0.4921],\n",
      "        [-0.4984],\n",
      "        [-0.5508],\n",
      "        [-0.5922],\n",
      "        [-0.5165],\n",
      "        [-0.4781],\n",
      "        [-0.5406],\n",
      "        [-0.5560],\n",
      "        [-0.6378],\n",
      "        [-0.6565],\n",
      "        [-0.5490],\n",
      "        [-0.4652],\n",
      "        [-0.5594],\n",
      "        [-0.6586],\n",
      "        [-0.4789],\n",
      "        [-0.6304],\n",
      "        [-0.5165],\n",
      "        [-0.6496],\n",
      "        [-0.4798],\n",
      "        [-0.6565],\n",
      "        [-0.5063],\n",
      "        [-0.4648],\n",
      "        [-0.4697],\n",
      "        [-0.6371],\n",
      "        [-0.6549],\n",
      "        [-0.6652],\n",
      "        [-0.5438],\n",
      "        [-0.4670],\n",
      "        [-0.4982],\n",
      "        [-0.6200],\n",
      "        [-0.6605],\n",
      "        [-0.4798],\n",
      "        [-0.4669],\n",
      "        [-0.4773],\n",
      "        [-0.5770],\n",
      "        [-0.5506],\n",
      "        [-0.5078],\n",
      "        [-0.5611],\n",
      "        [-0.6671],\n",
      "        [-0.4788],\n",
      "        [-0.5105],\n",
      "        [-0.6593],\n",
      "        [-0.6600],\n",
      "        [-0.6490],\n",
      "        [-0.6623],\n",
      "        [-0.6156],\n",
      "        [-0.6580],\n",
      "        [-0.4983],\n",
      "        [-0.6110],\n",
      "        [-0.6496],\n",
      "        [-0.4780],\n",
      "        [-0.6270],\n",
      "        [-0.5438],\n",
      "        [-0.5663],\n",
      "        [-0.6649]], grad_fn=<AddmmBackward0>)\n",
      "Test Error: Avg loss: 0.892236 \n",
      "\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "learning_rate = .001\n",
    "batch_size = 64\n",
    "epochs = 1\n",
    "loss_fn = nn.MSELoss()\n",
    "optimizer = torch.optim.SGD(model_sin.parameters(), lr=learning_rate)\n",
    "\n",
    "# print(list(model_sin.parameters()))\n",
    "for t in range(epochs):\n",
    "    # print(f\"Epoch {t+1}\\n-------------------------------\")\n",
    "    train_loop(train_dataloader_sin, model_sin, loss_fn, optimizer)\n",
    "    test_loop(test_dataloader_sin, model_sin, loss_fn)\n",
    "print(\"Done!\")\n",
    "# print(list(model.parameters()))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_______________________________________________________________________________-"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 300,
   "metadata": {},
   "outputs": [],
   "source": [
    "#torch data set\n",
    "class dataSetAll(Dataset):\n",
    "    def __init__(self, yearLow, yearHigh):\n",
    "        #import data from CDC\n",
    "        self.df = pd.read_csv(\"data\\FluViewPhase2Data\\WHO_NREVSS_Combined_prior_to_2015_16.csv\")\n",
    "        self.df = self.df[(yearLow <= self.df[\"YEAR\"]) & (self.df[\"YEAR\"] < yearHigh)]\n",
    "        #turn data into features and output\n",
    "        #features: 5 previous + one from last year for predicted\n",
    "        #output: prediction for next time\n",
    "\n",
    "        #create test data\n",
    "        shift = 3\n",
    "        self.X = np.empty([shift,len(self.df)-shift],dtype=np.float32)\n",
    "        self.Y = np.empty([1,len(self.df)-shift],dtype=np.float32)\n",
    "        for i in range(self.Y.shape[1]):\n",
    "            self.X[:,i] = np.asarray(self.df.iloc[i:i+shift][\"TOTAL\"])\n",
    "            self.Y[:,i] = self.df.iloc[[i+shift]][\"TOTAL\"]\n",
    "\n",
    "        self.X = torch.from_numpy(self.X)\n",
    "        self.Y = torch.from_numpy(self.Y)\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.Y.shape[1]\n",
    "    \n",
    "    def __getitem__(self,idx):\n",
    "        return self.X[:,idx].T, self.Y[:,idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 301,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create data loaders\n",
    "train_data = dataSetAll(1900,2013)\n",
    "test_data = dataSetAll(2013,2100)\n",
    "train_dataloader = DataLoader(train_data, batch_size=64)\n",
    "test_dataloader = DataLoader(test_data, batch_size=64)\n",
    "# print(list(test_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 302,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create neural netowrk\n",
    "class NeuralNetwork(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(NeuralNetwork, self).__init__()\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.linear_relu_stack = nn.Sequential(\n",
    "            nn.Linear(3, 10),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(10, 1),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.flatten(x)\n",
    "        logits = self.linear_relu_stack(x)\n",
    "        return logits\n",
    "\n",
    "model = NeuralNetwork()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 303,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train/test loop\n",
    "def train_loop(dataloader, model, loss_fn, optimizer):\n",
    "    size = len(dataloader.dataset)\n",
    "    for batch, (X, y) in enumerate(dataloader):\n",
    "        # Compute prediction and loss\n",
    "        pred = model(X)\n",
    "        loss = loss_fn(pred, y)\n",
    "        # print(pred)\n",
    "        # print(y)\n",
    "        # Backpropagation\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if batch % size == 0:\n",
    "            loss, current = loss.item(), batch * len(X)\n",
    "            print(f\"loss: {loss:>7f}  [{current:>5d}/{size:>5d}]\")\n",
    "\n",
    "def test_loop(dataloader, model, loss_fn):\n",
    "    size = len(dataloader.dataset)\n",
    "    num_batches = len(dataloader)\n",
    "    test_loss, correct = 0, 0\n",
    "    y1 = torch.tensor(0)\n",
    "    with torch.no_grad():\n",
    "        for X, y in dataloader:\n",
    "            # print(X)\n",
    "            pred = model(X)\n",
    "            # print(pred)\n",
    "            test_loss += loss_fn(pred, y).item()\n",
    "            correct += (pred.argmax(1) == y).type(torch.float).sum().item()\n",
    "            y1 = y\n",
    "\n",
    "    test_loss /= num_batches\n",
    "    correct /= size\n",
    "    print(f\"Test Error:\\nAvg loss: {test_loss:>8f} \\n\")\n",
    "    return pred, y1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 304,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss: 5709984.000000  [    0/  790]\n",
      "Test Error:\n",
      "Avg loss: 10657619.041667 \n",
      "\n",
      "loss: 822978.312500  [    0/  790]\n",
      "Test Error:\n",
      "Avg loss: 9020141.208333 \n",
      "\n",
      "loss: 727122.750000  [    0/  790]\n",
      "Test Error:\n",
      "Avg loss: 8305671.208333 \n",
      "\n",
      "loss: 670576.562500  [    0/  790]\n",
      "Test Error:\n",
      "Avg loss: 7647259.416667 \n",
      "\n",
      "loss: 620016.437500  [    0/  790]\n",
      "Test Error:\n",
      "Avg loss: 7070340.812500 \n",
      "\n",
      "loss: 572002.000000  [    0/  790]\n",
      "Test Error:\n",
      "Avg loss: 6455594.354167 \n",
      "\n",
      "loss: 517297.750000  [    0/  790]\n",
      "Test Error:\n",
      "Avg loss: 5567557.687500 \n",
      "\n",
      "loss: 460528.625000  [    0/  790]\n",
      "Test Error:\n",
      "Avg loss: 4986147.250000 \n",
      "\n",
      "loss: 423558.062500  [    0/  790]\n",
      "Test Error:\n",
      "Avg loss: 4597143.833333 \n",
      "\n",
      "loss: 396182.781250  [    0/  790]\n",
      "Test Error:\n",
      "Avg loss: 4217654.020833 \n",
      "\n",
      "loss: 371197.125000  [    0/  790]\n",
      "Test Error:\n",
      "Avg loss: 3938862.614583 \n",
      "\n",
      "loss: 350960.125000  [    0/  790]\n",
      "Test Error:\n",
      "Avg loss: 3668026.531250 \n",
      "\n",
      "loss: 331967.406250  [    0/  790]\n",
      "Test Error:\n",
      "Avg loss: 3436312.083333 \n",
      "\n",
      "loss: 315446.343750  [    0/  790]\n",
      "Test Error:\n",
      "Avg loss: 3231114.000000 \n",
      "\n",
      "loss: 301136.031250  [    0/  790]\n",
      "Test Error:\n",
      "Avg loss: 3050341.552083 \n",
      "\n",
      "loss: 288709.781250  [    0/  790]\n",
      "Test Error:\n",
      "Avg loss: 2891343.302083 \n",
      "\n",
      "loss: 277952.562500  [    0/  790]\n",
      "Test Error:\n",
      "Avg loss: 2751901.250000 \n",
      "\n",
      "loss: 268677.281250  [    0/  790]\n",
      "Test Error:\n",
      "Avg loss: 2630011.354167 \n",
      "\n",
      "loss: 260712.703125  [    0/  790]\n",
      "Test Error:\n",
      "Avg loss: 2521801.489583 \n",
      "\n",
      "loss: 253819.812500  [    0/  790]\n",
      "Test Error:\n",
      "Avg loss: 2428797.364583 \n",
      "\n",
      "loss: 247988.359375  [    0/  790]\n",
      "Test Error:\n",
      "Avg loss: 2347906.635417 \n",
      "\n",
      "loss: 243026.156250  [    0/  790]\n",
      "Test Error:\n",
      "Avg loss: 2282232.354167 \n",
      "\n",
      "loss: 239020.312500  [    0/  790]\n",
      "Test Error:\n",
      "Avg loss: 2219934.625000 \n",
      "\n",
      "loss: 235391.843750  [    0/  790]\n",
      "Test Error:\n",
      "Avg loss: 2165812.354167 \n",
      "\n",
      "loss: 232313.968750  [    0/  790]\n",
      "Test Error:\n",
      "Avg loss: 2119201.166667 \n",
      "\n",
      "loss: 229727.671875  [    0/  790]\n",
      "Test Error:\n",
      "Avg loss: 2078762.687500 \n",
      "\n",
      "loss: 227548.093750  [    0/  790]\n",
      "Test Error:\n",
      "Avg loss: 2043708.505208 \n",
      "\n",
      "loss: 225718.656250  [    0/  790]\n",
      "Test Error:\n",
      "Avg loss: 2013330.291667 \n",
      "\n",
      "loss: 224189.750000  [    0/  790]\n",
      "Test Error:\n",
      "Avg loss: 1987012.328125 \n",
      "\n",
      "loss: 222919.015625  [    0/  790]\n",
      "Test Error:\n",
      "Avg loss: 1964213.755208 \n",
      "\n",
      "loss: 221869.593750  [    0/  790]\n",
      "Test Error:\n",
      "Avg loss: 1945192.109375 \n",
      "\n",
      "loss: 221052.109375  [    0/  790]\n",
      "Test Error:\n",
      "Avg loss: 1928435.171875 \n",
      "\n",
      "loss: 220381.906250  [    0/  790]\n",
      "Test Error:\n",
      "Avg loss: 1914003.348958 \n",
      "\n",
      "loss: 219854.140625  [    0/  790]\n",
      "Test Error:\n",
      "Avg loss: 1901571.276042 \n",
      "\n",
      "loss: 219447.953125  [    0/  790]\n",
      "Test Error:\n",
      "Avg loss: 1890873.817708 \n",
      "\n",
      "loss: 219145.937500  [    0/  790]\n",
      "Test Error:\n",
      "Avg loss: 1881679.520833 \n",
      "\n",
      "loss: 218933.343750  [    0/  790]\n",
      "Test Error:\n",
      "Avg loss: 1870227.921875 \n",
      "\n",
      "loss: 218684.781250  [    0/  790]\n",
      "Test Error:\n",
      "Avg loss: 1867772.682292 \n",
      "\n",
      "loss: 218798.515625  [    0/  790]\n",
      "Test Error:\n",
      "Avg loss: 1862222.088542 \n",
      "\n",
      "loss: 218796.156250  [    0/  790]\n",
      "Test Error:\n",
      "Avg loss: 1853835.046875 \n",
      "\n",
      "loss: 218728.218750  [    0/  790]\n",
      "Test Error:\n",
      "Avg loss: 1853861.744792 \n",
      "\n",
      "loss: 218982.218750  [    0/  790]\n",
      "Test Error:\n",
      "Avg loss: 1847094.119792 \n",
      "\n",
      "loss: 219004.687500  [    0/  790]\n",
      "Test Error:\n",
      "Avg loss: 1848280.390625 \n",
      "\n",
      "loss: 219320.718750  [    0/  790]\n",
      "Test Error:\n",
      "Avg loss: 1842598.515625 \n",
      "\n",
      "loss: 219402.765625  [    0/  790]\n",
      "Test Error:\n",
      "Avg loss: 1844611.713542 \n",
      "\n",
      "loss: 219760.453125  [    0/  790]\n",
      "Test Error:\n",
      "Avg loss: 1839754.546875 \n",
      "\n",
      "loss: 219885.312500  [    0/  790]\n",
      "Test Error:\n",
      "Avg loss: 1842331.869792 \n",
      "\n",
      "loss: 220268.593750  [    0/  790]\n",
      "Test Error:\n",
      "Avg loss: 1838110.796875 \n",
      "\n",
      "loss: 220424.015625  [    0/  790]\n",
      "Test Error:\n",
      "Avg loss: 1837750.562500 \n",
      "\n",
      "loss: 220737.093750  [    0/  790]\n",
      "Test Error:\n",
      "Avg loss: 1841147.015625 \n",
      "\n",
      "loss: 221152.515625  [    0/  790]\n",
      "Test Error:\n",
      "Avg loss: 1837572.552083 \n",
      "\n",
      "loss: 221335.265625  [    0/  790]\n",
      "Test Error:\n",
      "Avg loss: 1837672.588542 \n",
      "\n",
      "loss: 221664.062500  [    0/  790]\n",
      "Test Error:\n",
      "Avg loss: 1838131.447917 \n",
      "\n",
      "loss: 222006.828125  [    0/  790]\n",
      "Test Error:\n",
      "Avg loss: 1838691.625000 \n",
      "\n",
      "loss: 222351.343750  [    0/  790]\n",
      "Test Error:\n",
      "Avg loss: 1839333.807292 \n",
      "\n",
      "loss: 222696.328125  [    0/  790]\n",
      "Test Error:\n",
      "Avg loss: 1840041.375000 \n",
      "\n",
      "loss: 223040.687500  [    0/  790]\n",
      "Test Error:\n",
      "Avg loss: 1835578.005208 \n",
      "\n",
      "loss: 223118.187500  [    0/  790]\n",
      "Test Error:\n",
      "Avg loss: 1835007.901042 \n",
      "\n",
      "loss: 223368.109375  [    0/  790]\n",
      "Test Error:\n",
      "Avg loss: 1834357.276042 \n",
      "\n",
      "loss: 223609.578125  [    0/  790]\n",
      "Test Error:\n",
      "Avg loss: 1833758.671875 \n",
      "\n",
      "loss: 223848.578125  [    0/  790]\n",
      "Test Error:\n",
      "Avg loss: 1833196.546875 \n",
      "\n",
      "loss: 224083.984375  [    0/  790]\n",
      "Test Error:\n",
      "Avg loss: 1833367.505208 \n",
      "\n",
      "loss: 224370.890625  [    0/  790]\n",
      "Test Error:\n",
      "Avg loss: 1833169.911458 \n",
      "\n",
      "loss: 224635.765625  [    0/  790]\n",
      "Test Error:\n",
      "Avg loss: 1833006.171875 \n",
      "\n",
      "loss: 224896.859375  [    0/  790]\n",
      "Test Error:\n",
      "Avg loss: 1832852.916667 \n",
      "\n",
      "loss: 225152.906250  [    0/  790]\n",
      "Test Error:\n",
      "Avg loss: 1832700.583333 \n",
      "\n",
      "loss: 225403.406250  [    0/  790]\n",
      "Test Error:\n",
      "Avg loss: 1832539.244792 \n",
      "\n",
      "loss: 225647.890625  [    0/  790]\n",
      "Test Error:\n",
      "Avg loss: 1832364.677083 \n",
      "\n",
      "loss: 225886.062500  [    0/  790]\n",
      "Test Error:\n",
      "Avg loss: 1832169.291667 \n",
      "\n",
      "loss: 226117.593750  [    0/  790]\n",
      "Test Error:\n",
      "Avg loss: 1831949.718750 \n",
      "\n",
      "loss: 226342.312500  [    0/  790]\n",
      "Test Error:\n",
      "Avg loss: 1831778.500000 \n",
      "\n",
      "loss: 226564.968750  [    0/  790]\n",
      "Test Error:\n",
      "Avg loss: 1832415.083333 \n",
      "\n",
      "loss: 226819.375000  [    0/  790]\n",
      "Test Error:\n",
      "Avg loss: 1832248.598958 \n",
      "\n",
      "loss: 227031.312500  [    0/  790]\n",
      "Test Error:\n",
      "Avg loss: 1832061.411458 \n",
      "\n",
      "loss: 227238.093750  [    0/  790]\n",
      "Test Error:\n",
      "Avg loss: 1832803.213542 \n",
      "\n",
      "loss: 227482.046875  [    0/  790]\n",
      "Test Error:\n",
      "Avg loss: 1832635.333333 \n",
      "\n",
      "loss: 227678.625000  [    0/  790]\n",
      "Test Error:\n",
      "Avg loss: 1832413.869792 \n",
      "\n",
      "loss: 227867.140625  [    0/  790]\n",
      "Test Error:\n",
      "Avg loss: 1832284.666667 \n",
      "\n",
      "loss: 228056.140625  [    0/  790]\n",
      "Test Error:\n",
      "Avg loss: 1832827.770833 \n",
      "\n",
      "loss: 228270.593750  [    0/  790]\n",
      "Test Error:\n",
      "Avg loss: 1832549.328125 \n",
      "\n",
      "loss: 228441.968750  [    0/  790]\n",
      "Test Error:\n",
      "Avg loss: 1832335.802083 \n",
      "\n",
      "loss: 228613.125000  [    0/  790]\n",
      "Test Error:\n",
      "Avg loss: 1832888.859375 \n",
      "\n",
      "loss: 228813.828125  [    0/  790]\n",
      "Test Error:\n",
      "Avg loss: 1832547.468750 \n",
      "\n",
      "loss: 228968.593750  [    0/  790]\n",
      "Test Error:\n",
      "Avg loss: 1832243.963542 \n",
      "\n",
      "loss: 229122.171875  [    0/  790]\n",
      "Test Error:\n",
      "Avg loss: 1832772.661458 \n",
      "\n",
      "loss: 229308.515625  [    0/  790]\n",
      "Test Error:\n",
      "Avg loss: 1832355.796875 \n",
      "\n",
      "loss: 229446.812500  [    0/  790]\n",
      "Test Error:\n",
      "Avg loss: 1831962.828125 \n",
      "\n",
      "loss: 229583.750000  [    0/  790]\n",
      "Test Error:\n",
      "Avg loss: 1832449.265625 \n",
      "\n",
      "loss: 229755.812500  [    0/  790]\n",
      "Test Error:\n",
      "Avg loss: 1831952.161458 \n",
      "\n",
      "loss: 229878.406250  [    0/  790]\n",
      "Test Error:\n",
      "Avg loss: 1831477.984375 \n",
      "\n",
      "loss: 230000.000000  [    0/  790]\n",
      "Test Error:\n",
      "Avg loss: 1831909.812500 \n",
      "\n",
      "loss: 230158.015625  [    0/  790]\n",
      "Test Error:\n",
      "Avg loss: 1831332.015625 \n",
      "\n",
      "loss: 230265.703125  [    0/  790]\n",
      "Test Error:\n",
      "Avg loss: 1830797.000000 \n",
      "\n",
      "loss: 230373.906250  [    0/  790]\n",
      "Test Error:\n",
      "Avg loss: 1831158.182292 \n",
      "\n",
      "loss: 230518.046875  [    0/  790]\n",
      "Test Error:\n",
      "Avg loss: 1830507.406250 \n",
      "\n",
      "loss: 230612.125000  [    0/  790]\n",
      "Test Error:\n",
      "Avg loss: 1829941.713542 \n",
      "\n",
      "loss: 230709.687500  [    0/  790]\n",
      "Test Error:\n",
      "Avg loss: 1830240.067708 \n",
      "\n",
      "loss: 230841.828125  [    0/  790]\n",
      "Test Error:\n",
      "Avg loss: 1829526.265625 \n",
      "\n",
      "loss: 230924.187500  [    0/  790]\n",
      "Test Error:\n",
      "Avg loss: 1828932.817708 \n",
      "\n",
      "loss: 231011.640625  [    0/  790]\n",
      "Test Error:\n",
      "Avg loss: 1829147.223958 \n",
      "\n",
      "loss: 231131.093750  [    0/  790]\n",
      "Test Error:\n",
      "Avg loss: 1828367.875000 \n",
      "\n",
      "loss: 231201.937500  [    0/  790]\n",
      "Test Error:\n",
      "Avg loss: 1827769.067708 \n",
      "\n",
      "loss: 231281.062500  [    0/  790]\n",
      "Test Error:\n",
      "Avg loss: 1827894.583333 \n",
      "\n",
      "loss: 231388.390625  [    0/  790]\n",
      "Test Error:\n",
      "Avg loss: 1827093.807292 \n",
      "\n",
      "loss: 231454.281250  [    0/  790]\n",
      "Test Error:\n",
      "Avg loss: 1827473.916667 \n",
      "\n",
      "loss: 231569.093750  [    0/  790]\n",
      "Test Error:\n",
      "Avg loss: 1826667.770833 \n",
      "\n",
      "loss: 231626.953125  [    0/  790]\n",
      "Test Error:\n",
      "Avg loss: 1825901.041667 \n",
      "\n",
      "loss: 231687.312500  [    0/  790]\n",
      "Test Error:\n",
      "Avg loss: 1827623.671875 \n",
      "\n",
      "loss: 231877.343750  [    0/  790]\n",
      "Test Error:\n",
      "Avg loss: 1826988.036458 \n",
      "\n",
      "loss: 231957.281250  [    0/  790]\n",
      "Test Error:\n",
      "Avg loss: 1826548.151042 \n",
      "\n",
      "loss: 232046.796875  [    0/  790]\n",
      "Test Error:\n",
      "Avg loss: 1826944.994792 \n",
      "\n",
      "loss: 232169.046875  [    0/  790]\n",
      "Test Error:\n",
      "Avg loss: 1826328.979167 \n",
      "\n",
      "loss: 232246.484375  [    0/  790]\n",
      "Test Error:\n",
      "Avg loss: 1826972.401042 \n",
      "\n",
      "loss: 232375.687500  [    0/  790]\n",
      "Test Error:\n",
      "Avg loss: 1826361.651042 \n",
      "\n",
      "loss: 232444.921875  [    0/  790]\n",
      "Test Error:\n",
      "Avg loss: 1825829.557292 \n",
      "\n",
      "loss: 232519.093750  [    0/  790]\n",
      "Test Error:\n",
      "Avg loss: 1826294.437500 \n",
      "\n",
      "loss: 232633.187500  [    0/  790]\n",
      "Test Error:\n",
      "Avg loss: 1825606.505208 \n",
      "\n",
      "loss: 232692.062500  [    0/  790]\n",
      "Test Error:\n",
      "Avg loss: 1825157.968750 \n",
      "\n",
      "loss: 232763.687500  [    0/  790]\n",
      "Test Error:\n",
      "Avg loss: 1825445.250000 \n",
      "\n",
      "loss: 232863.468750  [    0/  790]\n",
      "Test Error:\n",
      "Avg loss: 1824832.947917 \n",
      "\n",
      "loss: 232924.781250  [    0/  790]\n",
      "Test Error:\n",
      "Avg loss: 1825355.286458 \n",
      "\n",
      "loss: 233032.250000  [    0/  790]\n",
      "Test Error:\n",
      "Avg loss: 1824647.942708 \n",
      "\n",
      "loss: 233081.250000  [    0/  790]\n",
      "Test Error:\n",
      "Avg loss: 1824157.000000 \n",
      "\n",
      "loss: 233142.640625  [    0/  790]\n",
      "Test Error:\n",
      "Avg loss: 1824462.302083 \n",
      "\n",
      "loss: 233234.828125  [    0/  790]\n",
      "Test Error:\n",
      "Avg loss: 1823813.421875 \n",
      "\n",
      "loss: 233286.734375  [    0/  790]\n",
      "Test Error:\n",
      "Avg loss: 1824344.947917 \n",
      "\n",
      "loss: 233386.671875  [    0/  790]\n",
      "Test Error:\n",
      "Avg loss: 1823603.848958 \n",
      "\n",
      "loss: 233426.531250  [    0/  790]\n",
      "Test Error:\n",
      "Avg loss: 1823097.822917 \n",
      "\n",
      "loss: 233480.265625  [    0/  790]\n",
      "Test Error:\n",
      "Avg loss: 1823384.411458 \n",
      "\n",
      "loss: 233564.500000  [    0/  790]\n",
      "Test Error:\n",
      "Avg loss: 1822734.781250 \n",
      "\n",
      "loss: 233609.968750  [    0/  790]\n",
      "Test Error:\n",
      "Avg loss: 1823240.411458 \n",
      "\n",
      "loss: 233702.093750  [    0/  790]\n",
      "Test Error:\n",
      "Avg loss: 1822466.130208 \n",
      "\n",
      "loss: 233734.125000  [    0/  790]\n",
      "Test Error:\n",
      "Avg loss: 1821985.187500 \n",
      "\n",
      "loss: 233783.390625  [    0/  790]\n",
      "Test Error:\n",
      "Avg loss: 1822221.432292 \n",
      "\n",
      "loss: 233859.187500  [    0/  790]\n",
      "Test Error:\n",
      "Avg loss: 1821610.651042 \n",
      "\n",
      "loss: 233901.250000  [    0/  790]\n",
      "Test Error:\n",
      "Avg loss: 1822057.817708 \n",
      "\n",
      "loss: 233985.187500  [    0/  790]\n",
      "Test Error:\n",
      "Avg loss: 1821332.807292 \n",
      "\n",
      "loss: 234020.640625  [    0/  790]\n",
      "Test Error:\n",
      "Avg loss: 1821972.489583 \n",
      "\n",
      "loss: 234111.859375  [    0/  790]\n",
      "Test Error:\n",
      "Avg loss: 1821202.645833 \n",
      "\n",
      "loss: 234137.375000  [    0/  790]\n",
      "Test Error:\n",
      "Avg loss: 1820674.932292 \n",
      "\n",
      "loss: 234178.468750  [    0/  790]\n",
      "Test Error:\n",
      "Avg loss: 1820991.515625 \n",
      "\n",
      "loss: 234251.703125  [    0/  790]\n",
      "Test Error:\n",
      "Avg loss: 1820345.151042 \n",
      "\n",
      "loss: 234286.562500  [    0/  790]\n",
      "Test Error:\n",
      "Avg loss: 1820857.390625 \n",
      "\n",
      "loss: 234367.484375  [    0/  790]\n",
      "Test Error:\n",
      "Avg loss: 1820112.114583 \n",
      "\n",
      "loss: 234396.968750  [    0/  790]\n",
      "Test Error:\n",
      "Avg loss: 1820800.369792 \n",
      "\n",
      "loss: 234484.890625  [    0/  790]\n",
      "Test Error:\n",
      "Avg loss: 1820020.130208 \n",
      "\n",
      "loss: 234504.593750  [    0/  790]\n",
      "Test Error:\n",
      "Avg loss: 1819506.880208 \n",
      "\n",
      "loss: 234541.656250  [    0/  790]\n",
      "Test Error:\n",
      "Avg loss: 1819828.078125 \n",
      "\n",
      "loss: 234610.062500  [    0/  790]\n",
      "Test Error:\n",
      "Avg loss: 1819217.151042 \n",
      "\n",
      "loss: 234642.281250  [    0/  790]\n",
      "Test Error:\n",
      "Avg loss: 1819704.151042 \n",
      "\n",
      "loss: 234716.625000  [    0/  790]\n",
      "Test Error:\n",
      "Avg loss: 1819000.484375 \n",
      "\n",
      "loss: 234743.343750  [    0/  790]\n",
      "Test Error:\n",
      "Avg loss: 1819657.786458 \n",
      "\n",
      "loss: 234824.656250  [    0/  790]\n",
      "Test Error:\n",
      "Avg loss: 1818867.536458 \n",
      "\n",
      "loss: 234846.984375  [    0/  790]\n",
      "Test Error:\n",
      "Avg loss: 1819671.947917 \n",
      "\n",
      "loss: 234934.234375  [    0/  790]\n",
      "Test Error:\n",
      "Avg loss: 1818892.494792 \n",
      "\n",
      "loss: 234948.312500  [    0/  790]\n",
      "Test Error:\n",
      "Avg loss: 1818373.932292 \n",
      "\n",
      "loss: 234980.500000  [    0/  790]\n",
      "Test Error:\n",
      "Avg loss: 1818743.598958 \n",
      "\n",
      "loss: 235045.656250  [    0/  790]\n",
      "Test Error:\n",
      "Avg loss: 1818144.921875 \n",
      "\n",
      "loss: 235073.984375  [    0/  790]\n",
      "Test Error:\n",
      "Avg loss: 1818674.812500 \n",
      "\n",
      "loss: 235146.031250  [    0/  790]\n",
      "Test Error:\n",
      "Avg loss: 1818015.630208 \n",
      "\n",
      "loss: 235171.375000  [    0/  790]\n",
      "Test Error:\n",
      "Avg loss: 1818686.322917 \n",
      "\n",
      "loss: 235249.328125  [    0/  790]\n",
      "Test Error:\n",
      "Avg loss: 1817962.369792 \n",
      "\n",
      "loss: 235271.515625  [    0/  790]\n",
      "Test Error:\n",
      "Avg loss: 1818760.838542 \n",
      "\n",
      "loss: 235354.718750  [    0/  790]\n",
      "Test Error:\n",
      "Avg loss: 1817977.130208 \n",
      "\n",
      "loss: 235364.875000  [    0/  790]\n",
      "Test Error:\n",
      "Avg loss: 1817566.973958 \n",
      "\n",
      "loss: 235399.062500  [    0/  790]\n",
      "Test Error:\n",
      "Avg loss: 1817879.869792 \n",
      "\n",
      "loss: 235458.125000  [    0/  790]\n",
      "Test Error:\n",
      "Avg loss: 1817413.145833 \n",
      "\n",
      "loss: 235489.796875  [    0/  790]\n",
      "Test Error:\n",
      "Avg loss: 1817869.244792 \n",
      "\n",
      "loss: 235555.140625  [    0/  790]\n",
      "Test Error:\n",
      "Avg loss: 1817361.171875 \n",
      "\n",
      "loss: 235584.859375  [    0/  790]\n",
      "Test Error:\n",
      "Avg loss: 1817939.604167 \n",
      "\n",
      "loss: 235655.468750  [    0/  790]\n",
      "Test Error:\n",
      "Avg loss: 1817388.869792 \n",
      "\n",
      "loss: 235683.140625  [    0/  790]\n",
      "Test Error:\n",
      "Avg loss: 1818077.421875 \n",
      "\n",
      "loss: 235758.406250  [    0/  790]\n",
      "Test Error:\n",
      "Avg loss: 1817482.161458 \n",
      "\n",
      "loss: 235784.000000  [    0/  790]\n",
      "Test Error:\n",
      "Avg loss: 1818264.213542 \n",
      "\n",
      "loss: 235863.093750  [    0/  790]\n",
      "Test Error:\n",
      "Avg loss: 1817628.041667 \n",
      "\n",
      "loss: 235886.734375  [    0/  790]\n",
      "Test Error:\n",
      "Avg loss: 1818491.140625 \n",
      "\n",
      "loss: 235969.171875  [    0/  790]\n",
      "Test Error:\n",
      "Avg loss: 1817814.359375 \n",
      "\n",
      "loss: 235990.750000  [    0/  790]\n",
      "Test Error:\n",
      "Avg loss: 1818748.067708 \n",
      "\n",
      "loss: 236076.000000  [    0/  790]\n",
      "Test Error:\n",
      "Avg loss: 1818036.630208 \n",
      "\n",
      "loss: 236095.859375  [    0/  790]\n",
      "Test Error:\n",
      "Avg loss: 1819034.401042 \n",
      "\n",
      "loss: 236183.578125  [    0/  790]\n",
      "Test Error:\n",
      "Avg loss: 1818289.138021 \n",
      "\n",
      "loss: 236191.140625  [    0/  790]\n",
      "Test Error:\n",
      "Avg loss: 1817960.364583 \n",
      "\n",
      "loss: 236226.078125  [    0/  790]\n",
      "Test Error:\n",
      "Avg loss: 1818340.643229 \n",
      "\n",
      "loss: 236283.968750  [    0/  790]\n",
      "Test Error:\n",
      "Avg loss: 1817995.216146 \n",
      "\n",
      "loss: 236318.375000  [    0/  790]\n",
      "Test Error:\n",
      "Avg loss: 1818471.325521 \n",
      "\n",
      "loss: 236380.328125  [    0/  790]\n",
      "Test Error:\n",
      "Avg loss: 1818121.466146 \n",
      "\n",
      "loss: 236414.640625  [    0/  790]\n",
      "Test Error:\n",
      "Avg loss: 1818673.841146 \n",
      "\n",
      "loss: 236479.937500  [    0/  790]\n",
      "Test Error:\n",
      "Avg loss: 1818321.575521 \n",
      "\n",
      "loss: 236514.203125  [    0/  790]\n",
      "Test Error:\n",
      "Avg loss: 1818932.804688 \n",
      "\n",
      "loss: 236582.015625  [    0/  790]\n",
      "Test Error:\n",
      "Avg loss: 1818575.190104 \n",
      "\n",
      "loss: 236615.937500  [    0/  790]\n",
      "Test Error:\n",
      "Avg loss: 1819236.690104 \n",
      "\n",
      "loss: 236685.687500  [    0/  790]\n",
      "Test Error:\n",
      "Avg loss: 1818875.263021 \n",
      "\n",
      "loss: 236719.421875  [    0/  790]\n",
      "Test Error:\n",
      "Avg loss: 1819572.906250 \n",
      "\n",
      "loss: 236790.609375  [    0/  790]\n",
      "Test Error:\n",
      "Avg loss: 1819209.278646 \n",
      "\n",
      "loss: 236824.203125  [    0/  790]\n",
      "Test Error:\n",
      "Avg loss: 1819932.906250 \n",
      "\n",
      "loss: 236896.312500  [    0/  790]\n",
      "Test Error:\n",
      "Avg loss: 1819571.447917 \n",
      "\n",
      "loss: 236929.750000  [    0/  790]\n",
      "Test Error:\n",
      "Avg loss: 1820314.856771 \n",
      "\n",
      "loss: 237002.421875  [    0/  790]\n",
      "Test Error:\n",
      "Avg loss: 1819956.549479 \n",
      "\n",
      "loss: 237036.046875  [    0/  790]\n",
      "Test Error:\n",
      "Avg loss: 1820711.419271 \n",
      "\n",
      "loss: 237108.890625  [    0/  790]\n",
      "Test Error:\n",
      "Avg loss: 1820362.312500 \n",
      "\n",
      "loss: 237142.812500  [    0/  790]\n",
      "Test Error:\n",
      "Avg loss: 1821121.361979 \n",
      "\n",
      "loss: 237215.593750  [    0/  790]\n",
      "Test Error:\n",
      "Avg loss: 1820786.971354 \n",
      "\n",
      "loss: 237249.906250  [    0/  790]\n",
      "Test Error:\n",
      "Avg loss: 1821568.041667 \n",
      "\n",
      "loss: 237324.843750  [    0/  790]\n",
      "Test Error:\n",
      "Avg loss: 1821268.635417 \n",
      "\n",
      "loss: 237362.078125  [    0/  790]\n",
      "Test Error:\n",
      "Avg loss: 1822031.481771 \n",
      "\n",
      "loss: 237435.625000  [    0/  790]\n",
      "Test Error:\n",
      "Avg loss: 1821755.565104 \n",
      "\n",
      "loss: 237473.750000  [    0/  790]\n",
      "Test Error:\n",
      "Avg loss: 1822504.054688 \n",
      "\n",
      "loss: 237546.328125  [    0/  790]\n",
      "Test Error:\n",
      "Avg loss: 1822256.958333 \n",
      "\n",
      "loss: 237585.468750  [    0/  790]\n",
      "Test Error:\n",
      "Avg loss: 1822982.635417 \n",
      "\n",
      "loss: 237656.578125  [    0/  790]\n",
      "Test Error:\n",
      "Avg loss: 1822769.273438 \n",
      "\n",
      "loss: 237697.031250  [    0/  790]\n",
      "Test Error:\n",
      "Avg loss: 1823467.947917 \n",
      "\n",
      "loss: 237766.562500  [    0/  790]\n",
      "Test Error:\n",
      "Avg loss: 1823292.742188 \n",
      "\n",
      "loss: 237808.562500  [    0/  790]\n",
      "Test Error:\n",
      "Avg loss: 1823957.815104 \n",
      "\n",
      "loss: 237876.171875  [    0/  790]\n",
      "Test Error:\n",
      "Avg loss: 1823824.239583 \n",
      "\n",
      "loss: 237919.750000  [    0/  790]\n",
      "Test Error:\n",
      "Avg loss: 1824452.585938 \n",
      "\n",
      "loss: 237999.500000  [    0/  790]\n",
      "Test Error:\n",
      "Avg loss: 1825853.148438 \n",
      "\n",
      "loss: 238101.531250  [    0/  790]\n",
      "Test Error:\n",
      "Avg loss: 1825319.802083 \n",
      "\n",
      "loss: 238128.765625  [    0/  790]\n",
      "Test Error:\n",
      "Avg loss: 1826627.117188 \n",
      "\n",
      "loss: 238225.843750  [    0/  790]\n",
      "Test Error:\n",
      "Avg loss: 1826119.408854 \n",
      "\n",
      "loss: 238253.609375  [    0/  790]\n",
      "Test Error:\n",
      "Avg loss: 1827318.497396 \n",
      "\n",
      "loss: 238345.125000  [    0/  790]\n",
      "Test Error:\n",
      "Avg loss: 1826849.335938 \n",
      "\n",
      "loss: 238374.296875  [    0/  790]\n",
      "Test Error:\n",
      "Avg loss: 1827947.552083 \n",
      "\n",
      "loss: 238460.609375  [    0/  790]\n",
      "Test Error:\n",
      "Avg loss: 1827531.820312 \n",
      "\n",
      "loss: 238491.687500  [    0/  790]\n",
      "Test Error:\n",
      "Avg loss: 1828530.841146 \n",
      "\n",
      "loss: 238572.953125  [    0/  790]\n",
      "Test Error:\n",
      "Avg loss: 1828182.174479 \n",
      "\n",
      "loss: 238606.718750  [    0/  790]\n",
      "Test Error:\n",
      "Avg loss: 1829082.145833 \n",
      "\n",
      "loss: 238683.078125  [    0/  790]\n",
      "Test Error:\n",
      "Avg loss: 1828807.356771 \n",
      "\n",
      "loss: 238719.796875  [    0/  790]\n",
      "Test Error:\n",
      "Avg loss: 1829609.895833 \n",
      "\n",
      "loss: 238791.312500  [    0/  790]\n",
      "Test Error:\n",
      "Avg loss: 1829415.018229 \n",
      "\n",
      "loss: 238831.328125  [    0/  790]\n",
      "Test Error:\n",
      "Avg loss: 1830115.684896 \n",
      "\n",
      "loss: 238897.734375  [    0/  790]\n",
      "Test Error:\n",
      "Avg loss: 1830014.367188 \n",
      "\n",
      "loss: 238941.781250  [    0/  790]\n",
      "Test Error:\n",
      "Avg loss: 1830696.872396 \n",
      "\n",
      "loss: 239022.984375  [    0/  790]\n",
      "Test Error:\n",
      "Avg loss: 1832087.929688 \n",
      "\n",
      "loss: 239122.000000  [    0/  790]\n",
      "Test Error:\n",
      "Avg loss: 1831634.205729 \n",
      "\n",
      "loss: 239152.000000  [    0/  790]\n",
      "Test Error:\n",
      "Avg loss: 1832864.854167 \n",
      "\n",
      "loss: 239243.031250  [    0/  790]\n",
      "Test Error:\n",
      "Avg loss: 1832486.721354 \n",
      "\n",
      "loss: 239275.937500  [    0/  790]\n",
      "Test Error:\n",
      "Avg loss: 1833552.388021 \n",
      "\n",
      "loss: 239358.984375  [    0/  790]\n",
      "Test Error:\n",
      "Avg loss: 1833266.257812 \n",
      "\n",
      "loss: 239395.578125  [    0/  790]\n",
      "Test Error:\n",
      "Avg loss: 1834178.705729 \n",
      "\n",
      "loss: 239471.484375  [    0/  790]\n",
      "Test Error:\n",
      "Avg loss: 1834034.705729 \n",
      "\n",
      "loss: 239514.500000  [    0/  790]\n",
      "Test Error:\n",
      "Avg loss: 1834821.190104 \n",
      "\n",
      "loss: 239584.531250  [    0/  790]\n",
      "Test Error:\n",
      "Avg loss: 1834809.997396 \n",
      "\n",
      "loss: 239633.546875  [    0/  790]\n",
      "Test Error:\n",
      "Avg loss: 1835600.854167 \n",
      "\n",
      "loss: 239720.734375  [    0/  790]\n",
      "Test Error:\n",
      "Avg loss: 1837054.070312 \n",
      "\n",
      "loss: 239822.500000  [    0/  790]\n",
      "Test Error:\n",
      "Avg loss: 1836748.013021 \n",
      "\n",
      "loss: 239860.359375  [    0/  790]\n",
      "Test Error:\n",
      "Avg loss: 1838003.815104 \n",
      "\n",
      "loss: 239952.718750  [    0/  790]\n",
      "Test Error:\n",
      "Avg loss: 1837820.070312 \n",
      "\n",
      "loss: 239995.812500  [    0/  790]\n",
      "Test Error:\n",
      "Avg loss: 1838866.070312 \n",
      "\n",
      "loss: 240070.281250  [    0/  790]\n",
      "Test Error:\n",
      "Avg loss: 1838822.429688 \n",
      "\n",
      "loss: 240095.640625  [    0/  790]\n",
      "Test Error:\n",
      "Avg loss: 1839807.200521 \n",
      "\n",
      "loss: 240193.718750  [    0/  790]\n",
      "Test Error:\n",
      "Avg loss: 1841396.960938 \n",
      "\n",
      "loss: 240302.640625  [    0/  790]\n",
      "Test Error:\n",
      "Avg loss: 1840983.117188 \n",
      "\n",
      "loss: 240336.671875  [    0/  790]\n",
      "Test Error:\n",
      "Avg loss: 1842259.239583 \n",
      "\n",
      "loss: 240429.953125  [    0/  790]\n",
      "Test Error:\n",
      "Avg loss: 1841923.054688 \n",
      "\n",
      "loss: 240466.203125  [    0/  790]\n",
      "Test Error:\n",
      "Avg loss: 1842888.914062 \n",
      "\n",
      "loss: 240545.531250  [    0/  790]\n",
      "Test Error:\n",
      "Avg loss: 1842652.520833 \n",
      "\n",
      "loss: 240586.140625  [    0/  790]\n",
      "Test Error:\n",
      "Avg loss: 1843366.294271 \n",
      "\n",
      "loss: 240638.468750  [    0/  790]\n",
      "Test Error:\n",
      "Avg loss: 1845057.687500 \n",
      "\n",
      "loss: 240754.953125  [    0/  790]\n",
      "Test Error:\n",
      "Avg loss: 1844533.750000 \n",
      "\n",
      "loss: 240782.875000  [    0/  790]\n",
      "Test Error:\n",
      "Avg loss: 1845747.263021 \n",
      "\n",
      "loss: 240871.468750  [    0/  790]\n",
      "Test Error:\n",
      "Avg loss: 1845354.528646 \n",
      "\n",
      "loss: 240904.281250  [    0/  790]\n",
      "Test Error:\n",
      "Avg loss: 1846247.645833 \n",
      "\n",
      "loss: 240977.453125  [    0/  790]\n",
      "Test Error:\n",
      "Avg loss: 1845994.919271 \n",
      "\n",
      "loss: 241015.484375  [    0/  790]\n",
      "Test Error:\n",
      "Avg loss: 1846680.059896 \n",
      "\n",
      "loss: 241097.781250  [    0/  790]\n",
      "Test Error:\n",
      "Avg loss: 1848070.919271 \n",
      "\n",
      "loss: 241157.984375  [    0/  790]\n",
      "Test Error:\n",
      "Avg loss: 1847764.072917 \n",
      "\n",
      "loss: 241200.171875  [    0/  790]\n",
      "Test Error:\n",
      "Avg loss: 1848847.822917 \n",
      "\n",
      "loss: 241287.000000  [    0/  790]\n",
      "Test Error:\n",
      "Avg loss: 1848466.169271 \n",
      "\n",
      "loss: 241319.375000  [    0/  790]\n",
      "Test Error:\n",
      "Avg loss: 1849226.377604 \n",
      "\n",
      "loss: 241385.031250  [    0/  790]\n",
      "Test Error:\n",
      "Avg loss: 1849025.950521 \n",
      "\n",
      "loss: 241424.625000  [    0/  790]\n",
      "Test Error:\n",
      "Avg loss: 1849698.450521 \n",
      "\n",
      "loss: 241465.546875  [    0/  790]\n",
      "Test Error:\n",
      "Avg loss: 1851213.533854 \n",
      "\n",
      "loss: 241571.593750  [    0/  790]\n",
      "Test Error:\n",
      "Avg loss: 1850777.427083 \n",
      "\n",
      "loss: 241607.765625  [    0/  790]\n",
      "Test Error:\n",
      "Avg loss: 1851675.072917 \n",
      "\n",
      "loss: 241679.406250  [    0/  790]\n",
      "Test Error:\n",
      "Avg loss: 1851410.195312 \n",
      "\n",
      "loss: 241716.421875  [    0/  790]\n",
      "Test Error:\n",
      "Avg loss: 1852105.562500 \n",
      "\n",
      "loss: 241798.734375  [    0/  790]\n",
      "Test Error:\n",
      "Avg loss: 1853464.992188 \n",
      "\n",
      "loss: 241892.015625  [    0/  790]\n",
      "Test Error:\n",
      "Avg loss: 1851709.210938 \n",
      "\n",
      "loss: 241798.718750  [    0/  790]\n",
      "Test Error:\n",
      "Avg loss: 1852683.468750 \n",
      "\n",
      "loss: 241858.937500  [    0/  790]\n",
      "Test Error:\n",
      "Avg loss: 1852062.020833 \n",
      "\n",
      "loss: 241866.515625  [    0/  790]\n",
      "Test Error:\n",
      "Avg loss: 1852456.000000 \n",
      "\n",
      "loss: 241923.359375  [    0/  790]\n",
      "Test Error:\n",
      "Avg loss: 1853560.570312 \n",
      "\n",
      "loss: 241941.656250  [    0/  790]\n",
      "Test Error:\n",
      "Avg loss: 1853024.997396 \n",
      "\n",
      "loss: 241956.281250  [    0/  790]\n",
      "Test Error:\n",
      "Avg loss: 1853785.971354 \n",
      "\n",
      "loss: 242011.078125  [    0/  790]\n",
      "Test Error:\n",
      "Avg loss: 1853126.471354 \n",
      "\n",
      "loss: 242021.250000  [    0/  790]\n",
      "Test Error:\n",
      "Avg loss: 1853511.916667 \n",
      "\n",
      "loss: 242081.906250  [    0/  790]\n",
      "Test Error:\n",
      "Avg loss: 1854653.523438 \n",
      "\n",
      "loss: 242102.359375  [    0/  790]\n",
      "Test Error:\n",
      "Avg loss: 1854096.674479 \n",
      "\n",
      "loss: 242119.781250  [    0/  790]\n",
      "Test Error:\n",
      "Avg loss: 1854852.882812 \n",
      "\n",
      "loss: 242177.531250  [    0/  790]\n",
      "Test Error:\n",
      "Avg loss: 1854193.997396 \n",
      "\n",
      "loss: 242191.265625  [    0/  790]\n",
      "Test Error:\n",
      "Avg loss: 1854591.750000 \n",
      "\n",
      "loss: 242199.031250  [    0/  790]\n",
      "Test Error:\n",
      "Avg loss: 1856095.726562 \n",
      "\n",
      "loss: 242292.250000  [    0/  790]\n",
      "Test Error:\n",
      "Avg loss: 1855301.018229 \n",
      "\n",
      "loss: 242302.046875  [    0/  790]\n",
      "Test Error:\n",
      "Avg loss: 1855997.486979 \n",
      "\n",
      "loss: 242359.343750  [    0/  790]\n",
      "Test Error:\n",
      "Avg loss: 1855370.304688 \n",
      "\n",
      "loss: 242377.265625  [    0/  790]\n",
      "Test Error:\n",
      "Avg loss: 1855806.408854 \n",
      "\n",
      "loss: 242386.312500  [    0/  790]\n",
      "Test Error:\n",
      "Avg loss: 1857220.239583 \n",
      "\n",
      "loss: 242477.484375  [    0/  790]\n",
      "Test Error:\n",
      "Avg loss: 1856483.718750 \n",
      "\n",
      "loss: 242492.265625  [    0/  790]\n",
      "Test Error:\n",
      "Avg loss: 1857082.408854 \n",
      "\n",
      "loss: 242546.875000  [    0/  790]\n",
      "Test Error:\n",
      "Avg loss: 1856545.052083 \n",
      "\n",
      "loss: 242516.328125  [    0/  790]\n",
      "Test Error:\n",
      "Avg loss: 1857340.864583 \n",
      "\n",
      "loss: 242602.546875  [    0/  790]\n",
      "Test Error:\n",
      "Avg loss: 1858350.791667 \n",
      "\n",
      "loss: 242676.921875  [    0/  790]\n",
      "Test Error:\n",
      "Avg loss: 1857665.898438 \n",
      "\n",
      "loss: 242696.937500  [    0/  790]\n",
      "Test Error:\n",
      "Avg loss: 1858172.632812 \n",
      "\n",
      "loss: 242715.921875  [    0/  790]\n",
      "Test Error:\n",
      "Avg loss: 1859809.539062 \n",
      "\n",
      "loss: 242820.515625  [    0/  790]\n",
      "Test Error:\n",
      "Avg loss: 1858985.072917 \n",
      "\n",
      "loss: 242835.234375  [    0/  790]\n",
      "Test Error:\n",
      "Avg loss: 1859736.101562 \n",
      "\n",
      "loss: 242899.578125  [    0/  790]\n",
      "Test Error:\n",
      "Avg loss: 1859151.958333 \n",
      "\n",
      "loss: 242866.015625  [    0/  790]\n",
      "Test Error:\n",
      "Avg loss: 1859970.062500 \n",
      "\n",
      "loss: 242956.359375  [    0/  790]\n",
      "Test Error:\n",
      "Avg loss: 1861080.375000 \n",
      "\n",
      "loss: 243037.265625  [    0/  790]\n",
      "Test Error:\n",
      "Avg loss: 1860346.125000 \n",
      "\n",
      "loss: 242997.031250  [    0/  790]\n",
      "Test Error:\n",
      "Avg loss: 1861297.585938 \n",
      "\n",
      "loss: 243069.734375  [    0/  790]\n",
      "Test Error:\n",
      "Avg loss: 1860924.843750 \n",
      "\n",
      "loss: 243104.437500  [    0/  790]\n",
      "Test Error:\n",
      "Avg loss: 1861443.356771 \n",
      "\n",
      "loss: 243181.500000  [    0/  790]\n",
      "Test Error:\n",
      "Avg loss: 1862304.950521 \n",
      "\n",
      "loss: 243188.906250  [    0/  790]\n",
      "Test Error:\n",
      "Avg loss: 1862147.333333 \n",
      "\n",
      "loss: 243234.953125  [    0/  790]\n",
      "Test Error:\n",
      "Avg loss: 1862756.179688 \n",
      "\n",
      "loss: 243317.640625  [    0/  790]\n",
      "Test Error:\n",
      "Avg loss: 1864143.250000 \n",
      "\n",
      "loss: 243414.281250  [    0/  790]\n",
      "Test Error:\n",
      "Avg loss: 1863383.450521 \n",
      "\n",
      "loss: 243370.203125  [    0/  790]\n",
      "Test Error:\n",
      "Avg loss: 1864482.101562 \n",
      "\n",
      "loss: 243450.609375  [    0/  790]\n",
      "Test Error:\n",
      "Avg loss: 1864025.179688 \n",
      "\n",
      "loss: 243483.234375  [    0/  790]\n",
      "Test Error:\n",
      "Avg loss: 1864558.354167 \n",
      "\n",
      "loss: 243495.625000  [    0/  790]\n",
      "Test Error:\n",
      "Avg loss: 1866222.221354 \n",
      "\n",
      "loss: 243604.703125  [    0/  790]\n",
      "Test Error:\n",
      "Avg loss: 1865600.721354 \n",
      "\n",
      "loss: 243631.250000  [    0/  790]\n",
      "Test Error:\n",
      "Avg loss: 1866210.268229 \n",
      "\n",
      "loss: 243715.937500  [    0/  790]\n",
      "Test Error:\n",
      "Avg loss: 1867742.507812 \n",
      "\n",
      "loss: 243750.359375  [    0/  790]\n",
      "Test Error:\n",
      "Avg loss: 1867325.825521 \n",
      "\n",
      "loss: 243787.843750  [    0/  790]\n",
      "Test Error:\n",
      "Avg loss: 1868495.591146 \n",
      "\n",
      "loss: 243874.281250  [    0/  790]\n",
      "Test Error:\n",
      "Avg loss: 1867796.010417 \n",
      "\n",
      "loss: 243897.984375  [    0/  790]\n",
      "Test Error:\n",
      "Avg loss: 1868363.552083 \n",
      "\n",
      "loss: 243909.093750  [    0/  790]\n",
      "Test Error:\n",
      "Avg loss: 1870158.763021 \n",
      "\n",
      "loss: 244025.296875  [    0/  790]\n",
      "Test Error:\n",
      "Avg loss: 1869374.799479 \n",
      "\n",
      "loss: 244047.031250  [    0/  790]\n",
      "Test Error:\n",
      "Avg loss: 1870376.239583 \n",
      "\n",
      "loss: 244052.515625  [    0/  790]\n",
      "Test Error:\n",
      "Avg loss: 1870205.924479 \n",
      "\n",
      "loss: 244096.187500  [    0/  790]\n",
      "Test Error:\n",
      "Avg loss: 1870837.604167 \n",
      "\n",
      "loss: 244178.125000  [    0/  790]\n",
      "Test Error:\n",
      "Avg loss: 1872137.174479 \n",
      "\n",
      "loss: 244271.812500  [    0/  790]\n",
      "Test Error:\n",
      "Avg loss: 1871380.677083 \n",
      "\n",
      "loss: 244219.062500  [    0/  790]\n",
      "Test Error:\n",
      "Avg loss: 1872720.424479 \n",
      "\n",
      "loss: 244309.125000  [    0/  790]\n",
      "Test Error:\n",
      "Avg loss: 1872235.966146 \n",
      "\n",
      "loss: 244338.718750  [    0/  790]\n",
      "Test Error:\n",
      "Avg loss: 1872754.992188 \n",
      "\n",
      "loss: 244420.437500  [    0/  790]\n",
      "Test Error:\n",
      "Avg loss: 1873882.486979 \n",
      "\n",
      "loss: 244428.109375  [    0/  790]\n",
      "Test Error:\n",
      "Avg loss: 1873703.221354 \n",
      "\n",
      "loss: 244473.265625  [    0/  790]\n",
      "Test Error:\n",
      "Avg loss: 1874486.697917 \n",
      "\n",
      "loss: 244537.859375  [    0/  790]\n",
      "Test Error:\n",
      "Avg loss: 1874012.125000 \n",
      "\n",
      "loss: 244572.734375  [    0/  790]\n",
      "Test Error:\n",
      "Avg loss: 1874518.937500 \n",
      "\n",
      "loss: 244574.156250  [    0/  790]\n",
      "Test Error:\n",
      "Avg loss: 1875953.721354 \n",
      "\n",
      "loss: 244669.765625  [    0/  790]\n",
      "Test Error:\n",
      "Avg loss: 1875498.742188 \n",
      "\n",
      "loss: 244702.609375  [    0/  790]\n",
      "Test Error:\n",
      "Avg loss: 1876162.447917 \n",
      "\n",
      "loss: 244790.781250  [    0/  790]\n",
      "Test Error:\n",
      "Avg loss: 1877555.604167 \n",
      "\n",
      "loss: 244806.734375  [    0/  790]\n",
      "Test Error:\n",
      "Avg loss: 1877335.992188 \n",
      "\n",
      "loss: 244851.968750  [    0/  790]\n",
      "Test Error:\n",
      "Avg loss: 1878270.531250 \n",
      "\n",
      "loss: 244924.781250  [    0/  790]\n",
      "Test Error:\n",
      "Avg loss: 1877774.291667 \n",
      "\n",
      "loss: 244955.796875  [    0/  790]\n",
      "Test Error:\n",
      "Avg loss: 1878342.164062 \n",
      "\n",
      "loss: 244959.343750  [    0/  790]\n",
      "Test Error:\n",
      "Avg loss: 1880199.971354 \n",
      "\n",
      "loss: 245076.640625  [    0/  790]\n",
      "Test Error:\n",
      "Avg loss: 1879683.075521 \n",
      "\n",
      "loss: 245112.187500  [    0/  790]\n",
      "Test Error:\n",
      "Avg loss: 1880649.601562 \n",
      "\n",
      "loss: 245186.953125  [    0/  790]\n",
      "Test Error:\n",
      "Avg loss: 1880079.333333 \n",
      "\n",
      "loss: 245132.390625  [    0/  790]\n",
      "Test Error:\n",
      "Avg loss: 1881082.979167 \n",
      "\n",
      "loss: 245238.093750  [    0/  790]\n",
      "Test Error:\n",
      "Avg loss: 1882633.354167 \n",
      "\n",
      "loss: 245341.890625  [    0/  790]\n",
      "Test Error:\n",
      "Avg loss: 1881976.343750 \n",
      "\n",
      "loss: 245371.843750  [    0/  790]\n",
      "Test Error:\n",
      "Avg loss: 1883110.169271 \n",
      "\n",
      "loss: 245364.093750  [    0/  790]\n",
      "Test Error:\n",
      "Avg loss: 1883061.395833 \n",
      "\n",
      "loss: 245421.171875  [    0/  790]\n",
      "Test Error:\n",
      "Avg loss: 1883804.190104 \n",
      "\n",
      "loss: 245485.437500  [    0/  790]\n",
      "Test Error:\n",
      "Avg loss: 1883351.549479 \n",
      "\n",
      "loss: 245520.031250  [    0/  790]\n",
      "Test Error:\n",
      "Avg loss: 1883860.156250 \n",
      "\n",
      "loss: 245509.859375  [    0/  790]\n",
      "Test Error:\n",
      "Avg loss: 1885559.312500 \n",
      "\n",
      "loss: 245620.718750  [    0/  790]\n",
      "Test Error:\n",
      "Avg loss: 1885103.197917 \n",
      "\n",
      "loss: 245660.437500  [    0/  790]\n",
      "Test Error:\n",
      "Avg loss: 1885937.833333 \n",
      "\n",
      "loss: 245730.421875  [    0/  790]\n",
      "Test Error:\n",
      "Avg loss: 1885596.500000 \n",
      "\n",
      "loss: 245677.937500  [    0/  790]\n",
      "Test Error:\n",
      "Avg loss: 1886562.966146 \n",
      "\n",
      "loss: 245784.843750  [    0/  790]\n",
      "Test Error:\n",
      "Avg loss: 1888280.908854 \n",
      "\n",
      "loss: 245899.703125  [    0/  790]\n",
      "Test Error:\n",
      "Avg loss: 1887548.562500 \n",
      "\n",
      "loss: 245928.671875  [    0/  790]\n",
      "Test Error:\n",
      "Avg loss: 1888763.966146 \n",
      "\n",
      "loss: 245918.531250  [    0/  790]\n",
      "Test Error:\n",
      "Avg loss: 1888673.635417 \n",
      "\n",
      "loss: 245976.484375  [    0/  790]\n",
      "Test Error:\n",
      "Avg loss: 1889509.239583 \n",
      "\n",
      "loss: 246047.562500  [    0/  790]\n",
      "Test Error:\n",
      "Avg loss: 1889110.513021 \n",
      "\n",
      "loss: 246089.703125  [    0/  790]\n",
      "Test Error:\n",
      "Avg loss: 1889593.898438 \n",
      "\n",
      "loss: 246074.500000  [    0/  790]\n",
      "Test Error:\n",
      "Avg loss: 1891744.721354 \n",
      "\n",
      "loss: 246210.171875  [    0/  790]\n",
      "Test Error:\n",
      "Avg loss: 1891049.726562 \n",
      "\n",
      "loss: 246244.109375  [    0/  790]\n",
      "Test Error:\n",
      "Avg loss: 1892424.210938 \n",
      "\n",
      "loss: 246239.015625  [    0/  790]\n",
      "Test Error:\n",
      "Avg loss: 1892332.708333 \n",
      "\n",
      "loss: 246299.171875  [    0/  790]\n",
      "Test Error:\n",
      "Avg loss: 1893337.747396 \n",
      "\n",
      "loss: 246379.781250  [    0/  790]\n",
      "Test Error:\n",
      "Avg loss: 1892843.731771 \n",
      "\n",
      "loss: 246419.703125  [    0/  790]\n",
      "Test Error:\n",
      "Avg loss: 1893319.611979 \n",
      "\n",
      "loss: 246368.234375  [    0/  790]\n",
      "Test Error:\n",
      "Avg loss: 1893612.596354 \n",
      "\n",
      "loss: 246443.703125  [    0/  790]\n",
      "Test Error:\n",
      "Avg loss: 1894097.226562 \n",
      "\n",
      "loss: 246532.578125  [    0/  790]\n",
      "Test Error:\n",
      "Avg loss: 1895453.742188 \n",
      "\n",
      "loss: 246632.109375  [    0/  790]\n",
      "Test Error:\n",
      "Avg loss: 1894850.544271 \n",
      "\n",
      "loss: 246561.500000  [    0/  790]\n",
      "Test Error:\n",
      "Avg loss: 1896471.041667 \n",
      "\n",
      "loss: 246671.500000  [    0/  790]\n",
      "Test Error:\n",
      "Avg loss: 1895995.122396 \n",
      "\n",
      "loss: 246714.703125  [    0/  790]\n",
      "Test Error:\n",
      "Avg loss: 1896535.270833 \n",
      "\n",
      "loss: 246773.921875  [    0/  790]\n",
      "Test Error:\n",
      "Avg loss: 1896254.742188 \n",
      "\n",
      "loss: 246712.156250  [    0/  790]\n",
      "Test Error:\n",
      "Avg loss: 1897201.080729 \n",
      "\n",
      "loss: 246822.406250  [    0/  790]\n",
      "Test Error:\n",
      "Avg loss: 1898616.947917 \n",
      "\n",
      "loss: 246926.281250  [    0/  790]\n",
      "Test Error:\n",
      "Avg loss: 1898010.395833 \n",
      "\n",
      "loss: 246852.921875  [    0/  790]\n",
      "Test Error:\n",
      "Avg loss: 1899685.638021 \n",
      "\n",
      "loss: 246966.390625  [    0/  790]\n",
      "Test Error:\n",
      "Avg loss: 1899299.445312 \n",
      "\n",
      "loss: 247015.281250  [    0/  790]\n",
      "Test Error:\n",
      "Avg loss: 1900005.992188 \n",
      "\n",
      "loss: 247085.171875  [    0/  790]\n",
      "Test Error:\n",
      "Avg loss: 1899658.236979 \n",
      "\n",
      "loss: 247019.015625  [    0/  790]\n",
      "Test Error:\n",
      "Avg loss: 1900691.153646 \n",
      "\n",
      "loss: 247135.859375  [    0/  790]\n",
      "Test Error:\n",
      "Avg loss: 1902280.242188 \n",
      "\n",
      "loss: 247247.625000  [    0/  790]\n",
      "Test Error:\n",
      "Avg loss: 1901565.497396 \n",
      "\n",
      "loss: 247285.796875  [    0/  790]\n",
      "Test Error:\n",
      "Avg loss: 1902733.809896 \n",
      "\n",
      "loss: 247258.015625  [    0/  790]\n",
      "Test Error:\n",
      "Avg loss: 1902691.075521 \n",
      "\n",
      "loss: 247324.421875  [    0/  790]\n",
      "Test Error:\n",
      "Avg loss: 1903507.367188 \n",
      "\n",
      "loss: 247399.062500  [    0/  790]\n",
      "Test Error:\n",
      "Avg loss: 1903088.971354 \n",
      "\n",
      "loss: 247327.343750  [    0/  790]\n",
      "Test Error:\n",
      "Avg loss: 1904197.940104 \n",
      "\n",
      "loss: 247451.687500  [    0/  790]\n",
      "Test Error:\n",
      "Avg loss: 1905990.736979 \n",
      "\n",
      "loss: 247580.531250  [    0/  790]\n",
      "Test Error:\n",
      "Avg loss: 1905538.455729 \n",
      "\n",
      "loss: 247598.265625  [    0/  790]\n",
      "Test Error:\n",
      "Avg loss: 1905161.833333 \n",
      "\n",
      "loss: 247525.546875  [    0/  790]\n",
      "Test Error:\n",
      "Avg loss: 1906131.976562 \n",
      "\n",
      "loss: 247643.921875  [    0/  790]\n",
      "Test Error:\n",
      "Avg loss: 1907751.747396 \n",
      "\n",
      "loss: 247765.031250  [    0/  790]\n",
      "Test Error:\n",
      "Avg loss: 1907212.658854 \n",
      "\n",
      "loss: 247652.531250  [    0/  790]\n",
      "Test Error:\n",
      "Avg loss: 1907577.752604 \n",
      "\n",
      "loss: 247733.218750  [    0/  790]\n",
      "Test Error:\n",
      "Avg loss: 1908011.492188 \n",
      "\n",
      "loss: 247825.875000  [    0/  790]\n",
      "Test Error:\n",
      "Avg loss: 1909303.111979 \n",
      "\n",
      "loss: 247931.968750  [    0/  790]\n",
      "Test Error:\n",
      "Avg loss: 1908851.888021 \n",
      "\n",
      "loss: 247857.593750  [    0/  790]\n",
      "Test Error:\n",
      "Avg loss: 1911047.169271 \n",
      "\n",
      "loss: 248001.453125  [    0/  790]\n",
      "Test Error:\n",
      "Avg loss: 1910445.664062 \n",
      "\n",
      "loss: 248051.328125  [    0/  790]\n",
      "Test Error:\n",
      "Avg loss: 1912194.914062 \n",
      "\n",
      "loss: 248048.031250  [    0/  790]\n",
      "Test Error:\n",
      "Avg loss: 1912253.611979 \n",
      "\n",
      "loss: 248093.234375  [    0/  790]\n",
      "Test Error:\n",
      "Avg loss: 1912111.002604 \n",
      "\n",
      "loss: 248154.515625  [    0/  790]\n",
      "Test Error:\n",
      "Avg loss: 1912762.835938 \n",
      "\n",
      "loss: 248230.937500  [    0/  790]\n",
      "Test Error:\n",
      "Avg loss: 1912425.208333 \n",
      "\n",
      "loss: 248154.453125  [    0/  790]\n",
      "Test Error:\n",
      "Avg loss: 1913925.882812 \n",
      "\n",
      "loss: 248265.968750  [    0/  790]\n",
      "Test Error:\n",
      "Avg loss: 1913686.427083 \n",
      "\n",
      "loss: 248326.687500  [    0/  790]\n",
      "Test Error:\n",
      "Avg loss: 1914717.674479 \n",
      "\n",
      "loss: 248422.312500  [    0/  790]\n",
      "Test Error:\n",
      "Avg loss: 1914270.739583 \n",
      "\n",
      "loss: 248342.046875  [    0/  790]\n",
      "Test Error:\n",
      "Avg loss: 1916183.250000 \n",
      "\n",
      "loss: 248474.500000  [    0/  790]\n",
      "Test Error:\n",
      "Avg loss: 1915672.229167 \n",
      "\n",
      "loss: 248531.968750  [    0/  790]\n",
      "Test Error:\n",
      "Avg loss: 1917844.919271 \n",
      "\n",
      "loss: 248542.937500  [    0/  790]\n",
      "Test Error:\n",
      "Avg loss: 1917915.997396 \n",
      "\n",
      "loss: 248591.765625  [    0/  790]\n",
      "Test Error:\n",
      "Avg loss: 1917587.270833 \n",
      "\n",
      "loss: 248649.875000  [    0/  790]\n",
      "Test Error:\n",
      "Avg loss: 1918551.645833 \n",
      "\n",
      "loss: 248738.203125  [    0/  790]\n",
      "Test Error:\n",
      "Avg loss: 1918059.476562 \n",
      "\n",
      "loss: 248659.234375  [    0/  790]\n",
      "Test Error:\n",
      "Avg loss: 1920596.455729 \n",
      "\n",
      "loss: 248823.078125  [    0/  790]\n",
      "Test Error:\n",
      "Avg loss: 1919870.460938 \n",
      "\n",
      "loss: 248874.812500  [    0/  790]\n",
      "Test Error:\n",
      "Avg loss: 1922421.406250 \n",
      "\n",
      "loss: 248889.234375  [    0/  790]\n",
      "Test Error:\n",
      "Avg loss: 1922594.841146 \n",
      "\n",
      "loss: 248949.421875  [    0/  790]\n",
      "Test Error:\n",
      "Avg loss: 1922049.895833 \n",
      "\n",
      "loss: 249008.000000  [    0/  790]\n",
      "Test Error:\n",
      "Avg loss: 1923956.018229 \n",
      "\n",
      "loss: 249143.875000  [    0/  790]\n",
      "Test Error:\n",
      "Avg loss: 1923038.520833 \n",
      "\n",
      "loss: 248994.296875  [    0/  790]\n",
      "Test Error:\n",
      "Avg loss: 1923342.414062 \n",
      "\n",
      "loss: 249093.187500  [    0/  790]\n",
      "Test Error:\n",
      "Avg loss: 1924803.947917 \n",
      "\n",
      "loss: 249208.109375  [    0/  790]\n",
      "Test Error:\n",
      "Avg loss: 1924082.486979 \n",
      "\n",
      "loss: 249260.390625  [    0/  790]\n",
      "Test Error:\n",
      "Avg loss: 1926214.835938 \n",
      "\n",
      "loss: 249252.984375  [    0/  790]\n",
      "Test Error:\n",
      "Avg loss: 1926026.533854 \n",
      "\n",
      "loss: 249293.343750  [    0/  790]\n",
      "Test Error:\n",
      "Avg loss: 1925596.960938 \n",
      "\n",
      "loss: 249357.703125  [    0/  790]\n",
      "Test Error:\n",
      "Avg loss: 1927050.611979 \n",
      "\n",
      "loss: 249277.031250  [    0/  790]\n",
      "Test Error:\n",
      "Avg loss: 1927166.783854 \n",
      "\n",
      "loss: 249405.937500  [    0/  790]\n",
      "Test Error:\n",
      "Avg loss: 1929428.351562 \n",
      "\n",
      "loss: 249563.265625  [    0/  790]\n",
      "Test Error:\n",
      "Avg loss: 1928512.695312 \n",
      "\n",
      "loss: 249570.468750  [    0/  790]\n",
      "Test Error:\n",
      "Avg loss: 1928034.312500 \n",
      "\n",
      "loss: 249438.156250  [    0/  790]\n",
      "Test Error:\n",
      "Avg loss: 1930265.377604 \n",
      "\n",
      "loss: 249627.265625  [    0/  790]\n",
      "Test Error:\n",
      "Avg loss: 1929558.187500 \n",
      "\n",
      "loss: 249684.765625  [    0/  790]\n",
      "Test Error:\n",
      "Avg loss: 1931641.010417 \n",
      "\n",
      "loss: 249599.968750  [    0/  790]\n",
      "Test Error:\n",
      "Avg loss: 1931585.635417 \n",
      "\n",
      "loss: 249684.421875  [    0/  790]\n",
      "Test Error:\n",
      "Avg loss: 1931225.697917 \n",
      "\n",
      "loss: 249784.265625  [    0/  790]\n",
      "Test Error:\n",
      "Avg loss: 1932604.747396 \n",
      "\n",
      "loss: 249899.750000  [    0/  790]\n",
      "Test Error:\n",
      "Avg loss: 1931878.528646 \n",
      "\n",
      "loss: 249725.187500  [    0/  790]\n",
      "Test Error:\n",
      "Avg loss: 1934621.166667 \n",
      "\n",
      "loss: 249948.656250  [    0/  790]\n",
      "Test Error:\n",
      "Avg loss: 1933719.096354 \n",
      "\n",
      "loss: 250020.140625  [    0/  790]\n",
      "Test Error:\n",
      "Avg loss: 1936173.747396 \n",
      "\n",
      "loss: 249919.218750  [    0/  790]\n",
      "Test Error:\n",
      "Avg loss: 1936271.575521 \n",
      "\n",
      "loss: 250015.859375  [    0/  790]\n",
      "Test Error:\n",
      "Avg loss: 1935640.304688 \n",
      "\n",
      "loss: 250118.796875  [    0/  790]\n",
      "Test Error:\n",
      "Avg loss: 1937294.583333 \n",
      "\n",
      "loss: 250261.406250  [    0/  790]\n",
      "Test Error:\n",
      "Avg loss: 1936363.325521 \n",
      "\n",
      "loss: 250034.156250  [    0/  790]\n",
      "Test Error:\n",
      "Avg loss: 1939397.687500 \n",
      "\n",
      "loss: 250286.671875  [    0/  790]\n",
      "Test Error:\n",
      "Avg loss: 1938522.138021 \n",
      "\n",
      "loss: 250345.046875  [    0/  790]\n",
      "Test Error:\n",
      "Avg loss: 1937807.377604 \n",
      "\n",
      "loss: 250089.203125  [    0/  790]\n",
      "Test Error:\n",
      "Avg loss: 1940175.096354 \n",
      "\n",
      "loss: 250293.531250  [    0/  790]\n",
      "Test Error:\n",
      "Avg loss: 1939444.940104 \n",
      "\n",
      "loss: 250374.843750  [    0/  790]\n",
      "Test Error:\n",
      "Avg loss: 1941475.302083 \n",
      "\n",
      "loss: 250592.437500  [    0/  790]\n",
      "Test Error:\n",
      "Avg loss: 1940432.468750 \n",
      "\n",
      "loss: 250290.593750  [    0/  790]\n",
      "Test Error:\n",
      "Avg loss: 1940636.718750 \n",
      "\n",
      "loss: 250431.859375  [    0/  790]\n",
      "Test Error:\n",
      "Avg loss: 1941900.440104 \n",
      "\n",
      "loss: 250590.468750  [    0/  790]\n",
      "Test Error:\n",
      "Avg loss: 1941168.263021 \n",
      "\n",
      "loss: 250308.406250  [    0/  790]\n",
      "Test Error:\n",
      "Avg loss: 1943918.476562 \n",
      "\n",
      "loss: 250559.046875  [    0/  790]\n",
      "Test Error:\n",
      "Avg loss: 1943091.406250 \n",
      "\n",
      "loss: 250552.437500  [    0/  790]\n",
      "Test Error:\n",
      "Avg loss: 1945668.247396 \n",
      "\n",
      "loss: 250888.921875  [    0/  790]\n",
      "Test Error:\n",
      "Avg loss: 1944092.575521 \n",
      "\n",
      "loss: 250415.484375  [    0/  790]\n",
      "Test Error:\n",
      "Avg loss: 1947684.177083 \n",
      "\n",
      "loss: 250863.484375  [    0/  790]\n",
      "Test Error:\n",
      "Avg loss: 1946025.312500 \n",
      "\n",
      "loss: 250888.953125  [    0/  790]\n",
      "Test Error:\n",
      "Avg loss: 1945369.804688 \n",
      "\n",
      "loss: 250611.328125  [    0/  790]\n",
      "Test Error:\n",
      "Avg loss: 1947512.895833 \n",
      "\n",
      "loss: 250814.375000  [    0/  790]\n",
      "Test Error:\n",
      "Avg loss: 1946806.002604 \n",
      "\n",
      "loss: 250833.015625  [    0/  790]\n",
      "Test Error:\n",
      "Avg loss: 1948745.138021 \n",
      "\n",
      "loss: 251119.828125  [    0/  790]\n",
      "Test Error:\n",
      "Avg loss: 1947408.794271 \n",
      "\n",
      "loss: 250670.984375  [    0/  790]\n",
      "Test Error:\n",
      "Avg loss: 1950452.966146 \n",
      "\n",
      "loss: 251060.062500  [    0/  790]\n",
      "Test Error:\n",
      "Avg loss: 1949012.606771 \n",
      "\n",
      "loss: 250935.609375  [    0/  790]\n",
      "Test Error:\n",
      "Avg loss: 1951364.205729 \n",
      "\n",
      "loss: 250968.843750  [    0/  790]\n",
      "Test Error:\n",
      "Avg loss: 1950777.382812 \n",
      "\n",
      "loss: 250815.921875  [    0/  790]\n",
      "Test Error:\n",
      "Avg loss: 1953583.312500 \n",
      "\n",
      "loss: 251312.453125  [    0/  790]\n",
      "Test Error:\n",
      "Avg loss: 1951725.835938 \n",
      "\n",
      "loss: 251048.515625  [    0/  790]\n",
      "Test Error:\n",
      "Avg loss: 1954590.645833 \n",
      "\n",
      "loss: 251206.937500  [    0/  790]\n",
      "Test Error:\n",
      "Avg loss: 1953597.229167 \n",
      "\n",
      "loss: 251285.546875  [    0/  790]\n",
      "Test Error:\n",
      "Avg loss: 1952905.447917 \n",
      "\n",
      "loss: 251271.062500  [    0/  790]\n",
      "Test Error:\n",
      "Avg loss: 1954149.414062 \n",
      "\n",
      "loss: 251153.546875  [    0/  790]\n",
      "Test Error:\n",
      "Avg loss: 1954012.664062 \n",
      "\n",
      "loss: 251114.812500  [    0/  790]\n",
      "Test Error:\n",
      "Avg loss: 1955676.565104 \n",
      "\n",
      "loss: 251458.671875  [    0/  790]\n",
      "Test Error:\n",
      "Avg loss: 1954444.833333 \n",
      "\n",
      "loss: 251319.765625  [    0/  790]\n",
      "Test Error:\n",
      "Avg loss: 1956148.148438 \n",
      "\n",
      "loss: 251327.312500  [    0/  790]\n",
      "Test Error:\n",
      "Avg loss: 1955796.533854 \n",
      "\n",
      "loss: 251179.078125  [    0/  790]\n",
      "Test Error:\n",
      "Avg loss: 1957780.700521 \n",
      "\n",
      "loss: 251646.656250  [    0/  790]\n",
      "Test Error:\n",
      "Avg loss: 1956327.479167 \n",
      "\n",
      "loss: 251407.015625  [    0/  790]\n",
      "Test Error:\n",
      "Avg loss: 1958467.028646 \n",
      "\n",
      "loss: 251519.843750  [    0/  790]\n",
      "Test Error:\n",
      "Avg loss: 1957816.492188 \n",
      "\n",
      "loss: 251261.906250  [    0/  790]\n",
      "Test Error:\n",
      "Avg loss: 1960186.195312 \n",
      "\n",
      "loss: 251822.515625  [    0/  790]\n",
      "Test Error:\n",
      "Avg loss: 1958578.695312 \n",
      "\n",
      "loss: 251083.296875  [    0/  790]\n",
      "Test Error:\n",
      "Avg loss: 1962073.507812 \n",
      "\n",
      "loss: 251722.093750  [    0/  790]\n",
      "Test Error:\n",
      "Avg loss: 1960527.187500 \n",
      "\n",
      "loss: 251359.109375  [    0/  790]\n",
      "Test Error:\n",
      "Avg loss: 1963244.947917 \n",
      "\n",
      "loss: 251957.078125  [    0/  790]\n",
      "Test Error:\n",
      "Avg loss: 1961458.458333 \n",
      "\n",
      "loss: 251250.250000  [    0/  790]\n",
      "Test Error:\n",
      "Avg loss: 1964718.031250 \n",
      "\n",
      "loss: 251882.906250  [    0/  790]\n",
      "Test Error:\n",
      "Avg loss: 1962903.158854 \n",
      "\n",
      "loss: 251598.703125  [    0/  790]\n",
      "Test Error:\n",
      "Avg loss: 1965105.653646 \n",
      "\n",
      "loss: 251737.843750  [    0/  790]\n",
      "Test Error:\n",
      "Avg loss: 1964344.153646 \n",
      "\n",
      "loss: 251417.312500  [    0/  790]\n",
      "Test Error:\n",
      "Avg loss: 1967471.861979 \n",
      "\n",
      "loss: 252007.062500  [    0/  790]\n",
      "Test Error:\n",
      "Avg loss: 1965447.747396 \n",
      "\n",
      "loss: 252283.671875  [    0/  790]\n",
      "Test Error:\n",
      "Avg loss: 1964341.716146 \n",
      "\n",
      "loss: 251635.343750  [    0/  790]\n",
      "Test Error:\n",
      "Avg loss: 1966019.281250 \n",
      "\n",
      "loss: 252125.515625  [    0/  790]\n",
      "Test Error:\n",
      "Avg loss: 1964909.007812 \n",
      "\n",
      "loss: 251884.046875  [    0/  790]\n",
      "Test Error:\n",
      "Avg loss: 1965949.174479 \n",
      "\n",
      "loss: 251911.562500  [    0/  790]\n",
      "Test Error:\n",
      "Avg loss: 1965896.992188 \n",
      "\n",
      "loss: 251701.343750  [    0/  790]\n",
      "Test Error:\n",
      "Avg loss: 1967249.382812 \n",
      "\n",
      "loss: 252241.187500  [    0/  790]\n",
      "Test Error:\n",
      "Avg loss: 1966191.325521 \n",
      "\n",
      "loss: 251921.109375  [    0/  790]\n",
      "Test Error:\n",
      "Avg loss: 1967686.809896 \n",
      "\n",
      "loss: 252011.046875  [    0/  790]\n",
      "Test Error:\n",
      "Avg loss: 1967483.403646 \n",
      "\n",
      "loss: 251727.046875  [    0/  790]\n",
      "Test Error:\n",
      "Avg loss: 1969752.273438 \n",
      "\n",
      "loss: 252295.000000  [    0/  790]\n",
      "Test Error:\n",
      "Avg loss: 1968332.145833 \n",
      "\n",
      "loss: 251549.703125  [    0/  790]\n",
      "Test Error:\n",
      "Avg loss: 1971403.966146 \n",
      "\n",
      "loss: 252166.890625  [    0/  790]\n",
      "Test Error:\n",
      "Avg loss: 1970051.335938 \n",
      "\n",
      "loss: 251933.015625  [    0/  790]\n",
      "Test Error:\n",
      "Avg loss: 1971867.083333 \n",
      "\n",
      "loss: 252480.468750  [    0/  790]\n",
      "Test Error:\n",
      "Avg loss: 1970490.395833 \n",
      "\n",
      "loss: 251734.609375  [    0/  790]\n",
      "Test Error:\n",
      "Avg loss: 1973152.343750 \n",
      "\n",
      "loss: 252336.812500  [    0/  790]\n",
      "Test Error:\n",
      "Avg loss: 1971749.697917 \n",
      "\n",
      "loss: 252099.625000  [    0/  790]\n",
      "Test Error:\n",
      "Avg loss: 1973474.593750 \n",
      "\n",
      "loss: 252136.000000  [    0/  790]\n",
      "Test Error:\n",
      "Avg loss: 1973138.518229 \n",
      "\n",
      "loss: 251951.109375  [    0/  790]\n",
      "Test Error:\n",
      "Avg loss: 1974812.596354 \n",
      "\n",
      "loss: 252511.562500  [    0/  790]\n",
      "Test Error:\n",
      "Avg loss: 1973472.966146 \n",
      "\n",
      "loss: 251753.765625  [    0/  790]\n",
      "Test Error:\n",
      "Avg loss: 1976094.940104 \n",
      "\n",
      "loss: 252366.484375  [    0/  790]\n",
      "Test Error:\n",
      "Avg loss: 1974968.127604 \n",
      "\n",
      "loss: 252049.375000  [    0/  790]\n",
      "Test Error:\n",
      "Avg loss: 1976845.945312 \n",
      "\n",
      "loss: 252638.406250  [    0/  790]\n",
      "Test Error:\n",
      "Avg loss: 1975622.523438 \n",
      "\n",
      "loss: 251872.953125  [    0/  790]\n",
      "Test Error:\n",
      "Avg loss: 1978183.822917 \n",
      "\n",
      "loss: 252489.937500  [    0/  790]\n",
      "Test Error:\n",
      "Avg loss: 1976977.145833 \n",
      "\n",
      "loss: 252259.343750  [    0/  790]\n",
      "Test Error:\n",
      "Avg loss: 1978482.250000 \n",
      "\n",
      "loss: 252286.859375  [    0/  790]\n",
      "Test Error:\n",
      "Avg loss: 1978353.750000 \n",
      "\n",
      "loss: 252115.593750  [    0/  790]\n",
      "Test Error:\n",
      "Avg loss: 1979821.341146 \n",
      "\n",
      "loss: 252684.640625  [    0/  790]\n",
      "Test Error:\n",
      "Avg loss: 1978549.695312 \n",
      "\n",
      "loss: 252450.375000  [    0/  790]\n",
      "Test Error:\n",
      "Avg loss: 1979875.867188 \n",
      "\n",
      "loss: 252465.843750  [    0/  790]\n",
      "Test Error:\n",
      "Avg loss: 1979607.054688 \n",
      "\n",
      "loss: 252287.718750  [    0/  790]\n",
      "Test Error:\n",
      "Avg loss: 1980749.445312 \n",
      "\n",
      "loss: 252844.968750  [    0/  790]\n",
      "Test Error:\n",
      "Avg loss: 1979768.174479 \n",
      "\n",
      "loss: 252083.312500  [    0/  790]\n",
      "Test Error:\n",
      "Avg loss: 1981771.513021 \n",
      "\n",
      "loss: 252689.718750  [    0/  790]\n",
      "Test Error:\n",
      "Avg loss: 1980839.083333 \n",
      "\n",
      "loss: 252480.531250  [    0/  790]\n",
      "Test Error:\n",
      "Avg loss: 1981820.322917 \n",
      "\n",
      "loss: 251786.109375  [    0/  790]\n",
      "Test Error:\n",
      "Avg loss: 1985707.643229 \n",
      "\n",
      "loss: 252484.468750  [    0/  790]\n",
      "Test Error:\n",
      "Avg loss: 1984506.320312 \n",
      "\n",
      "loss: 252252.656250  [    0/  790]\n",
      "Test Error:\n",
      "Avg loss: 1987051.122396 \n",
      "\n",
      "loss: 252338.203125  [    0/  790]\n",
      "Test Error:\n",
      "Avg loss: 1986862.424479 \n",
      "\n",
      "loss: 252138.562500  [    0/  790]\n",
      "Test Error:\n",
      "Avg loss: 1989395.000000 \n",
      "\n",
      "loss: 252774.937500  [    0/  790]\n",
      "Test Error:\n",
      "Avg loss: 1987785.966146 \n",
      "\n",
      "loss: 252521.265625  [    0/  790]\n",
      "Test Error:\n",
      "Avg loss: 1990072.507812 \n",
      "\n",
      "loss: 252589.703125  [    0/  790]\n",
      "Test Error:\n",
      "Avg loss: 1989432.437500 \n",
      "\n",
      "loss: 252369.609375  [    0/  790]\n",
      "Test Error:\n",
      "Avg loss: 1991517.528646 \n",
      "\n",
      "loss: 253010.109375  [    0/  790]\n",
      "Test Error:\n",
      "Avg loss: 1989995.382812 \n",
      "\n",
      "loss: 252194.437500  [    0/  790]\n",
      "Test Error:\n",
      "Avg loss: 1993013.054688 \n",
      "\n",
      "loss: 252862.921875  [    0/  790]\n",
      "Test Error:\n",
      "Avg loss: 1991477.033854 \n",
      "\n",
      "loss: 252599.906250  [    0/  790]\n",
      "Test Error:\n",
      "Avg loss: 1993436.864583 \n",
      "\n",
      "loss: 252622.734375  [    0/  790]\n",
      "Test Error:\n",
      "Avg loss: 1993059.960938 \n",
      "\n",
      "loss: 252416.843750  [    0/  790]\n",
      "Test Error:\n",
      "Avg loss: 1994981.804688 \n",
      "\n",
      "loss: 253037.859375  [    0/  790]\n",
      "Test Error:\n",
      "Avg loss: 1993309.843750 \n",
      "\n",
      "loss: 252787.218750  [    0/  790]\n",
      "Test Error:\n",
      "Avg loss: 1995046.710938 \n",
      "\n",
      "loss: 252796.359375  [    0/  790]\n",
      "Test Error:\n",
      "Avg loss: 1994500.299479 \n",
      "\n",
      "loss: 252589.062500  [    0/  790]\n",
      "Test Error:\n",
      "Avg loss: 1996074.273438 \n",
      "\n",
      "loss: 253198.406250  [    0/  790]\n",
      "Test Error:\n",
      "Avg loss: 1994758.479167 \n",
      "\n",
      "loss: 252350.921875  [    0/  790]\n",
      "Test Error:\n",
      "Avg loss: 1997298.281250 \n",
      "\n",
      "loss: 252971.781250  [    0/  790]\n",
      "Test Error:\n",
      "Avg loss: 1996103.575521 \n",
      "\n",
      "loss: 252735.281250  [    0/  790]\n",
      "Test Error:\n",
      "Avg loss: 1997292.708333 \n",
      "\n",
      "loss: 253328.890625  [    0/  790]\n",
      "Test Error:\n",
      "Avg loss: 1996144.739583 \n",
      "\n",
      "loss: 252481.453125  [    0/  790]\n",
      "Test Error:\n",
      "Avg loss: 1998260.114583 \n",
      "\n",
      "loss: 253085.421875  [    0/  790]\n",
      "Test Error:\n",
      "Avg loss: 1997160.945312 \n",
      "\n",
      "loss: 252853.859375  [    0/  790]\n",
      "Test Error:\n",
      "Avg loss: 1998397.554688 \n",
      "\n",
      "loss: 252807.437500  [    0/  790]\n",
      "Test Error:\n",
      "Avg loss: 1998624.617188 \n",
      "\n",
      "loss: 252594.578125  [    0/  790]\n",
      "Test Error:\n",
      "Avg loss: 1999718.666667 \n",
      "\n",
      "loss: 253155.156250  [    0/  790]\n",
      "Test Error:\n",
      "Avg loss: 1998760.554688 \n",
      "\n",
      "loss: 252924.296875  [    0/  790]\n",
      "Test Error:\n",
      "Avg loss: 1999678.583333 \n",
      "\n",
      "loss: 252854.953125  [    0/  790]\n",
      "Test Error:\n",
      "Avg loss: 2000166.877604 \n",
      "\n",
      "loss: 252635.171875  [    0/  790]\n",
      "Test Error:\n",
      "Avg loss: 2001058.752604 \n",
      "\n",
      "loss: 252465.312500  [    0/  790]\n",
      "Test Error:\n",
      "Avg loss: 2003890.854167 \n",
      "\n",
      "loss: 253104.812500  [    0/  790]\n",
      "Test Error:\n",
      "Avg loss: 2002267.661458 \n",
      "\n",
      "loss: 252844.718750  [    0/  790]\n",
      "Test Error:\n",
      "Avg loss: 2005074.369792 \n",
      "\n",
      "loss: 252870.718750  [    0/  790]\n",
      "Test Error:\n",
      "Avg loss: 2004487.296875 \n",
      "\n",
      "loss: 252600.796875  [    0/  790]\n",
      "Test Error:\n",
      "Avg loss: 2006943.963542 \n",
      "\n",
      "loss: 253247.125000  [    0/  790]\n",
      "Test Error:\n",
      "Avg loss: 2005123.109375 \n",
      "\n",
      "loss: 252957.640625  [    0/  790]\n",
      "Test Error:\n",
      "Avg loss: 2007671.354167 \n",
      "\n",
      "loss: 252979.843750  [    0/  790]\n",
      "Test Error:\n",
      "Avg loss: 2007092.671875 \n",
      "\n",
      "loss: 252616.828125  [    0/  790]\n",
      "Test Error:\n",
      "Avg loss: 2009468.213542 \n",
      "\n",
      "loss: 253311.406250  [    0/  790]\n",
      "Test Error:\n",
      "Avg loss: 2008271.942708 \n",
      "\n",
      "loss: 252227.609375  [    0/  790]\n",
      "Test Error:\n",
      "Avg loss: 2012113.359375 \n",
      "\n",
      "loss: 252997.171875  [    0/  790]\n",
      "Test Error:\n",
      "Avg loss: 2010786.270833 \n",
      "\n",
      "loss: 252541.375000  [    0/  790]\n",
      "Test Error:\n",
      "Avg loss: 2013171.390625 \n",
      "\n",
      "loss: 253280.687500  [    0/  790]\n",
      "Test Error:\n",
      "Avg loss: 2011419.932292 \n",
      "\n",
      "loss: 252792.796875  [    0/  790]\n",
      "Test Error:\n",
      "Avg loss: 2013388.552083 \n",
      "\n",
      "loss: 253548.078125  [    0/  790]\n",
      "Test Error:\n",
      "Avg loss: 2012078.171875 \n",
      "\n",
      "loss: 252375.812500  [    0/  790]\n",
      "Test Error:\n",
      "Avg loss: 2015449.354167 \n",
      "\n",
      "loss: 253201.062500  [    0/  790]\n",
      "Test Error:\n",
      "Avg loss: 2013849.567708 \n",
      "\n",
      "loss: 252658.156250  [    0/  790]\n",
      "Test Error:\n",
      "Avg loss: 2015158.546875 \n",
      "\n",
      "loss: 253374.281250  [    0/  790]\n",
      "Test Error:\n",
      "Avg loss: 2013484.526042 \n",
      "\n",
      "loss: 252825.109375  [    0/  790]\n",
      "Test Error:\n",
      "Avg loss: 2016169.932292 \n",
      "\n",
      "loss: 252965.046875  [    0/  790]\n",
      "Test Error:\n",
      "Avg loss: 2015676.494792 \n",
      "\n",
      "loss: 252438.125000  [    0/  790]\n",
      "Test Error:\n",
      "Avg loss: 2017987.354167 \n",
      "\n",
      "loss: 253286.968750  [    0/  790]\n",
      "Test Error:\n",
      "Avg loss: 2016277.875000 \n",
      "\n",
      "loss: 252657.812500  [    0/  790]\n",
      "Test Error:\n",
      "Avg loss: 2018236.177083 \n",
      "\n",
      "loss: 253540.765625  [    0/  790]\n",
      "Test Error:\n",
      "Avg loss: 2017182.682292 \n",
      "\n",
      "loss: 252182.046875  [    0/  790]\n",
      "Test Error:\n",
      "Avg loss: 2020686.796875 \n",
      "\n",
      "loss: 253146.375000  [    0/  790]\n",
      "Test Error:\n",
      "Avg loss: 2019213.307292 \n",
      "\n",
      "loss: 252497.812500  [    0/  790]\n",
      "Test Error:\n",
      "Avg loss: 2020430.354167 \n",
      "\n",
      "loss: 253323.578125  [    0/  790]\n",
      "Test Error:\n",
      "Avg loss: 2018924.411458 \n",
      "\n",
      "loss: 252640.421875  [    0/  790]\n",
      "Test Error:\n",
      "Avg loss: 2021721.619792 \n",
      "\n",
      "loss: 252856.656250  [    0/  790]\n",
      "Test Error:\n",
      "Avg loss: 2022119.963542 \n",
      "\n",
      "loss: 251928.000000  [    0/  790]\n",
      "Test Error:\n",
      "Avg loss: 2025993.046875 \n",
      "\n",
      "loss: 252653.968750  [    0/  790]\n",
      "Test Error:\n",
      "Avg loss: 2026663.500000 \n",
      "\n",
      "loss: 251604.500000  [    0/  790]\n",
      "Test Error:\n",
      "Avg loss: 2031662.640625 \n",
      "\n",
      "loss: 252117.218750  [    0/  790]\n",
      "Test Error:\n",
      "Avg loss: 2033672.817708 \n",
      "\n",
      "loss: 250829.843750  [    0/  790]\n",
      "Test Error:\n",
      "Avg loss: 2038518.026042 \n",
      "\n",
      "loss: 250984.531250  [    0/  790]\n",
      "Test Error:\n",
      "Avg loss: 2032724.255208 \n",
      "\n",
      "loss: 249661.546875  [    0/  790]\n",
      "Test Error:\n",
      "Avg loss: 2027372.869792 \n",
      "\n",
      "loss: 249815.921875  [    0/  790]\n",
      "Test Error:\n",
      "Avg loss: 1995370.729167 \n",
      "\n",
      "loss: 247934.781250  [    0/  790]\n",
      "Test Error:\n",
      "Avg loss: 1959577.401042 \n",
      "\n",
      "loss: 245673.828125  [    0/  790]\n",
      "Test Error:\n",
      "Avg loss: 1919212.593750 \n",
      "\n",
      "loss: 244944.562500  [    0/  790]\n",
      "Test Error:\n",
      "Avg loss: 1899595.656250 \n",
      "\n",
      "loss: 244011.031250  [    0/  790]\n",
      "Test Error:\n",
      "Avg loss: 1881239.221354 \n",
      "\n",
      "loss: 242993.750000  [    0/  790]\n",
      "Test Error:\n",
      "Avg loss: 1862199.062500 \n",
      "\n",
      "loss: 243206.421875  [    0/  790]\n",
      "Test Error:\n",
      "Avg loss: 1840975.385417 \n",
      "\n",
      "loss: 242252.156250  [    0/  790]\n",
      "Test Error:\n",
      "Avg loss: 1823603.330729 \n",
      "\n",
      "loss: 241280.843750  [    0/  790]\n",
      "Test Error:\n",
      "Avg loss: 1807315.747396 \n",
      "\n",
      "loss: 241718.468750  [    0/  790]\n",
      "Test Error:\n",
      "Avg loss: 1800361.966146 \n",
      "\n",
      "loss: 241240.250000  [    0/  790]\n",
      "Test Error:\n",
      "Avg loss: 1791046.031250 \n",
      "\n",
      "loss: 240198.531250  [    0/  790]\n",
      "Test Error:\n",
      "Avg loss: 1787346.596354 \n",
      "\n",
      "loss: 240520.421875  [    0/  790]\n",
      "Test Error:\n",
      "Avg loss: 1779449.914062 \n",
      "\n",
      "loss: 239855.140625  [    0/  790]\n",
      "Test Error:\n",
      "Avg loss: 1773104.489583 \n",
      "\n",
      "loss: 239234.406250  [    0/  790]\n",
      "Test Error:\n",
      "Avg loss: 1770585.200521 \n",
      "\n",
      "loss: 240129.375000  [    0/  790]\n",
      "Test Error:\n",
      "Avg loss: 1775847.104167 \n",
      "\n",
      "loss: 240064.453125  [    0/  790]\n",
      "Test Error:\n",
      "Avg loss: 1776742.356771 \n",
      "\n",
      "loss: 239560.859375  [    0/  790]\n",
      "Test Error:\n",
      "Avg loss: 1780476.666667 \n",
      "\n",
      "loss: 240574.718750  [    0/  790]\n",
      "Test Error:\n",
      "Avg loss: 1781172.278646 \n",
      "\n",
      "loss: 240129.109375  [    0/  790]\n",
      "Test Error:\n",
      "Avg loss: 1784538.783854 \n",
      "\n",
      "loss: 239754.796875  [    0/  790]\n",
      "Test Error:\n",
      "Avg loss: 1788514.424479 \n",
      "\n",
      "loss: 240816.312500  [    0/  790]\n",
      "Test Error:\n",
      "Avg loss: 1789669.268229 \n",
      "\n",
      "loss: 240352.671875  [    0/  790]\n",
      "Test Error:\n",
      "Avg loss: 1792187.559896 \n",
      "\n",
      "loss: 239912.187500  [    0/  790]\n",
      "Test Error:\n",
      "Avg loss: 1796565.721354 \n",
      "\n",
      "loss: 241014.968750  [    0/  790]\n",
      "Test Error:\n",
      "Avg loss: 1797371.414062 \n",
      "\n",
      "loss: 240437.156250  [    0/  790]\n",
      "Test Error:\n",
      "Avg loss: 1798538.033854 \n",
      "\n",
      "loss: 241354.859375  [    0/  790]\n",
      "Test Error:\n",
      "Avg loss: 1798605.229167 \n",
      "\n",
      "loss: 240752.484375  [    0/  790]\n",
      "Test Error:\n",
      "Avg loss: 1798608.864583 \n",
      "\n",
      "loss: 240183.500000  [    0/  790]\n",
      "Test Error:\n",
      "Avg loss: 1801379.117188 \n",
      "\n",
      "loss: 241144.484375  [    0/  790]\n",
      "Test Error:\n",
      "Avg loss: 1800369.778646 \n",
      "\n",
      "loss: 240473.828125  [    0/  790]\n",
      "Test Error:\n",
      "Avg loss: 1800691.125000 \n",
      "\n",
      "loss: 241362.218750  [    0/  790]\n",
      "Test Error:\n",
      "Avg loss: 1800332.562500 \n",
      "\n",
      "loss: 240710.984375  [    0/  790]\n",
      "Test Error:\n",
      "Avg loss: 1800182.825521 \n",
      "\n",
      "loss: 240025.234375  [    0/  790]\n",
      "Test Error:\n",
      "Avg loss: 1804610.135417 \n",
      "\n",
      "loss: 241166.062500  [    0/  790]\n",
      "Test Error:\n",
      "Avg loss: 1804955.570312 \n",
      "\n",
      "loss: 240545.984375  [    0/  790]\n",
      "Test Error:\n",
      "Avg loss: 1805853.177083 \n",
      "\n",
      "loss: 241408.484375  [    0/  790]\n",
      "Test Error:\n",
      "Avg loss: 1804641.111979 \n",
      "\n",
      "loss: 240740.812500  [    0/  790]\n",
      "Test Error:\n",
      "Avg loss: 1804627.875000 \n",
      "\n",
      "loss: 240147.515625  [    0/  790]\n",
      "Test Error:\n",
      "Avg loss: 1807380.208333 \n",
      "\n",
      "loss: 241147.375000  [    0/  790]\n",
      "Test Error:\n",
      "Avg loss: 1805735.460938 \n",
      "\n",
      "loss: 240410.593750  [    0/  790]\n",
      "Test Error:\n",
      "Avg loss: 1805795.799479 \n",
      "\n",
      "loss: 241349.531250  [    0/  790]\n",
      "Test Error:\n",
      "Avg loss: 1805386.174479 \n",
      "\n",
      "loss: 240698.312500  [    0/  790]\n",
      "Test Error:\n",
      "Avg loss: 1805086.528646 \n",
      "\n",
      "loss: 240013.250000  [    0/  790]\n",
      "Test Error:\n",
      "Avg loss: 1805897.210938 \n",
      "\n",
      "loss: 241036.031250  [    0/  790]\n",
      "Test Error:\n",
      "Avg loss: 1805274.325521 \n",
      "\n",
      "loss: 240341.484375  [    0/  790]\n",
      "Test Error:\n",
      "Avg loss: 1804980.947917 \n",
      "\n",
      "loss: 241168.812500  [    0/  790]\n",
      "Test Error:\n",
      "Avg loss: 1802910.408854 \n",
      "\n",
      "loss: 240445.187500  [    0/  790]\n",
      "Test Error:\n",
      "Avg loss: 1802129.174479 \n",
      "\n",
      "loss: 239837.484375  [    0/  790]\n",
      "Test Error:\n",
      "Avg loss: 1804062.677083 \n",
      "\n",
      "loss: 240833.421875  [    0/  790]\n",
      "Test Error:\n",
      "Avg loss: 1801959.507812 \n",
      "\n",
      "loss: 240077.203125  [    0/  790]\n",
      "Test Error:\n",
      "Avg loss: 1792627.117188 \n",
      "\n",
      "loss: 240364.187500  [    0/  790]\n",
      "Test Error:\n",
      "Avg loss: 1786062.919271 \n",
      "\n",
      "loss: 239200.531250  [    0/  790]\n",
      "Test Error:\n",
      "Avg loss: 1781231.341146 \n",
      "\n",
      "loss: 238226.953125  [    0/  790]\n",
      "Test Error:\n",
      "Avg loss: 1777907.361979 \n",
      "\n",
      "loss: 238856.750000  [    0/  790]\n",
      "Test Error:\n",
      "Avg loss: 1784704.614583 \n",
      "\n",
      "loss: 238395.890625  [    0/  790]\n",
      "Test Error:\n",
      "Avg loss: 1785813.708333 \n",
      "\n",
      "loss: 237798.593750  [    0/  790]\n",
      "Test Error:\n",
      "Avg loss: 1788955.143229 \n",
      "\n",
      "loss: 238725.640625  [    0/  790]\n",
      "Test Error:\n",
      "Avg loss: 1787559.375000 \n",
      "\n",
      "loss: 238009.531250  [    0/  790]\n",
      "Test Error:\n",
      "Avg loss: 1789473.822917 \n",
      "\n",
      "loss: 238893.937500  [    0/  790]\n",
      "Test Error:\n",
      "Avg loss: 1788131.617188 \n",
      "\n",
      "loss: 238197.234375  [    0/  790]\n",
      "Test Error:\n",
      "Avg loss: 1789673.091146 \n",
      "\n",
      "loss: 237519.875000  [    0/  790]\n",
      "Test Error:\n",
      "Avg loss: 1790680.664062 \n",
      "\n",
      "loss: 238451.890625  [    0/  790]\n",
      "Test Error:\n",
      "Avg loss: 1790719.799479 \n",
      "\n",
      "loss: 237710.234375  [    0/  790]\n",
      "Test Error:\n",
      "Avg loss: 1791018.138021 \n",
      "\n",
      "loss: 238549.703125  [    0/  790]\n",
      "Test Error:\n",
      "Avg loss: 1789797.299479 \n",
      "\n",
      "loss: 237891.453125  [    0/  790]\n",
      "Test Error:\n",
      "Avg loss: 1791250.919271 \n",
      "\n",
      "loss: 237241.500000  [    0/  790]\n",
      "Test Error:\n",
      "Avg loss: 1791914.552083 \n",
      "\n",
      "loss: 238186.781250  [    0/  790]\n",
      "Test Error:\n",
      "Avg loss: 1792138.502604 \n",
      "\n",
      "loss: 237480.046875  [    0/  790]\n",
      "Test Error:\n",
      "Avg loss: 1791676.187500 \n",
      "\n",
      "loss: 236724.656250  [    0/  790]\n",
      "Test Error:\n",
      "Avg loss: 1794371.375000 \n",
      "\n",
      "loss: 237598.078125  [    0/  790]\n",
      "Test Error:\n",
      "Avg loss: 1808439.638021 \n",
      "\n",
      "loss: 237618.187500  [    0/  790]\n",
      "Test Error:\n",
      "Avg loss: 1809313.406250 \n",
      "\n",
      "loss: 238684.812500  [    0/  790]\n",
      "Test Error:\n",
      "Avg loss: 1794336.169271 \n",
      "\n",
      "loss: 237239.906250  [    0/  790]\n",
      "Test Error:\n",
      "Avg loss: 1809954.039062 \n",
      "\n",
      "loss: 237358.265625  [    0/  790]\n",
      "Test Error:\n",
      "Avg loss: 1813059.325521 \n",
      "\n",
      "loss: 238412.343750  [    0/  790]\n",
      "Test Error:\n",
      "Avg loss: 1811990.513021 \n",
      "\n",
      "loss: 237755.359375  [    0/  790]\n",
      "Test Error:\n",
      "Avg loss: 1812504.752604 \n",
      "\n",
      "loss: 238690.671875  [    0/  790]\n",
      "Test Error:\n",
      "Avg loss: 1810928.062500 \n",
      "\n",
      "loss: 238113.609375  [    0/  790]\n",
      "Test Error:\n",
      "Avg loss: 1812206.104167 \n",
      "\n",
      "loss: 237481.562500  [    0/  790]\n",
      "Test Error:\n",
      "Avg loss: 1813267.020833 \n",
      "\n",
      "loss: 238438.140625  [    0/  790]\n",
      "Test Error:\n",
      "Avg loss: 1795245.809896 \n",
      "\n",
      "loss: 236859.640625  [    0/  790]\n",
      "Test Error:\n",
      "Avg loss: 1811399.666667 \n",
      "\n",
      "loss: 238596.859375  [    0/  790]\n",
      "Test Error:\n",
      "Avg loss: 1811347.726562 \n",
      "\n",
      "loss: 237983.921875  [    0/  790]\n",
      "Test Error:\n",
      "Avg loss: 1810793.992188 \n",
      "\n",
      "loss: 237390.906250  [    0/  790]\n",
      "Test Error:\n",
      "Avg loss: 1812866.236979 \n",
      "\n",
      "loss: 238391.468750  [    0/  790]\n",
      "Test Error:\n",
      "Avg loss: 1811442.429688 \n",
      "\n",
      "loss: 237724.437500  [    0/  790]\n",
      "Test Error:\n",
      "Avg loss: 1811569.322917 \n",
      "\n",
      "loss: 236991.843750  [    0/  790]\n",
      "Test Error:\n",
      "Avg loss: 1812525.466146 \n",
      "\n",
      "loss: 238057.250000  [    0/  790]\n",
      "Test Error:\n",
      "Avg loss: 1812376.747396 \n",
      "\n",
      "loss: 237321.656250  [    0/  790]\n",
      "Test Error:\n",
      "Avg loss: 1812675.260417 \n",
      "\n",
      "loss: 238243.875000  [    0/  790]\n",
      "Test Error:\n",
      "Avg loss: 1810997.760417 \n",
      "\n",
      "loss: 237573.093750  [    0/  790]\n",
      "Test Error:\n",
      "Avg loss: 1810691.226562 \n",
      "\n",
      "loss: 236941.156250  [    0/  790]\n",
      "Test Error:\n",
      "Avg loss: 1812975.518229 \n",
      "\n",
      "loss: 237934.625000  [    0/  790]\n",
      "Test Error:\n",
      "Avg loss: 1811386.289062 \n",
      "\n",
      "loss: 237148.359375  [    0/  790]\n",
      "Test Error:\n",
      "Avg loss: 1811159.132812 \n",
      "\n",
      "loss: 238154.265625  [    0/  790]\n",
      "Test Error:\n",
      "Avg loss: 1810744.937500 \n",
      "\n",
      "loss: 237448.250000  [    0/  790]\n",
      "Test Error:\n",
      "Avg loss: 1810884.364583 \n",
      "\n",
      "loss: 236632.765625  [    0/  790]\n",
      "Test Error:\n",
      "Avg loss: 1811630.591146 \n",
      "\n",
      "loss: 237715.375000  [    0/  790]\n",
      "Test Error:\n",
      "Avg loss: 1811424.815104 \n",
      "\n",
      "loss: 236969.078125  [    0/  790]\n",
      "Test Error:\n",
      "Avg loss: 1811627.148438 \n",
      "\n",
      "loss: 237867.156250  [    0/  790]\n",
      "Test Error:\n",
      "Avg loss: 1809986.312500 \n",
      "\n",
      "loss: 237080.437500  [    0/  790]\n",
      "Test Error:\n",
      "Avg loss: 1809139.075521 \n",
      "\n",
      "loss: 238055.906250  [    0/  790]\n",
      "Test Error:\n",
      "Avg loss: 1808795.914062 \n",
      "\n",
      "loss: 237457.187500  [    0/  790]\n",
      "Test Error:\n",
      "Avg loss: 1808414.080729 \n",
      "\n",
      "loss: 236720.406250  [    0/  790]\n",
      "Test Error:\n",
      "Avg loss: 1808266.700521 \n",
      "\n",
      "loss: 237726.375000  [    0/  790]\n",
      "Test Error:\n",
      "Avg loss: 1808195.320312 \n",
      "\n",
      "loss: 237015.812500  [    0/  790]\n",
      "Test Error:\n",
      "Avg loss: 1808597.520833 \n",
      "\n",
      "loss: 236203.687500  [    0/  790]\n",
      "Test Error:\n",
      "Avg loss: 1809161.440104 \n",
      "\n",
      "loss: 237218.359375  [    0/  790]\n",
      "Test Error:\n",
      "Avg loss: 1809464.812500 \n",
      "\n",
      "loss: 236415.109375  [    0/  790]\n",
      "Test Error:\n",
      "Avg loss: 1809679.846354 \n",
      "\n",
      "loss: 237369.468750  [    0/  790]\n",
      "Test Error:\n",
      "Avg loss: 1808285.127604 \n",
      "\n",
      "loss: 236589.531250  [    0/  790]\n",
      "Test Error:\n",
      "Avg loss: 1807544.747396 \n",
      "\n",
      "loss: 237570.515625  [    0/  790]\n",
      "Test Error:\n",
      "Avg loss: 1807442.903646 \n",
      "\n",
      "loss: 236864.156250  [    0/  790]\n",
      "Test Error:\n",
      "Avg loss: 1807576.739583 \n",
      "\n",
      "loss: 236062.359375  [    0/  790]\n",
      "Test Error:\n",
      "Avg loss: 1807585.385417 \n",
      "\n",
      "loss: 237046.437500  [    0/  790]\n",
      "Test Error:\n",
      "Avg loss: 1807930.976562 \n",
      "\n",
      "loss: 236272.687500  [    0/  790]\n",
      "Test Error:\n",
      "Avg loss: 1807651.635417 \n",
      "\n",
      "loss: 237191.515625  [    0/  790]\n",
      "Test Error:\n",
      "Avg loss: 1806188.601562 \n",
      "\n",
      "loss: 236574.625000  [    0/  790]\n",
      "Test Error:\n",
      "Avg loss: 1807968.049479 \n",
      "\n",
      "loss: 235821.484375  [    0/  790]\n",
      "Test Error:\n",
      "Avg loss: 1808193.919271 \n",
      "\n",
      "loss: 236729.484375  [    0/  790]\n",
      "Test Error:\n",
      "Avg loss: 1807184.531250 \n",
      "\n",
      "loss: 236007.734375  [    0/  790]\n",
      "Test Error:\n",
      "Avg loss: 1808287.976562 \n",
      "\n",
      "loss: 237002.453125  [    0/  790]\n",
      "Test Error:\n",
      "Avg loss: 1807303.304688 \n",
      "\n",
      "loss: 236243.734375  [    0/  790]\n",
      "Test Error:\n",
      "Avg loss: 1807838.372396 \n",
      "\n",
      "loss: 235531.796875  [    0/  790]\n",
      "Test Error:\n",
      "Avg loss: 1809568.861979 \n",
      "\n",
      "loss: 236530.812500  [    0/  790]\n",
      "Test Error:\n",
      "Avg loss: 1808645.007812 \n",
      "\n",
      "loss: 235744.000000  [    0/  790]\n",
      "Test Error:\n",
      "Avg loss: 1808422.114583 \n",
      "\n",
      "loss: 236608.359375  [    0/  790]\n",
      "Test Error:\n",
      "Avg loss: 1806958.497396 \n",
      "\n",
      "loss: 235960.140625  [    0/  790]\n",
      "Test Error:\n",
      "Avg loss: 1809466.197917 \n",
      "\n",
      "loss: 235220.937500  [    0/  790]\n",
      "Test Error:\n",
      "Avg loss: 1810989.669271 \n",
      "\n",
      "loss: 235418.921875  [    0/  790]\n",
      "Test Error:\n",
      "Avg loss: 1809175.585938 \n",
      "\n",
      "loss: 236492.968750  [    0/  790]\n",
      "Test Error:\n",
      "Avg loss: 1809937.414062 \n",
      "\n",
      "loss: 235668.687500  [    0/  790]\n",
      "Test Error:\n",
      "Avg loss: 1808598.486979 \n",
      "\n",
      "loss: 236578.406250  [    0/  790]\n",
      "Test Error:\n",
      "Avg loss: 1807490.846354 \n",
      "\n",
      "loss: 235779.531250  [    0/  790]\n",
      "Test Error:\n",
      "Avg loss: 1808208.416667 \n",
      "\n",
      "loss: 235101.343750  [    0/  790]\n",
      "Test Error:\n",
      "Avg loss: 1810154.398438 \n",
      "\n",
      "loss: 235405.062500  [    0/  790]\n",
      "Test Error:\n",
      "Avg loss: 1809805.440104 \n",
      "\n",
      "loss: 235508.031250  [    0/  790]\n",
      "Test Error:\n",
      "Avg loss: 1810011.778646 \n",
      "\n",
      "loss: 234785.781250  [    0/  790]\n",
      "Test Error:\n",
      "Avg loss: 1810946.815104 \n",
      "\n",
      "loss: 235146.968750  [    0/  790]\n",
      "Test Error:\n",
      "Avg loss: 1811996.653646 \n",
      "\n",
      "loss: 235310.281250  [    0/  790]\n",
      "Test Error:\n",
      "Avg loss: 1810700.927083 \n",
      "\n",
      "loss: 235462.359375  [    0/  790]\n",
      "Test Error:\n",
      "Avg loss: 1810771.760417 \n",
      "\n",
      "loss: 234732.218750  [    0/  790]\n",
      "Test Error:\n",
      "Avg loss: 1811516.385417 \n",
      "\n",
      "loss: 235038.531250  [    0/  790]\n",
      "Test Error:\n",
      "Avg loss: 1812457.802083 \n",
      "\n",
      "loss: 235136.718750  [    0/  790]\n",
      "Test Error:\n",
      "Avg loss: 1811207.924479 \n",
      "\n",
      "loss: 235215.593750  [    0/  790]\n",
      "Test Error:\n",
      "Avg loss: 1812370.700521 \n",
      "\n",
      "loss: 233839.296875  [    0/  790]\n",
      "Test Error:\n",
      "Avg loss: 1819306.205729 \n",
      "\n",
      "loss: 233804.437500  [    0/  790]\n",
      "Test Error:\n",
      "Avg loss: 1823369.330729 \n",
      "\n",
      "loss: 233269.171875  [    0/  790]\n",
      "Test Error:\n",
      "Avg loss: 1826827.205729 \n",
      "\n",
      "loss: 232456.546875  [    0/  790]\n",
      "Test Error:\n",
      "Avg loss: 1829159.158854 \n",
      "\n",
      "loss: 231928.921875  [    0/  790]\n",
      "Test Error:\n",
      "Avg loss: 1836155.270833 \n",
      "\n",
      "loss: 232134.921875  [    0/  790]\n",
      "Test Error:\n",
      "Avg loss: 1839068.270833 \n",
      "\n",
      "loss: 232115.296875  [    0/  790]\n",
      "Test Error:\n",
      "Avg loss: 1835763.559896 \n",
      "\n",
      "loss: 231736.828125  [    0/  790]\n",
      "Test Error:\n",
      "Avg loss: 1806379.479167 \n",
      "\n",
      "loss: 230401.859375  [    0/  790]\n",
      "Test Error:\n",
      "Avg loss: 1779465.346354 \n",
      "\n",
      "loss: 229071.343750  [    0/  790]\n",
      "Test Error:\n",
      "Avg loss: 1741728.882812 \n",
      "\n",
      "loss: 227441.421875  [    0/  790]\n",
      "Test Error:\n",
      "Avg loss: 1702428.752604 \n",
      "\n",
      "loss: 226136.062500  [    0/  790]\n",
      "Test Error:\n",
      "Avg loss: 1719537.960938 \n",
      "\n",
      "loss: 227043.140625  [    0/  790]\n",
      "Test Error:\n",
      "Avg loss: 1727975.510417 \n",
      "\n",
      "loss: 227470.921875  [    0/  790]\n",
      "Test Error:\n",
      "Avg loss: 1731733.114583 \n",
      "\n",
      "loss: 227696.984375  [    0/  790]\n",
      "Test Error:\n",
      "Avg loss: 1757161.739583 \n",
      "\n",
      "loss: 229047.281250  [    0/  790]\n",
      "Test Error:\n",
      "Avg loss: 1734231.346354 \n",
      "\n",
      "loss: 228095.562500  [    0/  790]\n",
      "Test Error:\n",
      "Avg loss: 1764765.247396 \n",
      "\n",
      "loss: 229610.625000  [    0/  790]\n",
      "Test Error:\n",
      "Avg loss: 1738902.187500 \n",
      "\n",
      "loss: 228520.218750  [    0/  790]\n",
      "Test Error:\n",
      "Avg loss: 1767076.794271 \n",
      "\n",
      "loss: 229933.156250  [    0/  790]\n",
      "Test Error:\n",
      "Avg loss: 1734745.106771 \n",
      "\n",
      "loss: 228583.468750  [    0/  790]\n",
      "Test Error:\n",
      "Avg loss: 1757764.054688 \n",
      "\n",
      "loss: 229805.125000  [    0/  790]\n",
      "Test Error:\n",
      "Avg loss: 1703235.476562 \n",
      "\n",
      "loss: 227699.015625  [    0/  790]\n",
      "Test Error:\n",
      "Avg loss: 1706527.789062 \n",
      "\n",
      "loss: 228296.250000  [    0/  790]\n",
      "Test Error:\n",
      "Avg loss: 1694383.197917 \n",
      "\n",
      "loss: 228120.171875  [    0/  790]\n",
      "Test Error:\n",
      "Avg loss: 1660232.398438 \n",
      "\n",
      "loss: 226718.625000  [    0/  790]\n",
      "Test Error:\n",
      "Avg loss: 1680887.062500 \n",
      "\n",
      "loss: 227918.312500  [    0/  790]\n",
      "Test Error:\n",
      "Avg loss: 1645879.877604 \n",
      "\n",
      "loss: 226468.875000  [    0/  790]\n",
      "Test Error:\n",
      "Avg loss: 1666969.132812 \n",
      "\n",
      "loss: 227688.484375  [    0/  790]\n",
      "Test Error:\n",
      "Avg loss: 1661270.312500 \n",
      "\n",
      "loss: 227704.125000  [    0/  790]\n",
      "Test Error:\n",
      "Avg loss: 1610429.627604 \n",
      "\n",
      "loss: 225715.250000  [    0/  790]\n",
      "Test Error:\n",
      "Avg loss: 1636071.781250 \n",
      "\n",
      "loss: 227189.375000  [    0/  790]\n",
      "Test Error:\n",
      "Avg loss: 1631480.312500 \n",
      "\n",
      "loss: 227149.250000  [    0/  790]\n",
      "Test Error:\n",
      "Avg loss: 1600074.666667 \n",
      "\n",
      "loss: 225655.265625  [    0/  790]\n",
      "Test Error:\n",
      "Avg loss: 1631359.593750 \n",
      "\n",
      "loss: 227162.765625  [    0/  790]\n",
      "Test Error:\n",
      "Avg loss: 1634697.976562 \n",
      "\n",
      "loss: 227392.437500  [    0/  790]\n",
      "Test Error:\n",
      "Avg loss: 1634719.179688 \n",
      "\n",
      "loss: 227449.031250  [    0/  790]\n",
      "Test Error:\n",
      "Avg loss: 1605756.945312 \n",
      "\n",
      "loss: 225986.484375  [    0/  790]\n",
      "Test Error:\n",
      "Avg loss: 1632567.773438 \n",
      "\n",
      "loss: 227228.718750  [    0/  790]\n",
      "Test Error:\n",
      "Avg loss: 1630582.747396 \n",
      "\n",
      "loss: 227037.609375  [    0/  790]\n",
      "Test Error:\n",
      "Avg loss: 1613224.971354 \n",
      "\n",
      "loss: 225825.953125  [    0/  790]\n",
      "Test Error:\n",
      "Avg loss: 1575996.059896 \n",
      "\n",
      "loss: 223514.703125  [    0/  790]\n",
      "Test Error:\n",
      "Avg loss: 1596178.486979 \n",
      "\n",
      "loss: 224372.234375  [    0/  790]\n",
      "Test Error:\n",
      "Avg loss: 1595312.213542 \n",
      "\n",
      "loss: 224400.781250  [    0/  790]\n",
      "Test Error:\n",
      "Avg loss: 1596302.601562 \n",
      "\n",
      "loss: 224525.796875  [    0/  790]\n",
      "Test Error:\n",
      "Avg loss: 1569111.658854 \n",
      "\n",
      "loss: 223201.375000  [    0/  790]\n",
      "Test Error:\n",
      "Avg loss: 1611386.190104 \n",
      "\n",
      "loss: 225212.515625  [    0/  790]\n",
      "Test Error:\n",
      "Avg loss: 1615344.205729 \n",
      "\n",
      "loss: 225477.000000  [    0/  790]\n",
      "Test Error:\n",
      "Avg loss: 1618496.723958 \n",
      "\n",
      "loss: 225694.203125  [    0/  790]\n",
      "Test Error:\n",
      "Avg loss: 1620925.601562 \n",
      "\n",
      "loss: 225869.734375  [    0/  790]\n",
      "Test Error:\n",
      "Avg loss: 1622778.742188 \n",
      "\n",
      "loss: 226013.531250  [    0/  790]\n",
      "Test Error:\n",
      "Avg loss: 1599814.710938 \n",
      "\n",
      "loss: 224862.093750  [    0/  790]\n",
      "Test Error:\n",
      "Avg loss: 1627850.385417 \n",
      "\n",
      "loss: 226182.078125  [    0/  790]\n",
      "Test Error:\n",
      "Avg loss: 1630712.513021 \n",
      "\n",
      "loss: 226365.515625  [    0/  790]\n",
      "Test Error:\n",
      "Avg loss: 1632417.898438 \n",
      "\n",
      "loss: 226487.109375  [    0/  790]\n",
      "Test Error:\n",
      "Avg loss: 1633701.981771 \n",
      "\n",
      "loss: 226583.734375  [    0/  790]\n",
      "Test Error:\n",
      "Avg loss: 1634681.372396 \n",
      "\n",
      "loss: 226661.500000  [    0/  790]\n",
      "Test Error:\n",
      "Avg loss: 1634612.335938 \n",
      "\n",
      "loss: 226684.421875  [    0/  790]\n",
      "Test Error:\n",
      "Avg loss: 1634810.294271 \n",
      "\n",
      "loss: 226716.671875  [    0/  790]\n",
      "Test Error:\n",
      "Avg loss: 1636572.497396 \n",
      "\n",
      "loss: 226824.109375  [    0/  790]\n",
      "Test Error:\n",
      "Avg loss: 1636062.184896 \n",
      "\n",
      "loss: 226816.625000  [    0/  790]\n",
      "Test Error:\n",
      "Avg loss: 1637570.794271 \n",
      "\n",
      "loss: 226905.765625  [    0/  790]\n",
      "Test Error:\n",
      "Avg loss: 1636810.585938 \n",
      "\n",
      "loss: 226881.171875  [    0/  790]\n",
      "Test Error:\n",
      "Avg loss: 1638136.338542 \n",
      "\n",
      "loss: 226956.953125  [    0/  790]\n",
      "Test Error:\n",
      "Avg loss: 1637232.343750 \n",
      "\n",
      "loss: 226921.265625  [    0/  790]\n",
      "Test Error:\n",
      "Avg loss: 1638449.880208 \n",
      "\n",
      "loss: 226988.140625  [    0/  790]\n",
      "Test Error:\n",
      "Avg loss: 1639261.296875 \n",
      "\n",
      "loss: 227032.500000  [    0/  790]\n",
      "Test Error:\n",
      "Avg loss: 1638063.429688 \n",
      "\n",
      "loss: 226976.968750  [    0/  790]\n",
      "Test Error:\n",
      "Avg loss: 1639079.549479 \n",
      "\n",
      "loss: 227028.406250  [    0/  790]\n",
      "Test Error:\n",
      "Avg loss: 1637916.158854 \n",
      "\n",
      "loss: 226972.250000  [    0/  790]\n",
      "Test Error:\n",
      "Avg loss: 1638958.502604 \n",
      "\n",
      "loss: 227022.718750  [    0/  790]\n",
      "Test Error:\n",
      "Avg loss: 1639631.872396 \n",
      "\n",
      "loss: 227053.328125  [    0/  790]\n",
      "Test Error:\n",
      "Avg loss: 1638324.502604 \n",
      "\n",
      "loss: 226985.812500  [    0/  790]\n",
      "Test Error:\n",
      "Avg loss: 1639258.565104 \n",
      "\n",
      "loss: 227027.203125  [    0/  790]\n",
      "Test Error:\n",
      "Avg loss: 1638027.843750 \n",
      "\n",
      "loss: 226961.921875  [    0/  790]\n",
      "Test Error:\n",
      "Avg loss: 1639027.622396 \n",
      "\n",
      "loss: 227004.859375  [    0/  790]\n",
      "Test Error:\n",
      "Avg loss: 1639667.604167 \n",
      "\n",
      "loss: 227028.671875  [    0/  790]\n",
      "Test Error:\n",
      "Avg loss: 1638325.971354 \n",
      "\n",
      "loss: 226954.718750  [    0/  790]\n",
      "Test Error:\n",
      "Avg loss: 1639243.921875 \n",
      "\n",
      "loss: 226990.562500  [    0/  790]\n",
      "Test Error:\n",
      "Avg loss: 1639817.640625 \n",
      "\n",
      "loss: 227008.343750  [    0/  790]\n",
      "Test Error:\n",
      "Avg loss: 1638427.796875 \n",
      "\n",
      "loss: 226929.343750  [    0/  790]\n",
      "Test Error:\n",
      "Avg loss: 1639307.213542 \n",
      "\n",
      "loss: 226960.875000  [    0/  790]\n",
      "Test Error:\n",
      "Avg loss: 1638026.825521 \n",
      "\n",
      "loss: 226886.531250  [    0/  790]\n",
      "Test Error:\n",
      "Avg loss: 1638994.656250 \n",
      "\n",
      "loss: 226921.546875  [    0/  790]\n",
      "Test Error:\n",
      "Avg loss: 1639606.606771 \n",
      "\n",
      "loss: 226937.937500  [    0/  790]\n",
      "Test Error:\n",
      "Avg loss: 1638234.611979 \n",
      "\n",
      "loss: 226856.812500  [    0/  790]\n",
      "Test Error:\n",
      "Avg loss: 1639137.270833 \n",
      "\n",
      "loss: 226886.500000  [    0/  790]\n",
      "Test Error:\n",
      "Avg loss: 1639527.294271 \n",
      "\n",
      "loss: 226890.421875  [    0/  790]\n",
      "Test Error:\n",
      "Avg loss: 1638239.679688 \n",
      "\n",
      "loss: 226811.265625  [    0/  790]\n",
      "Test Error:\n",
      "Avg loss: 1638941.997396 \n",
      "\n",
      "loss: 226829.906250  [    0/  790]\n",
      "Test Error:\n",
      "Avg loss: 1639464.809896 \n",
      "\n",
      "loss: 226838.312500  [    0/  790]\n",
      "Test Error:\n",
      "Avg loss: 1638159.763021 \n",
      "\n",
      "loss: 226756.968750  [    0/  790]\n",
      "Test Error:\n",
      "Avg loss: 1638871.747396 \n",
      "\n",
      "loss: 226774.656250  [    0/  790]\n",
      "Test Error:\n",
      "Avg loss: 1639389.565104 \n",
      "\n",
      "loss: 226781.562500  [    0/  790]\n",
      "Test Error:\n",
      "Avg loss: 1637923.096354 \n",
      "\n",
      "loss: 226690.937500  [    0/  790]\n",
      "Test Error:\n",
      "Avg loss: 1638618.934896 \n",
      "\n",
      "loss: 226707.265625  [    0/  790]\n",
      "Test Error:\n",
      "Avg loss: 1639175.466146 \n",
      "\n",
      "loss: 226715.125000  [    0/  790]\n",
      "Test Error:\n",
      "Avg loss: 1637543.065104 \n",
      "\n",
      "loss: 226616.125000  [    0/  790]\n",
      "Test Error:\n",
      "Avg loss: 1638404.617188 \n",
      "\n",
      "loss: 226639.125000  [    0/  790]\n",
      "Test Error:\n",
      "Avg loss: 1638988.200521 \n",
      "\n",
      "loss: 226647.500000  [    0/  790]\n",
      "Test Error:\n",
      "Avg loss: 1639270.059896 \n",
      "\n",
      "loss: 226639.828125  [    0/  790]\n",
      "Test Error:\n",
      "Avg loss: 1637517.104167 \n",
      "\n",
      "loss: 226533.906250  [    0/  790]\n",
      "Test Error:\n",
      "Avg loss: 1638351.294271 \n",
      "\n",
      "loss: 226554.296875  [    0/  790]\n",
      "Test Error:\n",
      "Avg loss: 1638753.799479 \n",
      "\n",
      "loss: 226552.453125  [    0/  790]\n",
      "Test Error:\n",
      "Avg loss: 1638992.315104 \n",
      "\n",
      "loss: 226542.125000  [    0/  790]\n",
      "Test Error:\n",
      "Avg loss: 1637258.216146 \n",
      "\n",
      "loss: 226436.312500  [    0/  790]\n",
      "Test Error:\n",
      "Avg loss: 1638109.809896 \n",
      "\n",
      "loss: 226456.656250  [    0/  790]\n",
      "Test Error:\n",
      "Avg loss: 1638520.976562 \n",
      "\n",
      "loss: 226454.453125  [    0/  790]\n",
      "Test Error:\n",
      "Avg loss: 1638764.606771 \n",
      "\n",
      "loss: 226443.593750  [    0/  790]\n",
      "Test Error:\n",
      "Avg loss: 1638938.377604 \n",
      "\n",
      "loss: 226428.718750  [    0/  790]\n",
      "Test Error:\n",
      "Avg loss: 1638487.322917 \n",
      "\n",
      "loss: 226569.875000  [    0/  790]\n",
      "Test Error:\n",
      "Avg loss: 1640032.177083 \n",
      "\n",
      "loss: 226611.593750  [    0/  790]\n",
      "Test Error:\n",
      "Avg loss: 1640104.828125 \n",
      "\n",
      "loss: 226586.390625  [    0/  790]\n",
      "Test Error:\n",
      "Avg loss: 1638232.114583 \n",
      "\n",
      "loss: 226467.421875  [    0/  790]\n",
      "Test Error:\n",
      "Avg loss: 1638833.658854 \n",
      "\n",
      "loss: 226469.546875  [    0/  790]\n",
      "Test Error:\n",
      "Avg loss: 1639147.955729 \n",
      "\n",
      "loss: 226457.593750  [    0/  790]\n",
      "Test Error:\n",
      "Avg loss: 1639364.622396 \n",
      "\n",
      "loss: 226440.421875  [    0/  790]\n",
      "Test Error:\n",
      "Avg loss: 1639513.460938 \n",
      "\n",
      "loss: 226419.578125  [    0/  790]\n",
      "Test Error:\n",
      "Avg loss: 1639894.945312 \n",
      "\n",
      "loss: 226580.906250  [    0/  790]\n",
      "Test Error:\n",
      "Avg loss: 1640357.567708 \n",
      "\n",
      "loss: 226548.937500  [    0/  790]\n",
      "Test Error:\n",
      "Avg loss: 1639815.903646 \n",
      "\n",
      "loss: 226474.453125  [    0/  790]\n",
      "Test Error:\n",
      "Avg loss: 1639472.281250 \n",
      "\n",
      "loss: 226409.078125  [    0/  790]\n",
      "Test Error:\n",
      "Avg loss: 1639254.000000 \n",
      "\n",
      "loss: 226350.328125  [    0/  790]\n",
      "Test Error:\n",
      "Avg loss: 1639111.486979 \n",
      "\n",
      "loss: 226296.687500  [    0/  790]\n",
      "Test Error:\n",
      "Avg loss: 1640313.015625 \n",
      "\n",
      "loss: 226500.921875  [    0/  790]\n",
      "Test Error:\n",
      "Avg loss: 1641020.015625 \n",
      "\n",
      "loss: 226476.250000  [    0/  790]\n",
      "Test Error:\n",
      "Avg loss: 1641804.799479 \n",
      "\n",
      "loss: 226469.593750  [    0/  790]\n",
      "Test Error:\n",
      "Avg loss: 1641338.466146 \n",
      "\n",
      "loss: 226394.640625  [    0/  790]\n",
      "Test Error:\n",
      "Avg loss: 1642033.325521 \n",
      "\n",
      "loss: 226386.906250  [    0/  790]\n",
      "Test Error:\n",
      "Avg loss: 1641559.190104 \n",
      "\n",
      "loss: 226312.171875  [    0/  790]\n",
      "Test Error:\n",
      "Avg loss: 1641809.997396 \n",
      "\n",
      "loss: 226476.265625  [    0/  790]\n",
      "Test Error:\n",
      "Avg loss: 1642569.023438 \n",
      "\n",
      "loss: 226454.500000  [    0/  790]\n",
      "Test Error:\n",
      "Avg loss: 1643139.971354 \n",
      "\n",
      "loss: 226439.765625  [    0/  790]\n",
      "Test Error:\n",
      "Avg loss: 1642544.500000 \n",
      "\n",
      "loss: 226357.703125  [    0/  790]\n",
      "Test Error:\n",
      "Avg loss: 1643112.070312 \n",
      "\n",
      "loss: 226345.515625  [    0/  790]\n",
      "Test Error:\n",
      "Avg loss: 1642236.802083 \n",
      "\n",
      "loss: 226251.640625  [    0/  790]\n",
      "Test Error:\n",
      "Avg loss: 1642438.700521 \n",
      "\n",
      "loss: 226412.546875  [    0/  790]\n",
      "Test Error:\n",
      "Avg loss: 1644118.526042 \n",
      "\n",
      "loss: 226447.484375  [    0/  790]\n",
      "Test Error:\n",
      "Avg loss: 1643167.158854 \n",
      "\n",
      "loss: 226346.953125  [    0/  790]\n",
      "Test Error:\n",
      "Avg loss: 1641904.411458 \n",
      "\n",
      "loss: 226254.640625  [    0/  790]\n",
      "Test Error:\n",
      "Avg loss: 1642597.015625 \n",
      "\n",
      "loss: 226261.593750  [    0/  790]\n",
      "Test Error:\n",
      "Avg loss: 1642059.677083 \n",
      "\n",
      "loss: 226199.703125  [    0/  790]\n",
      "Test Error:\n",
      "Avg loss: 1643920.914062 \n",
      "\n",
      "loss: 226459.734375  [    0/  790]\n",
      "Test Error:\n",
      "Avg loss: 1643787.645833 \n",
      "\n",
      "loss: 226404.734375  [    0/  790]\n",
      "Test Error:\n",
      "Avg loss: 1642280.325521 \n",
      "\n",
      "loss: 226295.531250  [    0/  790]\n",
      "Test Error:\n",
      "Avg loss: 1642833.434896 \n",
      "\n",
      "loss: 226292.687500  [    0/  790]\n",
      "Test Error:\n",
      "Avg loss: 1642162.591146 \n",
      "\n",
      "loss: 226220.375000  [    0/  790]\n",
      "Test Error:\n",
      "Avg loss: 1640479.257812 \n",
      "\n",
      "loss: 226108.000000  [    0/  790]\n",
      "Test Error:\n",
      "Avg loss: 1642588.734375 \n",
      "\n",
      "loss: 226379.812500  [    0/  790]\n",
      "Test Error:\n",
      "Avg loss: 1642678.841146 \n",
      "\n",
      "loss: 226336.156250  [    0/  790]\n",
      "Test Error:\n",
      "Avg loss: 1641023.781250 \n",
      "\n",
      "loss: 226220.531250  [    0/  790]\n",
      "Test Error:\n",
      "Avg loss: 1641746.492188 \n",
      "\n",
      "loss: 226224.984375  [    0/  790]\n",
      "Test Error:\n",
      "Avg loss: 1641244.778646 \n",
      "\n",
      "loss: 226162.812500  [    0/  790]\n",
      "Test Error:\n",
      "Avg loss: 1641841.252604 \n",
      "\n",
      "loss: 226161.984375  [    0/  790]\n",
      "Test Error:\n",
      "Avg loss: 1642254.611979 \n",
      "\n",
      "loss: 226339.812500  [    0/  790]\n",
      "Test Error:\n",
      "Avg loss: 1641857.783854 \n",
      "\n",
      "loss: 226277.062500  [    0/  790]\n",
      "Test Error:\n",
      "Avg loss: 1642357.591146 \n",
      "\n",
      "loss: 226268.015625  [    0/  790]\n",
      "Test Error:\n",
      "Avg loss: 1641370.786458 \n",
      "\n",
      "loss: 226177.484375  [    0/  790]\n",
      "Test Error:\n",
      "Avg loss: 1640173.643229 \n",
      "\n",
      "loss: 226084.437500  [    0/  790]\n",
      "Test Error:\n",
      "Avg loss: 1642232.283854 \n",
      "\n",
      "loss: 226349.937500  [    0/  790]\n",
      "Test Error:\n",
      "Avg loss: 1642224.361979 \n",
      "\n",
      "loss: 226297.812500  [    0/  790]\n",
      "Test Error:\n",
      "Avg loss: 1640825.864583 \n",
      "\n",
      "loss: 226191.156250  [    0/  790]\n",
      "Test Error:\n",
      "Avg loss: 1641479.026042 \n",
      "\n",
      "loss: 226188.859375  [    0/  790]\n",
      "Test Error:\n",
      "Avg loss: 1640606.661458 \n",
      "\n",
      "loss: 226103.843750  [    0/  790]\n",
      "Test Error:\n",
      "Avg loss: 1639115.320312 \n",
      "\n",
      "loss: 225997.171875  [    0/  790]\n",
      "Test Error:\n",
      "Avg loss: 1641334.992188 \n",
      "\n",
      "loss: 226270.187500  [    0/  790]\n",
      "Test Error:\n",
      "Avg loss: 1641446.518229 \n",
      "\n",
      "loss: 226224.375000  [    0/  790]\n",
      "Test Error:\n",
      "Avg loss: 1640173.078125 \n",
      "\n",
      "loss: 226123.437500  [    0/  790]\n",
      "Test Error:\n",
      "Avg loss: 1638770.054688 \n",
      "\n",
      "loss: 226018.546875  [    0/  790]\n",
      "Test Error:\n",
      "Avg loss: 1639890.632812 \n",
      "\n",
      "loss: 226040.281250  [    0/  790]\n",
      "Test Error:\n",
      "Avg loss: 1639334.638021 \n",
      "\n",
      "loss: 225973.562500  [    0/  790]\n",
      "Test Error:\n",
      "Avg loss: 1639387.106771 \n",
      "\n",
      "loss: 226138.828125  [    0/  790]\n",
      "Test Error:\n",
      "Avg loss: 1639082.658854 \n",
      "\n",
      "loss: 226080.281250  [    0/  790]\n",
      "Test Error:\n",
      "Avg loss: 1640123.039062 \n",
      "\n",
      "loss: 226096.000000  [    0/  790]\n",
      "Test Error:\n",
      "Avg loss: 1639804.044271 \n",
      "\n",
      "loss: 226037.546875  [    0/  790]\n",
      "Test Error:\n",
      "Avg loss: 1638364.679688 \n",
      "\n",
      "loss: 225930.078125  [    0/  790]\n",
      "Test Error:\n",
      "Avg loss: 1640778.346354 \n",
      "\n",
      "loss: 226211.515625  [    0/  790]\n",
      "Test Error:\n",
      "Avg loss: 1640943.294271 \n",
      "\n",
      "loss: 226166.328125  [    0/  790]\n",
      "Test Error:\n",
      "Avg loss: 1639673.281250 \n",
      "\n",
      "loss: 226062.656250  [    0/  790]\n",
      "Test Error:\n",
      "Avg loss: 1638388.565104 \n",
      "\n",
      "loss: 225960.906250  [    0/  790]\n",
      "Test Error:\n",
      "Avg loss: 1639420.846354 \n",
      "\n",
      "loss: 225977.078125  [    0/  790]\n",
      "Test Error:\n",
      "Avg loss: 1638845.554688 \n",
      "\n",
      "loss: 225906.890625  [    0/  790]\n",
      "Test Error:\n",
      "Avg loss: 1638898.695312 \n",
      "\n",
      "loss: 226070.484375  [    0/  790]\n",
      "Test Error:\n",
      "Avg loss: 1637675.677083 \n",
      "\n",
      "loss: 225965.031250  [    0/  790]\n",
      "Test Error:\n",
      "Avg loss: 1638840.638021 \n",
      "\n",
      "loss: 225988.031250  [    0/  790]\n",
      "Test Error:\n",
      "Avg loss: 1638382.601562 \n",
      "\n",
      "loss: 225921.609375  [    0/  790]\n",
      "Test Error:\n",
      "Avg loss: 1636280.432292 \n",
      "\n",
      "loss: 225781.406250  [    0/  790]\n",
      "Test Error:\n",
      "Avg loss: 1639178.731771 \n",
      "\n",
      "loss: 226088.421875  [    0/  790]\n",
      "Test Error:\n",
      "Avg loss: 1639577.973958 \n",
      "\n",
      "loss: 226055.718750  [    0/  790]\n",
      "Test Error:\n",
      "Avg loss: 1637291.111979 \n",
      "\n",
      "loss: 225901.593750  [    0/  790]\n",
      "Test Error:\n",
      "Avg loss: 1638466.109375 \n",
      "\n",
      "loss: 225924.640625  [    0/  790]\n",
      "Test Error:\n",
      "Avg loss: 1638091.898438 \n",
      "\n",
      "loss: 225861.937500  [    0/  790]\n",
      "Test Error:\n",
      "Avg loss: 1637198.088542 \n",
      "\n",
      "loss: 225978.531250  [    0/  790]\n",
      "Test Error:\n",
      "Avg loss: 1639361.104167 \n",
      "\n",
      "loss: 226041.687500  [    0/  790]\n",
      "Test Error:\n",
      "Avg loss: 1638805.507812 \n",
      "\n",
      "loss: 225966.500000  [    0/  790]\n",
      "Test Error:\n",
      "Avg loss: 1636611.507812 \n",
      "\n",
      "loss: 225817.640625  [    0/  790]\n",
      "Test Error:\n",
      "Avg loss: 1637853.132812 \n",
      "\n",
      "loss: 225844.046875  [    0/  790]\n",
      "Test Error:\n",
      "Avg loss: 1638836.674479 \n",
      "\n",
      "loss: 226045.718750  [    0/  790]\n",
      "Test Error:\n",
      "Avg loss: 1638677.895833 \n",
      "\n",
      "loss: 225987.828125  [    0/  790]\n",
      "Test Error:\n",
      "Avg loss: 1636471.132812 \n",
      "\n",
      "loss: 225837.406250  [    0/  790]\n",
      "Test Error:\n",
      "Avg loss: 1637729.135417 \n",
      "\n",
      "loss: 225863.375000  [    0/  790]\n",
      "Test Error:\n",
      "Avg loss: 1637542.622396 \n",
      "\n",
      "loss: 225808.734375  [    0/  790]\n",
      "Test Error:\n",
      "Avg loss: 1636747.648438 \n",
      "\n",
      "loss: 225928.750000  [    0/  790]\n",
      "Test Error:\n",
      "Avg loss: 1636006.044271 \n",
      "\n",
      "loss: 225846.109375  [    0/  790]\n",
      "Test Error:\n",
      "Avg loss: 1637474.437500 \n",
      "\n",
      "loss: 225881.218750  [    0/  790]\n",
      "Test Error:\n",
      "Avg loss: 1636198.682292 \n",
      "\n",
      "loss: 225773.015625  [    0/  790]\n",
      "Test Error:\n",
      "Avg loss: 1637444.783854 \n",
      "\n",
      "loss: 225797.906250  [    0/  790]\n",
      "Test Error:\n",
      "Avg loss: 1637204.247396 \n",
      "\n",
      "loss: 225740.500000  [    0/  790]\n",
      "Test Error:\n",
      "Avg loss: 1636221.312500 \n",
      "\n",
      "loss: 225852.875000  [    0/  790]\n",
      "Test Error:\n",
      "Avg loss: 1635585.276042 \n",
      "\n",
      "loss: 225776.796875  [    0/  790]\n",
      "Test Error:\n",
      "Avg loss: 1637068.195312 \n",
      "\n",
      "loss: 225811.140625  [    0/  790]\n",
      "Test Error:\n",
      "Avg loss: 1636070.869792 \n",
      "\n",
      "loss: 225673.171875  [    0/  790]\n",
      "Test Error:\n",
      "Avg loss: 1636073.796875 \n",
      "\n",
      "loss: 225793.734375  [    0/  790]\n",
      "Test Error:\n",
      "Avg loss: 1635894.182292 \n",
      "\n",
      "loss: 225699.109375  [    0/  790]\n",
      "Test Error:\n",
      "Avg loss: 1636976.406250 \n",
      "\n",
      "loss: 225674.562500  [    0/  790]\n",
      "Test Error:\n",
      "Avg loss: 1636567.278646 \n",
      "\n",
      "loss: 225574.234375  [    0/  790]\n",
      "Test Error:\n",
      "Avg loss: 1639787.177083 \n",
      "\n",
      "loss: 225856.593750  [    0/  790]\n",
      "Test Error:\n",
      "Avg loss: 1640832.554688 \n",
      "\n",
      "loss: 225816.875000  [    0/  790]\n",
      "Test Error:\n",
      "Avg loss: 1638804.604167 \n",
      "\n",
      "loss: 225640.750000  [    0/  790]\n",
      "Test Error:\n",
      "Avg loss: 1637725.036458 \n",
      "\n",
      "loss: 225515.828125  [    0/  790]\n",
      "Test Error:\n",
      "Avg loss: 1636719.924479 \n",
      "\n",
      "loss: 225399.875000  [    0/  790]\n",
      "Test Error:\n",
      "Avg loss: 1639245.731771 \n",
      "\n",
      "loss: 225657.953125  [    0/  790]\n",
      "Test Error:\n",
      "Avg loss: 1639693.770833 \n",
      "\n",
      "loss: 225600.718750  [    0/  790]\n",
      "Test Error:\n",
      "Avg loss: 1638424.299479 \n",
      "\n",
      "loss: 225472.140625  [    0/  790]\n",
      "Test Error:\n",
      "Avg loss: 1639456.611979 \n",
      "\n",
      "loss: 225458.515625  [    0/  790]\n",
      "Test Error:\n",
      "Avg loss: 1640484.625000 \n",
      "\n",
      "tensor([[5554.2759],\n",
      "        [5278.3125],\n",
      "        [5010.0928],\n",
      "        [5046.3945],\n",
      "        [5082.4077],\n",
      "        [5249.7539],\n",
      "        [5582.3389],\n",
      "        [5805.0283],\n",
      "        [6746.3569],\n",
      "        [7455.9814],\n",
      "        [8525.3125],\n",
      "        [9823.3662]])\n",
      "tensor([[5039.],\n",
      "        [4811.],\n",
      "        [4785.],\n",
      "        [4790.],\n",
      "        [4910.],\n",
      "        [5170.],\n",
      "        [5381.],\n",
      "        [6115.],\n",
      "        [6786.],\n",
      "        [7694.],\n",
      "        [8855.],\n",
      "        [9876.]])\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "learning_rate = .000000001\n",
    "batch_size = 64\n",
    "epochs = 1000\n",
    "loss_fn = nn.MSELoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# print(list(model.parameters()))\n",
    "for t in range(epochs):\n",
    "    # print(f\"Epoch {t+1}\\n-------------------------------\")\n",
    "    train_loop(train_dataloader, model, loss_fn, optimizer)\n",
    "    pred, y = test_loop(test_dataloader, model, loss_fn)\n",
    "print(pred)\n",
    "print(y)\n",
    "print(\"Done!\")\n",
    "# print(list(model.parameters()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 305,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "Dimension out of range (expected to be in range of [-1, 0], but got 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\mburs\\OneDrive - Lehigh University\\Opportunities\\Winter2022 Projects\\NN\\NNAR.ipynb Cell 15\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/mburs/OneDrive%20-%20Lehigh%20University/Opportunities/Winter2022%20Projects/NN/NNAR.ipynb#X20sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m model\u001b[39m.\u001b[39meval()\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/mburs/OneDrive%20-%20Lehigh%20University/Opportunities/Winter2022%20Projects/NN/NNAR.ipynb#X20sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m model(test_data[\u001b[39m0\u001b[39;49m][\u001b[39m0\u001b[39;49m])\n",
      "File \u001b[1;32mc:\\Users\\mburs\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\nn\\modules\\module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1190\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1191\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1192\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1193\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1194\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39m\u001b[39minput\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1195\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "\u001b[1;32mc:\\Users\\mburs\\OneDrive - Lehigh University\\Opportunities\\Winter2022 Projects\\NN\\NNAR.ipynb Cell 15\u001b[0m in \u001b[0;36mNeuralNetwork.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/mburs/OneDrive%20-%20Lehigh%20University/Opportunities/Winter2022%20Projects/NN/NNAR.ipynb#X20sZmlsZQ%3D%3D?line=11'>12</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, x):\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/mburs/OneDrive%20-%20Lehigh%20University/Opportunities/Winter2022%20Projects/NN/NNAR.ipynb#X20sZmlsZQ%3D%3D?line=12'>13</a>\u001b[0m     x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mflatten(x)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/mburs/OneDrive%20-%20Lehigh%20University/Opportunities/Winter2022%20Projects/NN/NNAR.ipynb#X20sZmlsZQ%3D%3D?line=13'>14</a>\u001b[0m     logits \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlinear_relu_stack(x)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/mburs/OneDrive%20-%20Lehigh%20University/Opportunities/Winter2022%20Projects/NN/NNAR.ipynb#X20sZmlsZQ%3D%3D?line=14'>15</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m logits\n",
      "File \u001b[1;32mc:\\Users\\mburs\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\nn\\modules\\module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1190\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1191\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1192\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1193\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1194\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39m\u001b[39minput\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1195\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32mc:\\Users\\mburs\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\nn\\modules\\flatten.py:46\u001b[0m, in \u001b[0;36mFlatten.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m     45\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m: Tensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[1;32m---> 46\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39minput\u001b[39;49m\u001b[39m.\u001b[39;49mflatten(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mstart_dim, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mend_dim)\n",
      "\u001b[1;31mIndexError\u001b[0m: Dimension out of range (expected to be in range of [-1, 0], but got 1)"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "model(test_data[0][0])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "a148e496c0f49d57628151d2aab378855c5a8a7aaacdf2673cbe18e166795068"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
